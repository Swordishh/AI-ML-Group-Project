{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "8wqMxTDzjUmN"
   },
   "source": [
    "#Initial package import\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as math\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import xarray\n",
    "import tqdm\n",
    "import seaborn\n",
    "import statsmodels\n",
    "import shap\n",
    "import pymint\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "executionInfo": {
     "elapsed": 603,
     "status": "ok",
     "timestamp": 1623166583321,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "CJ-99UKBmebh",
    "outputId": "e5ef8acb-557c-4b7d-baeb-3d869a8f3b80"
   },
   "source": [
    "#Load in data for model training and testing from csv using pandas\n",
    "#Full dataset\n",
    "complete = pd.read_csv('FULL_ML_DATA.csv', engine='python')\n",
    "\n",
    "#Case study for operational implementation\n",
    "case_study = pd.read_csv('CASE_STUDY_ML_DATA.csv', engine='python')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7eHtJYMrwqg"
   },
   "source": [
    "# **Simple Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "URXkN8OgpRwc"
   },
   "source": [
    "#Import more packages and explore a single train-test split of data.\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data organization\n",
    "#CHOOSE PREDICTORS TO USE IN ML PIPELINE\n",
    "\n",
    "#Environmental Predictors\n",
    "#predictor_cols = ['Avg_Meso_Distance_(Km)','Discrete', 'QLCS', 'Multi',\t'Peak_(m/s)', 'Distance(km)', 'tr1meso', 'tr2meso', 'tr3meso', 'tr1dV', 'tr2dV', 'tr3dV', 'SBCAPE_MEAN', 'SBCIN_MEAN', 'MLCAPE_MEAN', 'MLCIN_MEAN', 'MUCAPE_MEAN', '03CAPE_MEAN', 'LI_MEAN', '08bulk_MEAN', '06bulk_MEAN', '03bulk_MEAN', '01bulk_MEAN', '0500bulk_MEAN', 'BR_speed_MEAN', 'meanmotion_MEAN', 'SRH03_MEAN', 'SRH01_MEAN', 'RAPSRH500_MEAN', 'eff_base_h_ma_MEAN', 'eff_top_h_ma_MEAN', 'eff_layer_depth_ma_MEAN', 'EBS_MEAN', 'ESRH_MEAN', 'srwind_02_MEAN', 'srwind_46_MEAN', 'srwind_911_MEAN', 'lapse_36_MEAN', 'lapse_03_MEAN', 'RH_36_MEAN', 'RH_03_MEAN', 'LCL_h_MEAN', 'LFC_h_MEAN', 'LCL_LFC_hdif_MEAN', 'RH_LCL_LFC_MEAN', 'EHI01_MEAN', 'EHI03_MEAN', 'SCP_fixed_MEAN', 'SCP_eff_MEAN', 'STP_fixed_MEAN', 'tor01_EHI_MEAN', 'TTS_MEAN', 'critang_MEAN']\n",
    "\n",
    "#Environmental Predictors\n",
    "#predictor_cols = ['SBCAPE_MEAN', 'SBCIN_MEAN', 'MLCAPE_MEAN', 'MLCIN_MEAN', 'MUCAPE_MEAN', '03CAPE_MEAN', 'LI_MEAN', '08bulk_MEAN', '06bulk_MEAN', '03bulk_MEAN', '01bulk_MEAN', '0500bulk_MEAN', 'BR_speed_MEAN', 'meanmotion_MEAN', 'SRH03_MEAN', 'SRH01_MEAN', 'RAPSRH500_MEAN', 'eff_base_h_ma_MEAN', 'eff_top_h_ma_MEAN', 'eff_layer_depth_ma_MEAN', 'EBS_MEAN', 'ESRH_MEAN', 'srwind_02_MEAN', 'srwind_46_MEAN', 'srwind_911_MEAN', 'lapse_36_MEAN', 'lapse_03_MEAN', 'RH_36_MEAN', 'RH_03_MEAN', 'LCL_h_MEAN', 'LFC_h_MEAN', 'LCL_LFC_hdif_MEAN', 'RH_LCL_LFC_MEAN', 'EHI01_MEAN', 'EHI03_MEAN', 'SCP_fixed_MEAN', 'SCP_eff_MEAN', 'STP_fixed_MEAN', 'tor01_EHI_MEAN', 'TTS_MEAN', 'critang_MEAN']\n",
    "\n",
    "#1st iteration\n",
    "predictor_cols = ['Avg_Meso_Distance_(Km)','Discrete', 'QLCS', 'Multi',\t'Peak_(m/s)', 'Distance(km)']\n",
    "\n",
    "#2nd iteration\n",
    "#predictor_cols = ['Avg_Meso_Distance_(Km)',\t'Mode',\t'Peak_(m/s)', 'tr3meso', 'tr2dV', 'tr3dV']\n",
    "\n",
    "#3rd iteration\n",
    "#predictor_cols = ['Avg_Meso_Distance_(Km)',\t'Mode',\t'Peak_(m/s)']\n",
    "\n",
    "rng=np.random.RandomState(0)\n",
    "\n",
    "target_col = ['Binary_EF']\n",
    "X = complete[predictor_cols]\n",
    "Y = complete[target_col]\n",
    "\n",
    "training_predictor_x, test_predictor_x, training_target_y, test_target_y = train_test_split(X, Y, test_size=0.3, random_state=rng, stratify=Y)\n",
    "\n",
    "#Robust Scaler\n",
    "scaler = preprocessing.RobustScaler()\n",
    "scaler.fit(training_predictor_x)\n",
    "training_predictor = scaler.transform(training_predictor_x)\n",
    "test_predictor = scaler.transform(test_predictor_x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ML Pipeline Examples**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AlWGpglS2ZgL"
   },
   "source": [
    "#Imports for pipeline\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRFClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SIkL1Y0kVK6k"
   },
   "source": [
    "#import needed modules for nested CV ML\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import permutation_test_score\n",
    "import collections\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Plot code for new calibration curves\n",
    "\n",
    "import numpy\n",
    "from descartes import PolygonPatch\n",
    "import shapely.geometry\n",
    "import matplotlib.colors\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "DEFAULT_NUM_BINS = 20\n",
    "RELIABILITY_LINE_COLOUR = numpy.array([228, 26, 28], dtype=float) / 255\n",
    "RELIABILITY_LINE_WIDTH = 3\n",
    "PERFECT_LINE_COLOUR = numpy.full(3, 152. / 255)\n",
    "PERFECT_LINE_WIDTH = 2\n",
    "\n",
    "NO_SKILL_LINE_COLOUR = numpy.array([31, 120, 180], dtype=float) / 255\n",
    "NO_SKILL_LINE_WIDTH = 2\n",
    "SKILL_AREA_TRANSPARENCY = 0.2\n",
    "CLIMATOLOGY_LINE_COLOUR = numpy.full(3, 152. / 255)\n",
    "CLIMATOLOGY_LINE_WIDTH = 2\n",
    "\n",
    "HISTOGRAM_FACE_COLOUR = numpy.array([228, 26, 28], dtype=float) / 255\n",
    "HISTOGRAM_EDGE_COLOUR = numpy.full(3, 0.)\n",
    "HISTOGRAM_EDGE_WIDTH = 2\n",
    "\n",
    "HISTOGRAM_LEFT_EDGE_COORD = 0.575\n",
    "HISTOGRAM_BOTTOM_EDGE_COORD = 0.175\n",
    "HISTOGRAM_WIDTH = 0.3\n",
    "HISTOGRAM_HEIGHT = 0.3\n",
    "\n",
    "HIST_LEFT_EDGE_FOR_REGRESSION = 0.575\n",
    "HIST_WIDTH_FOR_REGRESSION = 0.3\n",
    "HIST_BOTTOM_EDGE_FOR_REGRESSION = 0.225\n",
    "HIST_HEIGHT_FOR_REGRESSION = 0.25\n",
    "\n",
    "HISTOGRAM_X_TICK_VALUES = numpy.linspace(0, 1, num=6, dtype=float)\n",
    "HISTOGRAM_X_TICKS_FOR_REGRESSION = numpy.linspace(0, 0.02, num=11)\n",
    "HISTOGRAM_Y_TICK_SPACING = 0.1\n",
    "\n",
    "FIGURE_WIDTH_INCHES = 15\n",
    "FIGURE_HEIGHT_INCHES = 15\n",
    "\n",
    "FONT_SIZE = 20\n",
    "pyplot.rc('font', size=FONT_SIZE)\n",
    "pyplot.rc('axes', titlesize=FONT_SIZE)\n",
    "pyplot.rc('axes', labelsize=FONT_SIZE)\n",
    "pyplot.rc('xtick', labelsize=FONT_SIZE)\n",
    "pyplot.rc('ytick', labelsize=FONT_SIZE)\n",
    "pyplot.rc('legend', fontsize=FONT_SIZE)\n",
    "pyplot.rc('figure', titlesize=FONT_SIZE)\n",
    "\n",
    "\n",
    "def _vertices_to_polygon_object(x_vertices, y_vertices):\n",
    "    \"\"\"Converts two arrays of vertices to `shapely.geometry.Polygon` object.\n",
    "    V = number of vertices\n",
    "    This method allows for simple polygons only (no disjoint polygons, no\n",
    "    holes).\n",
    "    :param x_vertices: length-V numpy array of x-coordinates.\n",
    "    :param y_vertices: length-V numpy array of y-coordinates.\n",
    "    :return: polygon_object: Instance of `shapely.geometry.Polygon`.\n",
    "    \"\"\"\n",
    "\n",
    "    list_of_vertices = []\n",
    "\n",
    "    for i in range(len(x_vertices)):\n",
    "        list_of_vertices.append(\n",
    "            (x_vertices[i], y_vertices[i])\n",
    "        )\n",
    "\n",
    "    return shapely.geometry.Polygon(shell=list_of_vertices)\n",
    "\n",
    "\n",
    "def _plot_background(axes_object, observed_labels):\n",
    "    \"\"\"Plots background of attributes diagram.\n",
    "    E = number of examples\n",
    "    :param axes_object: Instance of `matplotlib.axes._subplots.AxesSubplot`.\n",
    "        Will plot on these axes.\n",
    "    :param observed_labels: length-E numpy array of class labels (integers in\n",
    "        0...1).\n",
    "    \"\"\"\n",
    "\n",
    "    # Plot positive-skill area.\n",
    "    climatology = numpy.mean(observed_labels)\n",
    "    skill_area_colour = matplotlib.colors.to_rgba(\n",
    "        NO_SKILL_LINE_COLOUR, SKILL_AREA_TRANSPARENCY)\n",
    "\n",
    "    x_vertices_left = numpy.array([0, climatology, climatology, 0, 0])\n",
    "    y_vertices_left = numpy.array([0, 0, climatology, climatology / 2, 0])\n",
    "\n",
    "    left_polygon_object = _vertices_to_polygon_object(\n",
    "        x_vertices=x_vertices_left, y_vertices=y_vertices_left)\n",
    "    left_polygon_patch = PolygonPatch(\n",
    "        left_polygon_object, lw=0, ec=skill_area_colour, fc=skill_area_colour)\n",
    "    axes_object.add_patch(left_polygon_patch)\n",
    "\n",
    "    x_vertices_right = numpy.array(\n",
    "        [climatology, 1, 1, climatology, climatology]\n",
    "    )\n",
    "    y_vertices_right = numpy.array(\n",
    "        [climatology, (1 + climatology) / 2, 1, 1, climatology]\n",
    "    )\n",
    "\n",
    "    right_polygon_object = _vertices_to_polygon_object(\n",
    "        x_vertices=x_vertices_right, y_vertices=y_vertices_right)\n",
    "    right_polygon_patch = PolygonPatch(\n",
    "        right_polygon_object, lw=0, ec=skill_area_colour, fc=skill_area_colour)\n",
    "    axes_object.add_patch(right_polygon_patch)\n",
    "\n",
    "    # Plot no-skill line (at edge of positive-skill area).\n",
    "    no_skill_x_coords = numpy.array([0, 1], dtype=float)\n",
    "    no_skill_y_coords = numpy.array([climatology, 1 + climatology]) / 2\n",
    "    axes_object.plot(\n",
    "        no_skill_x_coords, no_skill_y_coords, color=NO_SKILL_LINE_COLOUR,\n",
    "        linestyle='solid', linewidth=NO_SKILL_LINE_WIDTH)\n",
    "\n",
    "    # Plot climatology line (vertical).\n",
    "    climo_line_x_coords = numpy.full(2, climatology)\n",
    "    climo_line_y_coords = numpy.array([0, 1], dtype=float)\n",
    "    axes_object.plot(\n",
    "        climo_line_x_coords, climo_line_y_coords, color=CLIMATOLOGY_LINE_COLOUR,\n",
    "        linestyle='dashed', linewidth=CLIMATOLOGY_LINE_WIDTH)\n",
    "\n",
    "    # Plot no-resolution line (horizontal).\n",
    "    no_resolution_x_coords = climo_line_y_coords + 0.\n",
    "    no_resolution_y_coords = climo_line_x_coords + 0.\n",
    "    axes_object.plot(\n",
    "        no_resolution_x_coords, no_resolution_y_coords,\n",
    "        color=CLIMATOLOGY_LINE_COLOUR, linestyle='dashed',\n",
    "        linewidth=CLIMATOLOGY_LINE_WIDTH)\n",
    "\n",
    "\n",
    "def _floor_to_nearest(input_value_or_array, increment):\n",
    "    \"\"\"Rounds number(s) down to the nearest multiple of `increment`.\n",
    "    :param input_value_or_array: Input (either scalar or numpy array).\n",
    "    :param increment: Increment (or rounding base -- whatever you want to call\n",
    "        it).\n",
    "    :return: output_value_or_array: Rounded version of `input_value_or_array`.\n",
    "    \"\"\"\n",
    "\n",
    "    return increment * numpy.floor(input_value_or_array / increment)\n",
    "\n",
    "\n",
    "def _get_points_in_regression_relia_curve(observed_values, forecast_values,\n",
    "                                          num_bins):\n",
    "    \"\"\"Creates points for regression-based reliability curve.\n",
    "    E = number of examples\n",
    "    B = number of bins\n",
    "    :param observed_values: length-E numpy array of observed target values.\n",
    "    :param forecast_values: length-E numpy array of forecast target values.\n",
    "    :param num_bins: Number of bins for forecast value.\n",
    "    :return: mean_forecast_by_bin: length-B numpy array of mean forecast values.\n",
    "    :return: mean_observation_by_bin: length-B numpy array of mean observed\n",
    "        values.\n",
    "    :return: num_examples_by_bin: length-B numpy array with number of examples\n",
    "        in each forecast bin.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs_to_bins = _get_histogram(\n",
    "        input_values=forecast_values, num_bins=num_bins,\n",
    "        min_value=numpy.min(forecast_values),\n",
    "        max_value=numpy.max(forecast_values)\n",
    "    )\n",
    "\n",
    "    mean_forecast_by_bin = numpy.full(num_bins, numpy.nan)\n",
    "    mean_observation_by_bin = numpy.full(num_bins, numpy.nan)\n",
    "    num_examples_by_bin = numpy.full(num_bins, -1, dtype=int)\n",
    "\n",
    "    for k in range(num_bins):\n",
    "        these_example_indices = numpy.where(inputs_to_bins == k)[0]\n",
    "        num_examples_by_bin[k] = len(these_example_indices)\n",
    "\n",
    "        mean_forecast_by_bin[k] = numpy.mean(\n",
    "            forecast_values[these_example_indices]\n",
    "        )\n",
    "\n",
    "        mean_observation_by_bin[k] = numpy.mean(\n",
    "            observed_values[these_example_indices]\n",
    "        )\n",
    "\n",
    "    return mean_forecast_by_bin, mean_observation_by_bin, num_examples_by_bin\n",
    "\n",
    "\n",
    "def get_points_in_relia_curve(\n",
    "        observed_labels, forecast_probabilities, num_bins):\n",
    "    \"\"\"Creates points for reliability curve.\n",
    "    The reliability curve is the main component of the attributes diagram.\n",
    "    E = number of examples\n",
    "    B = number of bins\n",
    "    :param observed_labels: length-E numpy array of class labels (integers in\n",
    "        0...1).\n",
    "    :param forecast_probabilities: length-E numpy array with forecast\n",
    "        probabilities of label = 1.\n",
    "    :param num_bins: Number of bins for forecast probability.\n",
    "    :return: mean_forecast_probs: length-B numpy array of mean forecast\n",
    "        probabilities.\n",
    "    :return: mean_event_frequencies: length-B numpy array of conditional mean\n",
    "        event frequencies.  mean_event_frequencies[j] = frequency of label 1\n",
    "        when forecast probability is in the [j]th bin.\n",
    "    :return: num_examples_by_bin: length-B numpy array with number of examples\n",
    "        in each forecast bin.\n",
    "    \"\"\"\n",
    "\n",
    "    assert numpy.all(numpy.logical_or(\n",
    "        observed_labels == 0, observed_labels == 1\n",
    "    ))\n",
    "\n",
    "    assert numpy.all(numpy.logical_and(\n",
    "        forecast_probabilities >= 0, forecast_probabilities <= 1\n",
    "    ))\n",
    "\n",
    "    assert num_bins > 1\n",
    "\n",
    "    inputs_to_bins = _get_histogram(\n",
    "        input_values=forecast_probabilities, num_bins=num_bins, min_value=0.,\n",
    "        max_value=1.)\n",
    "\n",
    "    mean_forecast_probs = numpy.full(num_bins, numpy.nan)\n",
    "    mean_event_frequencies = numpy.full(num_bins, numpy.nan)\n",
    "    num_examples_by_bin = numpy.full(num_bins, -1, dtype=int)\n",
    "\n",
    "    for k in range(num_bins):\n",
    "        these_example_indices = numpy.where(inputs_to_bins == k)[0]\n",
    "        num_examples_by_bin[k] = len(these_example_indices)\n",
    "\n",
    "        mean_forecast_probs[k] = numpy.mean(\n",
    "            forecast_probabilities[these_example_indices])\n",
    "\n",
    "        mean_event_frequencies[k] = numpy.mean(\n",
    "            observed_labels[these_example_indices].astype(float)\n",
    "        )\n",
    "\n",
    "    return mean_forecast_probs, mean_event_frequencies, num_examples_by_bin\n",
    "\n",
    "\n",
    "def plot_reliability_curve(\n",
    "        observed_labels, forecast_probabilities, num_bins=DEFAULT_NUM_BINS,\n",
    "        axes_object=None):\n",
    "    \"\"\"Plots reliability curve.\n",
    "    E = number of examples\n",
    "    :param observed_labels: length-E numpy array of class labels (integers in\n",
    "        0...1).\n",
    "    :param forecast_probabilities: length-E numpy array with forecast\n",
    "        probabilities of label = 1.\n",
    "    :param num_bins: Number of bins for forecast probability.\n",
    "    :param axes_object: Will plot on these axes (instance of\n",
    "        `matplotlib.axes._subplots.AxesSubplot`).  If `axes_object is None`,\n",
    "        will create new axes.\n",
    "    :return: mean_forecast_probs: See doc for `get_points_in_relia_curve`.\n",
    "    :return: mean_event_frequencies: Same.\n",
    "    :return: num_examples_by_bin: Same.\n",
    "    \"\"\"\n",
    "\n",
    "    mean_forecast_probs, mean_event_frequencies, num_examples_by_bin = (\n",
    "        get_points_in_relia_curve(\n",
    "            observed_labels=observed_labels,\n",
    "            forecast_probabilities=forecast_probabilities, num_bins=num_bins)\n",
    "    )\n",
    "\n",
    "    if axes_object is None:\n",
    "        _, axes_object = pyplot.subplots(\n",
    "            1, 1, figsize=(FIGURE_WIDTH_INCHES, FIGURE_HEIGHT_INCHES)\n",
    "        )\n",
    "\n",
    "    perfect_x_coords = numpy.array([0, 1], dtype=float)\n",
    "    perfect_y_coords = perfect_x_coords + 0.\n",
    "    axes_object.plot(\n",
    "        perfect_x_coords, perfect_y_coords, color=PERFECT_LINE_COLOUR,\n",
    "        linestyle='dashed', linewidth=PERFECT_LINE_WIDTH)\n",
    "\n",
    "    real_indices = numpy.where(numpy.invert(numpy.logical_or(\n",
    "        numpy.isnan(mean_forecast_probs), numpy.isnan(mean_event_frequencies)\n",
    "    )))[0]\n",
    "\n",
    "    axes_object.plot(\n",
    "        mean_forecast_probs[real_indices], mean_event_frequencies[real_indices],\n",
    "        color=RELIABILITY_LINE_COLOUR,\n",
    "        linestyle='solid', linewidth=RELIABILITY_LINE_WIDTH)\n",
    "\n",
    "    axes_object.set_xlabel('Forecast probability')\n",
    "    axes_object.set_ylabel('Conditional event frequency')\n",
    "    axes_object.set_xlim(0., 1.)\n",
    "    axes_object.set_ylim(0., 1.)\n",
    "\n",
    "    return mean_forecast_probs, mean_event_frequencies, num_examples_by_bin\n",
    "\n",
    "\n",
    "def plot_regression_relia_curve(\n",
    "        observed_values, forecast_values, num_bins=DEFAULT_NUM_BINS,\n",
    "        figure_object=None, axes_object=None):\n",
    "    \"\"\"Plots reliability curve for regression.\n",
    "    :param observed_values: See doc for `get_points_in_regression_relia_curve`.\n",
    "    :param forecast_values: Same.\n",
    "    :param num_bins: Same.\n",
    "    :param figure_object: See doc for `plot_attributes_diagram`.\n",
    "    :param axes_object: Same.\n",
    "    \"\"\"\n",
    "\n",
    "    mean_forecast_by_bin, mean_observation_by_bin, num_examples_by_bin = (\n",
    "        _get_points_in_regression_relia_curve(\n",
    "            observed_values=observed_values, forecast_values=forecast_values,\n",
    "            num_bins=num_bins)\n",
    "    )\n",
    "\n",
    "    if figure_object is None or axes_object is None:\n",
    "        figure_object, axes_object = pyplot.subplots(\n",
    "            1, 1, figsize=(FIGURE_WIDTH_INCHES, FIGURE_HEIGHT_INCHES)\n",
    "        )\n",
    "\n",
    "    _plot_forecast_hist_for_regression(\n",
    "        figure_object=figure_object, mean_forecast_by_bin=mean_forecast_by_bin,\n",
    "        num_examples_by_bin=num_examples_by_bin)\n",
    "\n",
    "    max_forecast_or_observed = max([\n",
    "        numpy.max(forecast_values), numpy.max(observed_values)\n",
    "    ])\n",
    "\n",
    "    perfect_x_coords = numpy.array([0., max_forecast_or_observed])\n",
    "    perfect_y_coords = perfect_x_coords + 0.\n",
    "    axes_object.plot(\n",
    "        perfect_x_coords, perfect_y_coords, color=PERFECT_LINE_COLOUR,\n",
    "        linestyle='dashed', linewidth=PERFECT_LINE_WIDTH)\n",
    "\n",
    "    real_indices = numpy.where(numpy.invert(numpy.logical_or(\n",
    "        numpy.isnan(mean_forecast_by_bin), numpy.isnan(mean_observation_by_bin)\n",
    "    )))[0]\n",
    "\n",
    "    axes_object.plot(\n",
    "        mean_forecast_by_bin[real_indices],\n",
    "        mean_observation_by_bin[real_indices],\n",
    "        color=RELIABILITY_LINE_COLOUR,\n",
    "        linestyle='solid', linewidth=RELIABILITY_LINE_WIDTH)\n",
    "\n",
    "    axes_object.set_xlabel('Forecast value')\n",
    "    axes_object.set_ylabel('Conditional mean observation')\n",
    "    axes_object.set_xlim(0., max_forecast_or_observed)\n",
    "    axes_object.set_ylim(0., max_forecast_or_observed)\n",
    "\n",
    "\n",
    "def plot_attributes_diagram(\n",
    "        observed_labels, forecast_probabilities, num_bins=DEFAULT_NUM_BINS,\n",
    "        figure_object=None, axes_object=None):\n",
    "    \"\"\"Plots attributes diagram.\n",
    "    :param observed_labels: See doc for `plot_reliability_curve`.\n",
    "    :param forecast_probabilities: Same.\n",
    "    :param num_bins: Same.\n",
    "    :param figure_object: Will plot on this figure (instance of\n",
    "        `matplotlib.figure.Figure`).  If `figure_object is None`, will create a\n",
    "        new one.\n",
    "    :param axes_object: See doc for `plot_reliability_curve`.\n",
    "    :return: mean_forecast_probs: See doc for `get_points_in_relia_curve`.\n",
    "    :return: mean_event_frequencies: Same.\n",
    "    :return: num_examples_by_bin: Same.\n",
    "    \"\"\"\n",
    "\n",
    "    mean_forecast_probs, mean_event_frequencies, num_examples_by_bin = (\n",
    "        get_points_in_relia_curve(\n",
    "            observed_labels=observed_labels,\n",
    "            forecast_probabilities=forecast_probabilities, num_bins=num_bins)\n",
    "    )\n",
    "\n",
    "    if figure_object is None or axes_object is None:\n",
    "        figure_object, axes_object = pyplot.subplots(\n",
    "            1, 1, figsize=(FIGURE_WIDTH_INCHES, FIGURE_HEIGHT_INCHES)\n",
    "        )\n",
    "\n",
    "    _plot_background(axes_object=axes_object, observed_labels=observed_labels)\n",
    "\n",
    "    plot_reliability_curve(\n",
    "        observed_labels=observed_labels,\n",
    "        forecast_probabilities=forecast_probabilities, num_bins=num_bins,\n",
    "        axes_object=axes_object)\n",
    "\n",
    "    return mean_forecast_probs, mean_event_frequencies, num_examples_by_bin"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Plot code for performance diagrams\n",
    "\n",
    "\n",
    "DEFAULT_LINE_COLOUR = numpy.array([228, 26, 28], dtype=float) / 255\n",
    "DEFAULT_LINE_WIDTH = 3\n",
    "DEFAULT_BIAS_LINE_COLOUR = numpy.full(3, 152. / 255)\n",
    "DEFAULT_BIAS_LINE_WIDTH = 2\n",
    "\n",
    "LEVELS_FOR_CSI_CONTOURS = numpy.linspace(0, 1, num=11, dtype=float)\n",
    "LEVELS_FOR_BIAS_CONTOURS = numpy.array(\n",
    "    [0.25, 0.5, 0.75, 1., 1.5, 2., 3., 5.])\n",
    "\n",
    "BIAS_STRING_FORMAT = '%.2f'\n",
    "BIAS_LABEL_PADDING_PX = 10\n",
    "\n",
    "FIGURE_WIDTH_INCHES = 10\n",
    "FIGURE_HEIGHT_INCHES = 10\n",
    "\n",
    "FONT_SIZE = 20\n",
    "pyplot.rc('font', size=FONT_SIZE)\n",
    "pyplot.rc('axes', titlesize=FONT_SIZE)\n",
    "pyplot.rc('axes', labelsize=FONT_SIZE)\n",
    "pyplot.rc('xtick', labelsize=FONT_SIZE)\n",
    "pyplot.rc('ytick', labelsize=FONT_SIZE)\n",
    "pyplot.rc('legend', fontsize=FONT_SIZE)\n",
    "pyplot.rc('figure', titlesize=FONT_SIZE)\n",
    "\n",
    "\n",
    "def _get_sr_pod_grid(success_ratio_spacing=0.01, pod_spacing=0.01):\n",
    "    \"\"\"Creates grid in SR-POD (success ratio / probability of detection) space.\n",
    "    M = number of rows (unique POD values) in grid\n",
    "    N = number of columns (unique success ratios) in grid\n",
    "    :param success_ratio_spacing: Spacing between grid cells in adjacent\n",
    "        columns.\n",
    "    :param pod_spacing: Spacing between grid cells in adjacent rows.\n",
    "    :return: success_ratio_matrix: M-by-N numpy array of success ratios.\n",
    "        Success ratio increases with column index.\n",
    "    :return: pod_matrix: M-by-N numpy array of POD values.  POD decreases with\n",
    "        row index.\n",
    "    \"\"\"\n",
    "\n",
    "    num_success_ratios = 1 + int(numpy.ceil(1. / success_ratio_spacing))\n",
    "    num_pod_values = 1 + int(numpy.ceil(1. / pod_spacing))\n",
    "\n",
    "    unique_success_ratios = numpy.linspace(0., 1., num=num_success_ratios)\n",
    "    unique_pod_values = numpy.linspace(0., 1., num=num_pod_values)[::-1]\n",
    "    return numpy.meshgrid(unique_success_ratios, unique_pod_values)\n",
    "\n",
    "\n",
    "def _csi_from_sr_and_pod(success_ratio_array, pod_array):\n",
    "    \"\"\"Computes CSI (critical success index) from success ratio and POD.\n",
    "    POD = probability of detection\n",
    "    :param success_ratio_array: numpy array (any shape) of success ratios.\n",
    "    :param pod_array: numpy array (same shape) of POD values.\n",
    "    :return: csi_array: numpy array (same shape) of CSI values.\n",
    "    \"\"\"\n",
    "\n",
    "    return (success_ratio_array ** -1 + pod_array ** -1 - 1.) ** -1\n",
    "\n",
    "\n",
    "def _bias_from_sr_and_pod(success_ratio_array, pod_array):\n",
    "    \"\"\"Computes frequency bias from success ratio and POD.\n",
    "    POD = probability of detection\n",
    "    :param success_ratio_array: numpy array (any shape) of success ratios.\n",
    "    :param pod_array: numpy array (same shape) of POD values.\n",
    "    :return: frequency_bias_array: numpy array (same shape) of frequency biases.\n",
    "    \"\"\"\n",
    "\n",
    "    return pod_array / success_ratio_array\n",
    "\n",
    "\n",
    "def _get_csi_colour_scheme():\n",
    "    \"\"\"Returns colour scheme for CSI (critical success index).\n",
    "    :return: colour_map_object: Colour scheme (instance of\n",
    "        `matplotlib.colors.ListedColormap`).\n",
    "    :return: colour_norm_object: Instance of `matplotlib.colors.BoundaryNorm`,\n",
    "        defining the scale of the colour map.\n",
    "    \"\"\"\n",
    "\n",
    "    this_colour_map_object = pyplot.cm.Blues\n",
    "    this_colour_norm_object = matplotlib.colors.BoundaryNorm(\n",
    "        LEVELS_FOR_CSI_CONTOURS, this_colour_map_object.N)\n",
    "\n",
    "    rgba_matrix = this_colour_map_object(this_colour_norm_object(\n",
    "        LEVELS_FOR_CSI_CONTOURS))\n",
    "    colour_list = [\n",
    "        rgba_matrix[i, ..., :-1] for i in range(rgba_matrix.shape[0])\n",
    "    ]\n",
    "\n",
    "    colour_map_object = matplotlib.colors.ListedColormap(colour_list)\n",
    "    colour_map_object.set_under(numpy.array([1, 1, 1]))\n",
    "    colour_norm_object = matplotlib.colors.BoundaryNorm(\n",
    "        LEVELS_FOR_CSI_CONTOURS, colour_map_object.N)\n",
    "\n",
    "    return colour_map_object, colour_norm_object\n",
    "\n",
    "\n",
    "def _add_colour_bar(\n",
    "        axes_object, colour_map_object, values_to_colour, min_colour_value,\n",
    "        max_colour_value, colour_norm_object=None,\n",
    "        orientation_string='vertical', extend_min=True, extend_max=True,\n",
    "        fraction_of_axis_length=1., font_size=FONT_SIZE):\n",
    "    \"\"\"Adds colour bar to existing axes.\n",
    "    :param axes_object: Existing axes (instance of\n",
    "        `matplotlib.axes._subplots.AxesSubplot`).\n",
    "    :param colour_map_object: Colour scheme (instance of\n",
    "        `matplotlib.pyplot.cm`).\n",
    "    :param values_to_colour: numpy array of values to colour.\n",
    "    :param min_colour_value: Minimum value in colour map.\n",
    "    :param max_colour_value: Max value in colour map.\n",
    "    :param colour_norm_object: Instance of `matplotlib.colors.BoundaryNorm`,\n",
    "        defining the scale of the colour map.  If `colour_norm_object is None`,\n",
    "        will assume that scale is linear.\n",
    "    :param orientation_string: Orientation of colour bar (\"vertical\" or\n",
    "        \"horizontal\").\n",
    "    :param extend_min: Boolean flag.  If True, the bottom of the colour bar will\n",
    "        have an arrow.  If False, it will be a flat line, suggesting that lower\n",
    "        values are not possible.\n",
    "    :param extend_max: Same but for top of colour bar.\n",
    "    :param fraction_of_axis_length: Fraction of axis length (y-axis if\n",
    "        orientation is \"vertical\", x-axis if orientation is \"horizontal\")\n",
    "        occupied by colour bar.\n",
    "    :param font_size: Font size for labels on colour bar.\n",
    "    :return: colour_bar_object: Colour bar (instance of\n",
    "        `matplotlib.pyplot.colorbar`) created by this method.\n",
    "    \"\"\"\n",
    "\n",
    "    if colour_norm_object is None:\n",
    "        colour_norm_object = matplotlib.colors.Normalize(\n",
    "            vmin=min_colour_value, vmax=max_colour_value, clip=False)\n",
    "\n",
    "    scalar_mappable_object = pyplot.cm.ScalarMappable(\n",
    "        cmap=colour_map_object, norm=colour_norm_object)\n",
    "    scalar_mappable_object.set_array(values_to_colour)\n",
    "\n",
    "    if extend_min and extend_max:\n",
    "        extend_string = 'both'\n",
    "    elif extend_min:\n",
    "        extend_string = 'min'\n",
    "    elif extend_max:\n",
    "        extend_string = 'max'\n",
    "    else:\n",
    "        extend_string = 'neither'\n",
    "\n",
    "    if orientation_string == 'horizontal':\n",
    "        padding = 0.075\n",
    "    else:\n",
    "        padding = 0.05\n",
    "\n",
    "    colour_bar_object = pyplot.colorbar(\n",
    "        ax=axes_object, mappable=scalar_mappable_object,\n",
    "        orientation=orientation_string, pad=padding, extend=extend_string,\n",
    "        shrink=fraction_of_axis_length)\n",
    "\n",
    "    colour_bar_object.ax.tick_params(labelsize=font_size)\n",
    "    return colour_bar_object\n",
    "\n",
    "\n",
    "def get_points_in_perf_diagram(observed_labels, forecast_probabilities):\n",
    "    \"\"\"Creates points for performance diagram.\n",
    "    E = number of examples\n",
    "    T = number of binarization thresholds\n",
    "    :param observed_labels: length-E numpy array of class labels (integers in\n",
    "        0...1).\n",
    "    :param forecast_probabilities: length-E numpy array with forecast\n",
    "        probabilities of label = 1.\n",
    "    :return: pod_by_threshold: length-T numpy array of POD (probability of\n",
    "        detection) values.\n",
    "    :return: success_ratio_by_threshold: length-T numpy array of success ratios.\n",
    "    \"\"\"\n",
    "\n",
    "    assert numpy.all(numpy.logical_or(\n",
    "        observed_labels == 0, observed_labels == 1\n",
    "    ))\n",
    "\n",
    "    assert numpy.all(numpy.logical_and(\n",
    "        forecast_probabilities >= 0, forecast_probabilities <= 1\n",
    "    ))\n",
    "\n",
    "    observed_labels = observed_labels.astype(int)\n",
    "    binarization_thresholds = numpy.linspace(0, 1, num=1001, dtype=float)\n",
    "\n",
    "    num_thresholds = len(binarization_thresholds)\n",
    "    pod_by_threshold = numpy.full(num_thresholds, numpy.nan)\n",
    "    success_ratio_by_threshold = numpy.full(num_thresholds, numpy.nan)\n",
    "\n",
    "    for k in range(num_thresholds):\n",
    "        these_forecast_labels = (\n",
    "            forecast_probabilities >= binarization_thresholds[k]\n",
    "        ).astype(int)\n",
    "\n",
    "        this_num_hits = numpy.sum(numpy.logical_and(\n",
    "            these_forecast_labels == 1, observed_labels == 1\n",
    "        ))\n",
    "\n",
    "        this_num_false_alarms = numpy.sum(numpy.logical_and(\n",
    "            these_forecast_labels == 1, observed_labels == 0\n",
    "        ))\n",
    "\n",
    "        this_num_misses = numpy.sum(numpy.logical_and(\n",
    "            these_forecast_labels == 0, observed_labels == 1\n",
    "        ))\n",
    "\n",
    "        try:\n",
    "            pod_by_threshold[k] = (\n",
    "                float(this_num_hits) / (this_num_hits + this_num_misses)\n",
    "            )\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            success_ratio_by_threshold[k] = (\n",
    "                float(this_num_hits) / (this_num_hits + this_num_false_alarms)\n",
    "            )\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "\n",
    "    pod_by_threshold = numpy.array([1.] + pod_by_threshold.tolist() + [0.])\n",
    "    success_ratio_by_threshold = numpy.array(\n",
    "        [0.] + success_ratio_by_threshold.tolist() + [1.]\n",
    "    )\n",
    "\n",
    "    return pod_by_threshold, success_ratio_by_threshold\n",
    "\n",
    "\n",
    "def plot_performance_diagram(\n",
    "        observed_labels, forecast_probabilities,\n",
    "        line_colour=DEFAULT_LINE_COLOUR, line_width=DEFAULT_LINE_WIDTH,\n",
    "        bias_line_colour=DEFAULT_BIAS_LINE_COLOUR,\n",
    "        bias_line_width=DEFAULT_BIAS_LINE_WIDTH, axes_object=None):\n",
    "    \"\"\"Plots performance diagram.\n",
    "    E = number of examples\n",
    "    :param observed_labels: length-E numpy array of class labels (integers in\n",
    "        0...1).\n",
    "    :param forecast_probabilities: length-E numpy array with forecast\n",
    "        probabilities of label = 1.\n",
    "    :param line_colour: Colour (in any format accepted by `matplotlib.colors`).\n",
    "    :param line_width: Line width (real positive number).\n",
    "    :param bias_line_colour: Colour of contour lines for frequency bias.\n",
    "    :param bias_line_width: Width of contour lines for frequency bias.\n",
    "    :param axes_object: Will plot on these axes (instance of\n",
    "        `matplotlib.axes._subplots.AxesSubplot`).  If `axes_object is None`,\n",
    "        will create new axes.\n",
    "    :return: pod_by_threshold: See doc for `get_points_in_perf_diagram`.\n",
    "        detection) values.\n",
    "    :return: success_ratio_by_threshold: Same.\n",
    "    \"\"\"\n",
    "\n",
    "    pod_by_threshold, success_ratio_by_threshold = get_points_in_perf_diagram(\n",
    "        observed_labels=observed_labels,\n",
    "        forecast_probabilities=forecast_probabilities)\n",
    "\n",
    "    if axes_object is None:\n",
    "        _, axes_object = pyplot.subplots(\n",
    "            1, 1, figsize=(FIGURE_WIDTH_INCHES, FIGURE_HEIGHT_INCHES)\n",
    "        )\n",
    "\n",
    "    success_ratio_matrix, pod_matrix = _get_sr_pod_grid()\n",
    "    csi_matrix = _csi_from_sr_and_pod(success_ratio_matrix, pod_matrix)\n",
    "    frequency_bias_matrix = _bias_from_sr_and_pod(\n",
    "        success_ratio_matrix, pod_matrix)\n",
    "\n",
    "    this_colour_map_object, this_colour_norm_object = _get_csi_colour_scheme()\n",
    "\n",
    "    pyplot.contourf(\n",
    "        success_ratio_matrix, pod_matrix, csi_matrix, LEVELS_FOR_CSI_CONTOURS,\n",
    "        cmap=this_colour_map_object, norm=this_colour_norm_object, vmin=0.,\n",
    "        vmax=1., axes=axes_object)\n",
    "\n",
    "    colour_bar_object = _add_colour_bar(\n",
    "        axes_object=axes_object, colour_map_object=this_colour_map_object,\n",
    "        colour_norm_object=this_colour_norm_object,\n",
    "        values_to_colour=csi_matrix, min_colour_value=0.,\n",
    "        max_colour_value=1., orientation_string='vertical',\n",
    "        extend_min=False, extend_max=False)\n",
    "    colour_bar_object.set_label('CSI (critical success index)')\n",
    "\n",
    "    bias_colour_tuple = ()\n",
    "    for _ in range(len(LEVELS_FOR_BIAS_CONTOURS)):\n",
    "        bias_colour_tuple += (bias_line_colour,)\n",
    "\n",
    "    bias_contour_object = pyplot.contour(\n",
    "        success_ratio_matrix, pod_matrix, frequency_bias_matrix,\n",
    "        LEVELS_FOR_BIAS_CONTOURS, colors=bias_colour_tuple,\n",
    "        linewidths=bias_line_width, linestyles='dashed', axes=axes_object)\n",
    "    pyplot.clabel(\n",
    "        bias_contour_object, inline=True, inline_spacing=BIAS_LABEL_PADDING_PX,\n",
    "        fmt=BIAS_STRING_FORMAT, fontsize=FONT_SIZE)\n",
    "\n",
    "    nan_flags = numpy.logical_or(\n",
    "        numpy.isnan(success_ratio_by_threshold), numpy.isnan(pod_by_threshold)\n",
    "    )\n",
    "\n",
    "    if not numpy.all(nan_flags):\n",
    "        real_indices = numpy.where(numpy.invert(nan_flags))[0]\n",
    "        axes_object.plot(\n",
    "            success_ratio_by_threshold[real_indices],\n",
    "            pod_by_threshold[real_indices], color=line_colour,\n",
    "            linestyle='solid', linewidth=line_width)\n",
    "\n",
    "    axes_object.set_xlabel('Success ratio (1 - FAR)')\n",
    "    axes_object.set_ylabel('POD (probability of detection)')\n",
    "    axes_object.set_xlim(0., 1.)\n",
    "    axes_object.set_ylim(0., 1.)\n",
    "\n",
    "    return pod_by_threshold, success_ratio_by_threshold"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Set training event frequency\n",
    "training_event_frequency = np.mean(\n",
    "    train_y_cs['Binary_EF'].values)\n",
    "training_event_frequency"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zCa8t-upscEE",
    "outputId": "ecf1ab88-90cd-4764-999c-9aedf82a3269",
    "scrolled": false
   },
   "source": [
    "#ML Pipeline with RFE for all analyses except SHAP\n",
    "train_sizes = [50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160]\n",
    "\n",
    "training_event_frequency = np.mean(\n",
    "    train_y_cs['Binary_EF'].values)\n",
    "\n",
    "#Permutation Test 2\n",
    "n_uncorrelated_features = 1000\n",
    "rng = np.random.RandomState(seed=4)\n",
    "# Use same number of samples as in iris and 1000 features\n",
    "X_rand = rng.normal(size=(200, n_uncorrelated_features))\n",
    "\n",
    "ran=4\n",
    "# configure the cross-validation procedure\n",
    "outer_cv = RepeatedStratifiedKFold(n_splits=5,n_repeats=3, random_state=ran)\n",
    "# enumerate splits\n",
    "outer_acc = list()\n",
    "outer_jac = list()\n",
    "outer_brier = list()\n",
    "outer_ll = list()\n",
    "outer_mas = list()\n",
    "outer_mse = list()\n",
    "outer_prec = list()\n",
    "outer_recall = list()\n",
    "outer_auc = list()\n",
    "outer_f1 = list()\n",
    "outer_f2 = list()\n",
    "outer_TP = list()\n",
    "outer_FP = list()\n",
    "outer_FN = list()\n",
    "outer_TN = list()\n",
    "outer_FPR = list()\n",
    "outer_FNR = list()\n",
    "outer_TNR = list()\n",
    "outer_NPV = list()\n",
    "outer_FDR = list()\n",
    "outer_cohen_kappa = list()\n",
    "outer_matt_cc = list()\n",
    "outer_avg_prec_auc = list()\n",
    "\n",
    "outer_train_scores_LC_ac = list()\n",
    "outer_val_scores_LC_ac = list()\n",
    "outer_train_scores_LC_ll = list()\n",
    "outer_val_scores_LC_ll = list()\n",
    "\n",
    "#ROC Curve Initialization\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "i=0\n",
    "\n",
    "#PR Curve Initialization\n",
    "y_real = []\n",
    "y_proba = []\n",
    "j=0\n",
    "\n",
    "#Initialize plots for ROC and P-R Curves\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=[40,20])\n",
    "\n",
    "#Permutation Tests\n",
    "score = []\n",
    "perm_scores = []\n",
    "pvalue = []\n",
    "\n",
    "score_rand = []\n",
    "perm_scores_rand = []\n",
    "pvalue_rand = []\n",
    "\n",
    "#Initiate incorrect counts\n",
    "FN_EF_total = []\n",
    "FP_EF_total = []\n",
    "\n",
    "FN_mode_total = []\n",
    "FP_mode_total = []\n",
    "\n",
    "#Calibration Curve\n",
    "fop_ = []\n",
    "mpv_ = []\n",
    "\n",
    "yprobs = []\n",
    "\n",
    "for train_ix, test_ix in outer_cv.split(X,Y.values.ravel()):\n",
    "\t  # split data\n",
    "    X_train, X_test = X.iloc[train_ix, :], X.iloc[test_ix, :]\n",
    "    y_train, y_test = Y.values.ravel()[train_ix], Y.values.ravel()[test_ix]\n",
    "    # configure the cross-validation procedure\n",
    "    inner_cv = StratifiedKFold(3, shuffle=True, random_state=ran)\n",
    "    # define the model\n",
    "    scaler = preprocessing.RobustScaler()\n",
    "    #rfe = RFE(estimator=LogisticRegression(solver='liblinear', max_iter=1000000), n_features_to_select=2)\n",
    "    rfe = RFECV(estimator=LogisticRegression(solver='liblinear', max_iter=100000),cv=inner_cv, min_features_to_select=1)\n",
    "    model = RandomForestClassifier(max_features='sqrt', random_state=ran)\n",
    "    calibrated = CalibratedClassifierCV(model, method='sigmoid', cv=inner_cv)\n",
    "    pipeline = Pipeline(steps=[('scaler', scaler), ('feature_selection', rfe), ('model', model)])\n",
    "    # define search space\n",
    "    n_estimators = [4, 8, 16, 32, 64, 100, 200]\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = range(1,13,3)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2,3,4,5,6,7,8]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1,2,3,4,5]\n",
    "    grid = dict(model__n_estimators=n_estimators, model__max_depth=max_depth, model__min_samples_split=min_samples_split, model__min_samples_leaf=min_samples_leaf)\n",
    "    # define search\n",
    "    search = RandomizedSearchCV(estimator=pipeline, param_distributions=grid, n_iter=600, cv=inner_cv, n_jobs=-1, scoring='accuracy', refit=True)\n",
    "    # execute search\n",
    "    result = search.fit(X_train, y_train)\n",
    "    #rfe_ob = rfe.fit(X_train, y_train)\n",
    "    #print(rfe_ob.support_)\n",
    "    #print(rfe_ob.ranking_)\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = result.best_estimator_\n",
    "    print(best_model['feature_selection'].ranking_)\n",
    "    # evaluate model on the hold out dataset\n",
    "    yhat = best_model.predict(X_test)\n",
    "    yprob = best_model.predict_proba(X_test)\n",
    "    cnf_matrix = sklearn.metrics.confusion_matrix(y_test, yhat)\n",
    "    TP = cnf_matrix[1,1]\n",
    "    FP = cnf_matrix[0,1]\n",
    "    FN = cnf_matrix[1,0]\n",
    "    TN = cnf_matrix[0,0]\n",
    "    print(cnf_matrix)\n",
    "    \n",
    "    #Calibration Curve\n",
    "    fop, mpv = calibration_curve(y_test, yprob[:,1], n_bins=5)\n",
    "    \n",
    "    #Plot ROC curve\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_test, yprob[:, 1])\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "        \n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = sklearn.metrics.roc_auc_score(y_test, yprob[:,1])\n",
    "    aucs.append(roc_auc)\n",
    "    ax1.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "    i += 1\n",
    "    \n",
    "    #Precision-recall curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, yprob[:, 1])\n",
    "        \n",
    "    #Plotting each individual PR Curve\n",
    "    ax2.plot(recall, precision, lw=1, alpha=0.3, label='PR fold %d (AUC = %0.2f)' % (j, sklearn.metrics.average_precision_score(y_test, yprob[:, 1])))\n",
    "    y_real.append(y_test)\n",
    "    y_proba.append(yprob[:, 1])\n",
    "    j += 1\n",
    "    \n",
    "    #Learning Curves\n",
    "    #accuracy\n",
    "    train_sizes, train_scores1, validation_scores1 = learning_curve(estimator = best_model, X = X_train, y = y_train, train_sizes = train_sizes, cv = inner_cv, scoring = 'accuracy')\n",
    "    #Log loss\n",
    "    train_sizes, train_scores2, validation_scores2 = learning_curve(estimator = best_model, X = X_train, y = y_train, train_sizes = train_sizes, cv = inner_cv, scoring = 'neg_log_loss')\n",
    "\n",
    "    #Permutation Tests\n",
    "    #Permutation Test 1\n",
    "    #score1, perm_scores1, pvalue1 = permutation_test_score(best_model, X = X_train, y = y_train, scoring=\"accuracy\", cv=inner_cv, n_permutations=675)\n",
    "    \n",
    "    #Permutation Test 2\n",
    "    #score_rand1, perm_scores_rand1, pvalue_rand1 = permutation_test_score(best_model, X = X_train, y = y_train, scoring=\"accuracy\", cv=inner_cv, n_permutations=675)\n",
    "    \n",
    "    #Incorrect Counts\n",
    "    #pred_delta=y_test-yhat\n",
    "    #FN_loc = np.where(pred_delta==1)[0]\n",
    "    #FP_loc = np.where(pred_delta==-1)[0]\n",
    "    #locations=X_test.index\n",
    "    #EF_ratings = complete['EF_Rating'][locations]\n",
    "    #F_ratings_loc = EF_ratings.reset_index()\n",
    "    #FN_EF = EF_ratings_loc['EF_Rating'][FN_loc]\n",
    "    #FP_EF = EF_ratings_loc['EF_Rating'][FP_loc]\n",
    "    pred_delta=y_test-yhat\n",
    "    FN_loc = np.where(pred_delta==1)[0]\n",
    "    FP_loc = np.where(pred_delta==-1)[0]\n",
    "    locations=X_test.index\n",
    "    Modes = complete['Mode'][locations]\n",
    "    Modes_loc = Modes.reset_index()\n",
    "    FN_modes = Modes_loc['Mode'][FN_loc]\n",
    "    FP_modes = Modes_loc['Mode'][FP_loc]\n",
    "\n",
    "    \n",
    "    # evaluate the model\n",
    "    acc = sklearn.metrics.accuracy_score(y_test, yhat)\n",
    "    jac = sklearn.metrics.jaccard_score(y_test, yhat)\n",
    "    brier = sklearn.metrics.brier_score_loss(y_test, yprob[:,1])\n",
    "    ll = sklearn.metrics.log_loss(y_test, yprob[:,1])\n",
    "    mas = sklearn.metrics.mean_absolute_error(y_test, yhat)\n",
    "    mse = sklearn.metrics.mean_squared_error(y_test, yhat)\n",
    "    prec = sklearn.metrics.precision_score(y_test, yhat)\n",
    "    recall = sklearn.metrics.recall_score(y_test, yhat)\n",
    "    auc = sklearn.metrics.roc_auc_score(y_test, yprob[:,1])\n",
    "    f1 = sklearn.metrics.f1_score(y_test, yhat)\n",
    "    f2 = sklearn.metrics.fbeta_score(y_test, yhat, beta=2)\n",
    "    FPR = FP/(FP+TN)\n",
    "    FNR = FN/(TP+FN)\n",
    "    TNR = TN/(TN+FP)\n",
    "    NPV = TN/(TN+FN)\n",
    "    FDR = FP/(TP+FP)\n",
    "    cohen_kappa = sklearn.metrics.cohen_kappa_score(y_test, yhat)\n",
    "    matt_cc = sklearn.metrics.matthews_corrcoef(y_test, yhat)\n",
    "    avg_prec_auc = sklearn.metrics.average_precision_score(y_test, yprob[:,1])\n",
    "    # store the result\n",
    "    outer_acc.append(acc)\n",
    "    outer_jac.append(jac)\n",
    "    outer_brier.append(brier)\n",
    "    outer_ll.append(ll)\n",
    "    outer_mas.append(mas)\n",
    "    outer_mse.append(mse)\n",
    "    outer_prec.append(prec)\n",
    "    outer_recall.append(recall)\n",
    "    outer_auc.append(auc)\n",
    "    outer_f1.append(f1)\n",
    "    outer_f2.append(f2)\n",
    "    outer_TP.append(TP)\n",
    "    outer_FP.append(FP)\n",
    "    outer_FN.append(FN)\n",
    "    outer_TN.append(TN)\n",
    "    outer_FPR.append(FPR)\n",
    "    outer_FNR.append(FNR)\n",
    "    outer_TNR.append(TNR)\n",
    "    outer_NPV.append(NPV)\n",
    "    outer_FDR.append(FDR)\n",
    "    outer_cohen_kappa.append(cohen_kappa)\n",
    "    outer_matt_cc.append(matt_cc)\n",
    "    outer_avg_prec_auc.append(avg_prec_auc)\n",
    "    \n",
    "    outer_train_scores_LC_ac.append(train_scores1)\n",
    "    outer_train_scores_LC_ll.append(train_scores2)\n",
    "    outer_val_scores_LC_ac.append(validation_scores1)\n",
    "    outer_val_scores_LC_ll.append(validation_scores2)\n",
    "    \n",
    "    #score.append(score1)\n",
    "    #perm_scores.append(perm_scores1)\n",
    "    #pvalue.append(pvalue1)\n",
    "    \n",
    "    #score_rand.append(score_rand1)\n",
    "    #perm_scores_rand.append(perm_scores_rand1)\n",
    "    #pvalue_rand.append(pvalue_rand1)\n",
    "    \n",
    "    #FN_EF_total.append(FN_EF.values)\n",
    "    #FP_EF_total.append(FP_EF.values)\n",
    "    \n",
    "    FN_mode_total.append(FN_modes.values)\n",
    "    FP_mode_total.append(FP_modes.values)\n",
    "    \n",
    "    fop_.append(fop)\n",
    "    mpv_.append(mpv)\n",
    "    \n",
    "    yprobs.append(yprob)\n",
    "    \n",
    "    # report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))\n",
    "    \n",
    "# summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(outer_acc), np.std(outer_acc)))\n",
    "print(np.mean(outer_acc))\n",
    "print(np.mean(outer_jac))\n",
    "print(np.mean(outer_brier))\n",
    "print(np.mean(outer_ll))\n",
    "print(np.mean(outer_mas))\n",
    "print(np.mean(outer_mse))\n",
    "print(np.mean(outer_prec))\n",
    "print(np.mean(outer_recall))\n",
    "print(np.mean(outer_auc))\n",
    "print(np.mean(outer_f1))\n",
    "print(np.mean(outer_f2))\n",
    "print(np.mean(outer_FPR))\n",
    "print(np.mean(outer_FNR))\n",
    "print(np.mean(outer_TNR))\n",
    "print(np.mean(outer_NPV))\n",
    "print(np.mean(outer_FDR))\n",
    "print(np.mean(outer_cohen_kappa))\n",
    "print(np.mean(outer_matt_cc))\n",
    "print(np.mean(outer_avg_prec_auc))\n",
    "\n",
    "print(np.std(outer_acc))\n",
    "print(np.std(outer_jac))\n",
    "print(np.std(outer_brier))\n",
    "print(np.std(outer_ll))\n",
    "print(np.std(outer_mas))\n",
    "print(np.std(outer_mse))\n",
    "print(np.std(outer_prec))\n",
    "print(np.std(outer_recall))\n",
    "print(np.std(outer_auc))\n",
    "print(np.std(outer_f1))\n",
    "print(np.std(outer_f2))\n",
    "print(np.std(outer_FPR))\n",
    "print(np.std(outer_FNR))\n",
    "print(np.std(outer_TNR))\n",
    "print(np.std(outer_NPV))\n",
    "print(np.std(outer_FDR))\n",
    "print(np.std(outer_cohen_kappa))\n",
    "print(np.std(outer_matt_cc))\n",
    "print(np.std(outer_avg_prec_auc))\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(np.sum(outer_TP))\n",
    "print(np.sum(outer_TN))\n",
    "print(np.sum(outer_FP))\n",
    "print(np.sum(outer_FN))\n",
    "\n",
    "#Mean Learning Curves\n",
    "mean_train_ac = np.mean(np.mean(outer_train_scores_LC_ac, axis=2), axis=0)\n",
    "mean_train_ll = np.mean(np.mean(outer_train_scores_LC_ll, axis=2),axis=0)\n",
    "mean_val_ac = np.mean(np.mean(outer_val_scores_LC_ac, axis=2),axis=0)\n",
    "mean_val_ll = np.mean(np.mean(outer_val_scores_LC_ll, axis=2),axis=0)\n",
    "\n",
    "#Mean Permutation Tests\n",
    "#Test 1\n",
    "#mean_score_perm = np.mean(score)\n",
    "#mean_perm_scores_in = np.mean(perm_scores)\n",
    "#mean_pvalue_perm = np.mean(pvalue)\n",
    "\n",
    "\n",
    "#Test 2\n",
    "#mean_score_rand_perm = np.mean(score_rand)\n",
    "#mean_perm_scores_rand_in = np.mean(perm_scores_rand, axis=0)\n",
    "#mean_pvalue_rand_perm = np.mean(pvalue_rand)\n",
    "\n",
    "#Incorrect Counts\n",
    "#print('EF2s:',(np.concatenate(FN_EF_total)==2).sum())\n",
    "#print('EF3s:',(np.concatenate(FN_EF_total)==3).sum())\n",
    "#print('EF4s:',(np.concatenate(FN_EF_total)==4).sum())\n",
    "#print('EF5s:',(np.concatenate(FN_EF_total)==5).sum())\n",
    "#print('EF0s:',(np.concatenate(FP_EF_total)==0).sum())\n",
    "#print('EF1s:',(np.concatenate(FP_EF_total)==1).sum())\n",
    "\n",
    "print('FN QLCSs:',(np.concatenate(FN_mode_total)==0).sum())\n",
    "print('FP QLCSs:',(np.concatenate(FP_mode_total)==0).sum())\n",
    "print('FN Discrete:',(np.concatenate(FN_mode_total)==1).sum())\n",
    "print('FP Discrete:',(np.concatenate(FP_mode_total)==1).sum())\n",
    "print('FN Multicell:',(np.concatenate(FN_mode_total)==2).sum())\n",
    "print('FP Multicell:',(np.concatenate(FP_mode_total)==2).sum())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Define data separations for later analyses\n",
    "violent_loc = np.where((complete['EF_Rating'][train_ix]==4) | (complete['EF_Rating'][train_ix]==5))\n",
    "EF1_loc  = np.where((complete['EF_Rating'][train_ix]==1))\n",
    "EF2_loc  = np.where((complete['EF_Rating'][train_ix]==2))\n",
    "dsc_loc = np.where((complete['Mode'][train_ix]==1))\n",
    "qlcs_loc = np.where((complete['Mode'][train_ix]==0))\n",
    "mul_loc = np.where((complete['Mode'][train_ix]==2))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#ML Pipeline without RFE for focus on SHAP Analyses\n",
    "\n",
    "#SET PREDICTORS\n",
    "#Environmental Predictors\n",
    "#predictor_cols = ['SBCAPE_MEAN', 'SBCIN_MEAN', 'MLCAPE_MEAN', 'MLCIN_MEAN', 'MUCAPE_MEAN', '03CAPE_MEAN', 'LI_MEAN', '08bulk_MEAN', '06bulk_MEAN', '03bulk_MEAN', '01bulk_MEAN', '0500bulk_MEAN', 'BR_speed_MEAN', 'meanmotion_MEAN', 'SRH03_MEAN', 'SRH01_MEAN', 'RAPSRH500_MEAN', 'eff_base_h_ma_MEAN', 'eff_top_h_ma_MEAN', 'eff_layer_depth_ma_MEAN', 'EBS_MEAN', 'ESRH_MEAN', 'srwind_02_MEAN', 'srwind_46_MEAN', 'srwind_911_MEAN', 'lapse_36_MEAN', 'lapse_03_MEAN', 'RH_36_MEAN', 'RH_03_MEAN', 'LCL_h_MEAN', 'LFC_h_MEAN', 'LCL_LFC_hdif_MEAN', 'RH_LCL_LFC_MEAN', 'EHI01_MEAN', 'EHI03_MEAN', 'SCP_fixed_MEAN', 'SCP_eff_MEAN', 'STP_fixed_MEAN', 'tor01_EHI_MEAN', 'TTS_MEAN', 'critang_MEAN']\n",
    "#predictor_cols = ['Avg_Meso_Distance_(Km)','Discrete', 'QLCS', 'Multi',\t'Peak_(m/s)', 'Distance(km)', 'SBCAPE_MEAN', 'SBCIN_MEAN', 'MLCAPE_MEAN', 'MLCIN_MEAN', 'MUCAPE_MEAN', '03CAPE_MEAN', 'LI_MEAN', '08bulk_MEAN', '06bulk_MEAN', '03bulk_MEAN', '01bulk_MEAN', '0500bulk_MEAN', 'BR_speed_MEAN', 'meanmotion_MEAN', 'SRH03_MEAN', 'SRH01_MEAN', 'RAPSRH500_MEAN', 'eff_base_h_ma_MEAN', 'eff_top_h_ma_MEAN', 'eff_layer_depth_ma_MEAN', 'EBS_MEAN', 'ESRH_MEAN', 'srwind_02_MEAN', 'srwind_46_MEAN', 'srwind_911_MEAN', 'lapse_36_MEAN', 'lapse_03_MEAN', 'RH_36_MEAN', 'RH_03_MEAN', 'LCL_h_MEAN', 'LFC_h_MEAN', 'LCL_LFC_hdif_MEAN', 'RH_LCL_LFC_MEAN', 'EHI01_MEAN', 'EHI03_MEAN', 'SCP_fixed_MEAN', 'SCP_eff_MEAN', 'STP_fixed_MEAN', 'tor01_EHI_MEAN', 'TTS_MEAN', 'critang_MEAN']\n",
    "#names = ['Average Pretornadic Mesocyclone Width (km)', 'Discrete Mode', 'QLCS Mode', 'Multicell Mode', 'Peak Pretornadic Mesocyclone Intensity (m/s)', 'Distance from Radar (km)', 'SBCAPE_MEAN (J/kg)', 'SBCIN_MEAN (J/kg)', 'MLCAPE_MEAN (J/kg)', 'MLCIN_MEAN (J/kg)', 'MUCAPE_MEAN (J/kg)', '03CAPE_MEAN (J/kg)', 'LI_MEAN (C)', '08bulk_MEAN (m/s)', '06bulk_MEAN (m/s)', '03bulk_MEAN (m/s)', '01bulk_MEAN (m/s)', '0500bulk_MEAN (m/s)', 'BR_speed_MEAN (m/s)', 'meanmotion_MEAN (m/s)', 'SRH03_MEAN (m^2/s^2)', 'SRH01_MEAN (m^2/s^2)', 'RAPSRH500_MEAN (m^2/s^2)', 'eff_base_h_MEAN (m)', 'eff_top_h_MEAN (m)', 'eff_layer_depth_MEAN (m)', 'EBS_MEAN (m/s)', 'ESRH_MEAN (m^2/s^2)', 'srwind_02_MEAN (m/s)', 'srwind_46_MEAN (m/s)', 'srwind_911_MEAN (m/s)', 'lapse_36_MEAN (C/km)', 'lapse_03_MEAN (C/km)', 'RH_36_MEAN (%)', 'RH_03_MEAN (%)', 'LCL_h_MEAN (m)', 'LFC_h_MEAN (m)', 'LCL_LFC_hdif_MEAN (m)', 'RH_LCL_LFC_MEAN (%)', 'EHI01_MEAN', 'EHI03_MEAN', 'SCP_fixed_MEAN', 'SCP_eff_MEAN', 'STP_fixed_MEAN', 'tor01_EHI_MEAN', 'TTS_MEAN', 'critang_MEAN ()']\n",
    "#names = ['Average Pretornadic Mesocyclone Width (km)', 'Discrete Mode', 'QLCS Mode', 'Multicell Mode', 'Peak Pretornadic Mesocyclone Intensity (m/s)', 'Distance from Radar (km)']\n",
    "\n",
    "#predictor_cols = ['SBCAPE_MEAN', 'SBCIN_MEAN', 'MLCAPE_MEAN', 'MLCIN_MEAN', 'MUCAPE_MEAN', '03CAPE_MEAN', 'LI_MEAN', '08bulk_MEAN', '06bulk_MEAN', '03bulk_MEAN', '01bulk_MEAN', '0500bulk_MEAN', 'BR_speed_MEAN', 'meanmotion_MEAN', 'SRH03_MEAN', 'SRH01_MEAN', 'RAPSRH500_MEAN', 'eff_base_h_ma_MEAN', 'eff_top_h_ma_MEAN', 'eff_layer_depth_ma_MEAN', 'EBS_MEAN', 'ESRH_MEAN', 'srwind_02_MEAN', 'srwind_46_MEAN', 'srwind_911_MEAN', 'lapse_36_MEAN', 'lapse_03_MEAN', 'RH_36_MEAN', 'RH_03_MEAN', 'LCL_h_MEAN', 'LFC_h_MEAN', 'LCL_LFC_hdif_MEAN', 'RH_LCL_LFC_MEAN', 'EHI01_MEAN', 'EHI03_MEAN', 'SCP_fixed_MEAN', 'SCP_eff_MEAN', 'STP_fixed_MEAN', 'tor01_EHI_MEAN', 'TTS_MEAN', 'critang_MEAN']\n",
    "#names = ['SBCAPE_MEAN (J/kg)', 'SBCIN_MEAN (J/kg)', 'MLCAPE_MEAN (J/kg)', 'MLCIN_MEAN (J/kg)', 'MUCAPE_MEAN (J/kg)', '03CAPE_MEAN (J/kg)', 'LI_MEAN (C)', '08bulk_MEAN (m/s)', '06bulk_MEAN (m/s)', '03bulk_MEAN (m/s)', '01bulk_MEAN (m/s)', '0500bulk_MEAN (m/s)', 'BR_speed_MEAN (m/s)', 'meanmotion_MEAN (m/s)', 'SRH03_MEAN (m^2/s^2)', 'SRH01_MEAN (m^2/s^2)', 'RAPSRH500_MEAN (m^2/s^2)', 'eff_base_h_MEAN (m)', 'eff_top_h_MEAN (m)', 'eff_layer_depth_MEAN (m)', 'EBS_MEAN (m/s)', 'ESRH_MEAN (m^2/s^2)', 'srwind_02_MEAN (m/s)', 'srwind_46_MEAN (m/s)', 'srwind_911_MEAN (m/s)', 'lapse_36_MEAN (C/km)', 'lapse_03_MEAN (C/km)', 'RH_36_MEAN (%)', 'RH_03_MEAN (%)', 'LCL_h_MEAN (m)', 'LFC_h_MEAN (m)', 'LCL_LFC_hdif_MEAN (m)', 'RH_LCL_LFC_MEAN (%)', 'EHI01_MEAN', 'EHI03_MEAN', 'SCP_fixed_MEAN', 'SCP_eff_MEAN', 'STP_fixed_MEAN', 'tor01_EHI_MEAN', 'TTS_MEAN', 'critang_MEAN ()']\n",
    "\n",
    "\n",
    "#predictor_cols = ['Avg_Meso_Distance_(Km)','Discrete', 'QLCS', 'Multi',\t'Peak_(m/s)', 'Distance(km)', 'SBCAPE_MEAN', 'SBCIN_MEAN', 'MLCAPE_MEAN', 'MLCIN_MEAN', 'MUCAPE_MEAN', '03CAPE_MEAN', 'LI_MEAN', '08bulk_MEAN', '06bulk_MEAN', '03bulk_MEAN', '01bulk_MEAN', '0500bulk_MEAN', 'BR_speed_MEAN', 'meanmotion_MEAN', 'SRH03_MEAN', 'SRH01_MEAN', 'RAPSRH500_MEAN', 'eff_base_h_ma_MEAN', 'eff_top_h_ma_MEAN', 'eff_layer_depth_ma_MEAN', 'EBS_MEAN', 'ESRH_MEAN', 'srwind_02_MEAN', 'srwind_46_MEAN', 'srwind_911_MEAN', 'lapse_36_MEAN', 'lapse_03_MEAN', 'RH_36_MEAN', 'RH_03_MEAN', 'LCL_h_MEAN', 'LFC_h_MEAN', 'LCL_LFC_hdif_MEAN', 'RH_LCL_LFC_MEAN', 'EHI01_MEAN', 'EHI03_MEAN', 'SCP_fixed_MEAN', 'SCP_eff_MEAN', 'STP_fixed_MEAN', 'tor01_EHI_MEAN', 'TTS_MEAN', 'critang_MEAN']\n",
    "#names = ['Average Pretornadic Mesocyclone Width (km)', 'Discrete Mode', 'QLCS Mode', 'Multicell Mode', 'Peak Pretornadic Mesocyclone Intensity (m/s)', 'Distance from Radar (km)', 'SBCAPE_MEAN (J/kg)', 'SBCIN_MEAN (J/kg)', 'MLCAPE_MEAN (J/kg)', 'MLCIN_MEAN (J/kg)', 'MUCAPE_MEAN (J/kg)', '03CAPE_MEAN (J/kg)', 'LI_MEAN (C)', '08bulk_MEAN (m/s)', '06bulk_MEAN (m/s)', '03bulk_MEAN (m/s)', '01bulk_MEAN (m/s)', '0500bulk_MEAN (m/s)', 'BR_speed_MEAN (m/s)', 'meanmotion_MEAN (m/s)', 'SRH03_MEAN (m^2/s^2)', 'SRH01_MEAN (m^2/s^2)', 'RAPSRH500_MEAN (m^2/s^2)', 'eff_base_h_MEAN (m)', 'eff_top_h_MEAN (m)', 'eff_layer_depth_MEAN (m)', 'EBS_MEAN (m/s)', 'ESRH_MEAN (m^2/s^2)', 'srwind_02_MEAN (m/s)', 'srwind_46_MEAN (m/s)', 'srwind_911_MEAN (m/s)', 'lapse_36_MEAN (C/km)', 'lapse_03_MEAN (C/km)', 'RH_36_MEAN (%)', 'RH_03_MEAN (%)', 'LCL_h_MEAN (m)', 'LFC_h_MEAN (m)', 'LCL_LFC_hdif_MEAN (m)', 'RH_LCL_LFC_MEAN (%)', 'EHI01_MEAN', 'EHI03_MEAN', 'SCP_fixed_MEAN', 'SCP_eff_MEAN', 'STP_fixed_MEAN', 'tor01_EHI_MEAN', 'TTS_MEAN', 'critang_MEAN ()']\n",
    "\n",
    "predictor_cols = ['Avg_Meso_Distance_(Km)','Peak_(m/s)', 'Distance(km)', 'SBCAPE_MEAN', 'MLCAPE_MEAN', 'MLCIN_MEAN', 'MUCAPE_MEAN', 'LI_MEAN', '08bulk_MEAN', '06bulk_MEAN', '03bulk_MEAN', '01bulk_MEAN', '0500bulk_MEAN', 'BR_speed_MEAN', 'meanmotion_MEAN', 'SRH03_MEAN', 'SRH01_MEAN', 'RAPSRH500_MEAN', 'eff_base_h_ma_MEAN', 'eff_layer_depth_ma_MEAN', 'EBS_MEAN', 'ESRH_MEAN', 'lapse_36_MEAN', 'lapse_03_MEAN', 'RH_36_MEAN', 'RH_03_MEAN', 'LFC_h_MEAN', 'LCL_LFC_hdif_MEAN', 'RH_LCL_LFC_MEAN', 'EHI01_MEAN', 'EHI03_MEAN', 'SCP_fixed_MEAN', 'SCP_eff_MEAN', 'STP_fixed_MEAN', 'tor01_EHI_MEAN', 'TTS_MEAN']\n",
    "names = ['Average Pretornadic Mesocyclone Width (km)', 'Peak Pretornadic Mesocyclone Intensity (m/s)', 'Distance from Radar (km)', 'SBCAPE_MEAN (J/kg)', 'MLCAPE_MEAN (J/kg)', 'MLCIN_MEAN (J/kg)', 'MUCAPE_MEAN (J/kg)', 'LI_MEAN (C)', '08bulk_MEAN (m/s)', '06bulk_MEAN (m/s)', '03bulk_MEAN (m/s)', '01bulk_MEAN (m/s)', '0500bulk_MEAN (m/s)', 'BR_speed_MEAN (m/s)', 'meanmotion_MEAN (m/s)', 'SRH03_MEAN (m^2/s^2)', 'SRH01_MEAN (m^2/s^2)', 'RAPSRH500_MEAN (m^2/s^2)', 'eff_base_h_MEAN (m)', 'eff_layer_depth_MEAN (m)', 'EBS_MEAN (m/s)', 'ESRH_MEAN (m^2/s^2)', 'lapse_36_MEAN (C/km)', 'lapse_03_MEAN (C/km)', 'RH_36_MEAN (%)', 'RH_03_MEAN (%)', 'LFC_h_MEAN (m)', 'LCL_LFC_hdif_MEAN (m)', 'RH_LCL_LFC_MEAN (%)', 'EHI01_MEAN', 'EHI03_MEAN', 'SCP_fixed_MEAN', 'SCP_eff_MEAN', 'STP_fixed_MEAN', 'tor01_EHI_MEAN', 'TTS_MEAN']\n",
    "\n",
    "\n",
    "#predictor_cols = ['MLCAPE_MEAN', '06bulk_MEAN', '01bulk_MEAN', 'EBS_MEAN', 'ESRH_MEAN', 'lapse_36_MEAN', 'RH_36_MEAN', 'SCP_fixed_MEAN', 'STP_fixed_MEAN', 'TTS_MEAN']\n",
    "#predictor_cols = ['SBCAPE_MEAN', 'SBCIN_MEAN', 'MLCAPE_MEAN', 'MLCIN_MEAN', 'MUCAPE_MEAN', '03CAPE_MEAN', 'LI_MEAN', '08bulk_MEAN', '06bulk_MEAN', '03bulk_MEAN', '01bulk_MEAN', '0500bulk_MEAN', 'BR_speed_MEAN', 'meanmotion_MEAN', 'SRH03_MEAN', 'SRH01_MEAN', 'RAPSRH500_MEAN', 'eff_base_h_ma_MEAN', 'eff_top_h_ma_MEAN', 'eff_layer_depth_ma_MEAN', 'EBS_MEAN', 'ESRH_MEAN', 'srwind_02_MEAN', 'srwind_46_MEAN', 'srwind_911_MEAN', 'lapse_36_MEAN', 'lapse_03_MEAN', 'RH_36_MEAN', 'RH_03_MEAN', 'LCL_h_MEAN', 'LFC_h_MEAN', 'LCL_LFC_hdif_MEAN', 'RH_LCL_LFC_MEAN', 'critang_MEAN']\n",
    "#predictor_cols = ['MLCAPE_MEAN', 'eff_layer_depth_ma_MEAN', 'lapse_36_MEAN', 'RH_36_MEAN']\n",
    "#predictor_cols = ['06bulk_MEAN', 'EBS_MEAN', 'ESRH_MEAN']\n",
    "#predictor_cols = ['SCP_fixed_MEAN', 'STP_fixed_MEAN', 'TTS_MEAN']\n",
    "#predictor_cols = ['SCP_fixed_MEAN', 'STP_fixed_MEAN', 'TTS_MEAN', 'EBS_MEAN', 'ESRH_MEAN', '06bulk_MEAN', 'lapse_36_MEAN', 'RH_36_MEAN', 'MLCAPE_MEAN']\n",
    "\n",
    "\n",
    "target_col = ['Binary_EF']\n",
    "X = complete[predictor_cols]\n",
    "Y = complete[target_col]\n",
    "#Long code version\n",
    "train_sizes = [50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160]\n",
    "\n",
    "#Permutation Test 2\n",
    "n_uncorrelated_features = 1000\n",
    "rng = np.random.RandomState(seed=4)\n",
    "# Use same number of samples as in iris and 1000 features\n",
    "X_rand = rng.normal(size=(200, n_uncorrelated_features))\n",
    "\n",
    "ran=4\n",
    "# configure the cross-validation procedure\n",
    "outer_cv = RepeatedStratifiedKFold(n_splits=5,n_repeats=3, random_state=ran)\n",
    "# enumerate splits\n",
    "outer_acc = list()\n",
    "outer_jac = list()\n",
    "outer_brier = list()\n",
    "outer_ll = list()\n",
    "outer_mas = list()\n",
    "outer_mse = list()\n",
    "outer_prec = list()\n",
    "outer_recall = list()\n",
    "outer_auc = list()\n",
    "outer_f1 = list()\n",
    "outer_f2 = list()\n",
    "outer_TP = list()\n",
    "outer_FP = list()\n",
    "outer_FN = list()\n",
    "outer_TN = list()\n",
    "outer_FPR = list()\n",
    "outer_FNR = list()\n",
    "outer_TNR = list()\n",
    "outer_NPV = list()\n",
    "outer_FDR = list()\n",
    "outer_cohen_kappa = list()\n",
    "outer_matt_cc = list()\n",
    "outer_avg_prec_auc = list()\n",
    "\n",
    "outer_train_scores_LC_ac = list()\n",
    "outer_val_scores_LC_ac = list()\n",
    "outer_train_scores_LC_ll = list()\n",
    "outer_val_scores_LC_ll = list()\n",
    "\n",
    "#ROC Curve Initialization\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "i=0\n",
    "\n",
    "#PR Curve Initialization\n",
    "y_real = []\n",
    "y_proba = []\n",
    "j=0\n",
    "\n",
    "#Initialize plots for ROC and P-R Curves\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=[40,20])\n",
    "\n",
    "#Permutation Tests\n",
    "score = []\n",
    "perm_scores = []\n",
    "pvalue = []\n",
    "\n",
    "score_rand = []\n",
    "perm_scores_rand = []\n",
    "pvalue_rand = []\n",
    "\n",
    "#Initiate incorrect counts\n",
    "FN_EF_total = []\n",
    "FP_EF_total = []\n",
    "\n",
    "FN_mode_total = []\n",
    "FP_mode_total = []\n",
    "\n",
    "#Calibration Curve\n",
    "fop_ = []\n",
    "mpv_ = []\n",
    "\n",
    "yprobs = []\n",
    "\n",
    "#SHAP\n",
    "shap_values = []\n",
    "test_sets = []\n",
    "\n",
    "values_shap = []\n",
    "base_shap = []\n",
    "pred_shap = []\n",
    "\n",
    "values_shap_EF3 = []\n",
    "base_shap_EF3 = []\n",
    "pred_shap_EF3 = []\n",
    "X_EF3 = []\n",
    "\n",
    "values_shap_EF4 = []\n",
    "base_shap_EF4 = []\n",
    "pred_shap_EF4 = []\n",
    "X_EF4 = []\n",
    "\n",
    "values_shap_EF5 = []\n",
    "base_shap_EF5 = []\n",
    "pred_shap_EF5 = []\n",
    "X_EF5 = []\n",
    "\n",
    "values_shap_nonsig = []\n",
    "base_shap_nonsig = []\n",
    "pred_shap_nonsig = []\n",
    "\n",
    "values_shap_sig = []\n",
    "base_shap_sig = []\n",
    "pred_shap_sig = []\n",
    "\n",
    "values_shap_vio = []\n",
    "base_shap_vio = []\n",
    "pred_shap_vio = []\n",
    "X_vio = []\n",
    "\n",
    "values_shap_EF0 = []\n",
    "base_shap_EF0 = []\n",
    "pred_shap_EF0 = []\n",
    "X_EF0 = []\n",
    "\n",
    "values_shap_EF1 = []\n",
    "base_shap_EF1 = []\n",
    "pred_shap_EF1 = []\n",
    "X_EF1 = []\n",
    "\n",
    "values_shap_EF2 = []\n",
    "base_shap_EF2 = []\n",
    "pred_shap_EF2 = []\n",
    "X_EF2 = []\n",
    "\n",
    "values_shap_cor = []\n",
    "base_shap_cor = []\n",
    "pred_shap_cor = []\n",
    "X_cor = []\n",
    "\n",
    "values_shap_cor_EF0 = []\n",
    "base_shap_cor_EF0 = []\n",
    "pred_shap_cor_EF0 = []\n",
    "X_cor_EF0 = []\n",
    "\n",
    "values_shap_cor_EF1 = []\n",
    "base_shap_cor_EF1 = []\n",
    "pred_shap_cor_EF1 = []\n",
    "X_cor_EF1 = []\n",
    "\n",
    "values_shap_cor_EF2 = []\n",
    "base_shap_cor_EF2 = []\n",
    "pred_shap_cor_EF2 = []\n",
    "X_cor_EF2 = []\n",
    "\n",
    "values_shap_cor_EF3 = []\n",
    "base_shap_cor_EF3 = []\n",
    "pred_shap_cor_EF3 = []\n",
    "X_cor_EF3 = []\n",
    "\n",
    "values_shap_cor_EF4 = []\n",
    "base_shap_cor_EF4 = []\n",
    "pred_shap_cor_EF4 = []\n",
    "X_cor_EF4 = []\n",
    "\n",
    "values_shap_cor_EF5 = []\n",
    "base_shap_cor_EF5 = []\n",
    "pred_shap_cor_EF5 = []\n",
    "X_cor_EF5 = []\n",
    "\n",
    "values_shap_incor = []\n",
    "base_shap_incor = []\n",
    "pred_shap_incor = []\n",
    "X_incor = []\n",
    "\n",
    "values_shap_incor_EF0 = []\n",
    "base_shap_incor_EF0 = []\n",
    "pred_shap_incor_EF0 = []\n",
    "X_incor_EF0 = []\n",
    "\n",
    "values_shap_incor_EF1 = []\n",
    "base_shap_incor_EF1 = []\n",
    "pred_shap_incor_EF1 = []\n",
    "X_incor_EF1 = []\n",
    "\n",
    "values_shap_incor_EF2 = []\n",
    "base_shap_incor_EF2 = []\n",
    "pred_shap_incor_EF2 = []\n",
    "X_incor_EF2 = []\n",
    "\n",
    "values_shap_incor_EF3 = []\n",
    "base_shap_incor_EF3 = []\n",
    "pred_shap_incor_EF3 = []\n",
    "X_incor_EF3 = []\n",
    "\n",
    "values_shap_incor_EF4 = []\n",
    "base_shap_incor_EF4 = []\n",
    "pred_shap_incor_EF4 = []\n",
    "X_incor_EF4 = []\n",
    "\n",
    "values_shap_incor_EF5 = []\n",
    "base_shap_incor_EF5 = []\n",
    "pred_shap_incor_EF5 = []\n",
    "X_incor_EF5 = []\n",
    "\n",
    "values_shap_dsc = []\n",
    "base_shap_dsc = []\n",
    "pred_shap_dsc = []\n",
    "X_dsc = []\n",
    "\n",
    "values_shap_dsc_EF0 = []\n",
    "base_shap_dsc_EF0 = []\n",
    "pred_shap_dsc_EF0 = []\n",
    "X_dsc_EF0 = []\n",
    "\n",
    "values_shap_dsc_EF1 = []\n",
    "base_shap_dsc_EF1 = []\n",
    "pred_shap_dsc_EF1 = []\n",
    "X_dsc_EF1 = []\n",
    "\n",
    "values_shap_dsc_EF2 = []\n",
    "base_shap_dsc_EF2 = []\n",
    "pred_shap_dsc_EF2 = []\n",
    "X_dsc_EF2 = []\n",
    "\n",
    "values_shap_dsc_EF3 = []\n",
    "base_shap_dsc_EF3 = []\n",
    "pred_shap_dsc_EF3 = []\n",
    "X_dsc_EF3 = []\n",
    "\n",
    "values_shap_dsc_EF4 = []\n",
    "base_shap_dsc_EF4 = []\n",
    "pred_shap_dsc_EF4 = []\n",
    "X_dsc_EF4 = []\n",
    "\n",
    "values_shap_dsc_EF5 = []\n",
    "base_shap_dsc_EF5 = []\n",
    "pred_shap_dsc_EF5 = []\n",
    "X_dsc_EF5 = []\n",
    "\n",
    "values_shap_qlcs = []\n",
    "base_shap_qlcs = []\n",
    "pred_shap_qlcs = []\n",
    "X_qlcs = []\n",
    "\n",
    "values_shap_qlcs_EF0 = []\n",
    "base_shap_qlcs_EF0 = []\n",
    "pred_shap_qlcs_EF0 = []\n",
    "X_qlcs_EF0 = []\n",
    "\n",
    "values_shap_qlcs_EF1 = []\n",
    "base_shap_qlcs_EF1 = []\n",
    "pred_shap_qlcs_EF1 = []\n",
    "X_qlcs_EF1 = []\n",
    "\n",
    "values_shap_qlcs_EF2 = []\n",
    "base_shap_qlcs_EF2 = []\n",
    "pred_shap_qlcs_EF2 = []\n",
    "X_qlcs_EF2 = []\n",
    "\n",
    "values_shap_qlcs_EF3 = []\n",
    "base_shap_qlcs_EF3 = []\n",
    "pred_shap_qlcs_EF3 = []\n",
    "X_qlcs_EF3 = []\n",
    "\n",
    "values_shap_qlcs_EF4 = []\n",
    "base_shap_qlcs_EF4 = []\n",
    "pred_shap_qlcs_EF4 = []\n",
    "X_qlcs_EF4 = []\n",
    "\n",
    "values_shap_qlcs_EF5 = []\n",
    "base_shap_qlcs_EF5 = []\n",
    "pred_shap_qlcs_EF5 = []\n",
    "X_qlcs_EF5 = []\n",
    "\n",
    "values_shap_mul = []\n",
    "base_shap_mul = []\n",
    "pred_shap_mul = []\n",
    "X_mul = []\n",
    "\n",
    "values_shap_mul_EF0 = []\n",
    "base_shap_mul_EF0 = []\n",
    "pred_shap_mul_EF0 = []\n",
    "X_mul_EF0 = []\n",
    "\n",
    "values_shap_mul_EF1 = []\n",
    "base_shap_mul_EF1 = []\n",
    "pred_shap_mul_EF1 = []\n",
    "X_mul_EF1 = []\n",
    "\n",
    "values_shap_mul_EF2 = []\n",
    "base_shap_mul_EF2 = []\n",
    "pred_shap_mul_EF2 = []\n",
    "X_mul_EF2 = []\n",
    "\n",
    "values_shap_mul_EF3 = []\n",
    "base_shap_mul_EF3 = []\n",
    "pred_shap_mul_EF3 = []\n",
    "X_mul_EF3 = []\n",
    "\n",
    "values_shap_mul_EF4 = []\n",
    "base_shap_mul_EF4 = []\n",
    "pred_shap_mul_EF4 = []\n",
    "X_mul_EF4 = []\n",
    "\n",
    "values_shap_mul_EF5 = []\n",
    "base_shap_mul_EF5 = []\n",
    "pred_shap_mul_EF5 = []\n",
    "X_mul_EF5 = []\n",
    "\n",
    "values_shap_FN = []\n",
    "base_shap_FN = []\n",
    "pred_shap_FN = []\n",
    "X_FN = []\n",
    "\n",
    "values_shap_FP = []\n",
    "base_shap_FP = []\n",
    "pred_shap_FP = []\n",
    "X_FP = []\n",
    "\n",
    "values_shap_FP_EF1 = []\n",
    "base_shap_FP_EF1 = []\n",
    "pred_shap_FP_EF1 = []\n",
    "X_FP_EF1 = []\n",
    "\n",
    "values_shap_TP = []\n",
    "base_shap_TP = []\n",
    "pred_shap_TP = []\n",
    "X_TP = []\n",
    "\n",
    "values_shap_TN = []\n",
    "base_shap_TN = []\n",
    "pred_shap_TN = []\n",
    "X_TN = []\n",
    "\n",
    "values_shap_FN_EF2 = []\n",
    "base_shap_FN_EF2 = []\n",
    "pred_shap_FN_EF2 = []\n",
    "X_FN_EF2 = []\n",
    "\n",
    "X_nonsig = []\n",
    "X_sig = []\n",
    "\n",
    "dict_shap = {'Explanation'}\n",
    "\n",
    "#FI coefficients\n",
    "Coef = []\n",
    "\n",
    "all_data = []\n",
    "\n",
    "\n",
    "for train_ix, test_ix in outer_cv.split(X,Y.values.ravel()):\n",
    "\t  # split data\n",
    "    X_train, X_test = X.iloc[train_ix, :], X.iloc[test_ix, :]\n",
    "    y_train, y_test = Y.values.ravel()[train_ix], Y.values.ravel()[test_ix]\n",
    "    #Data separations for additional SHAP analysis\n",
    "    complete_train, complete_test = complete.iloc[train_ix, :], complete.iloc[test_ix, :]\n",
    "    violent_loc = np.where((complete_test['EF_Rating']==4) | (complete_test['EF_Rating']==5))\n",
    "    EF0_loc  = np.where((complete_test['EF_Rating']==0))\n",
    "    EF1_loc  = np.where((complete_test['EF_Rating']==1))\n",
    "    EF2_loc  = np.where((complete_test['EF_Rating']==2))\n",
    "    EF3_loc  = np.where((complete_test['EF_Rating']==3))\n",
    "    EF4_loc  = np.where((complete_test['EF_Rating']==4))\n",
    "    EF5_loc  = np.where((complete_test['EF_Rating']==5))\n",
    "    dsc_loc = np.where((complete_test['Mode']==1))\n",
    "    dsc_EF0_loc = np.where((complete_test['Mode']==1) & (complete_test['EF_Rating']==0))\n",
    "    dsc_EF1_loc = np.where((complete_test['Mode']==1) & (complete_test['EF_Rating']==1))\n",
    "    dsc_EF2_loc = np.where((complete_test['Mode']==1) & (complete_test['EF_Rating']==2))\n",
    "    dsc_EF3_loc = np.where((complete_test['Mode']==1) & (complete_test['EF_Rating']==3))\n",
    "    dsc_EF4_loc = np.where((complete_test['Mode']==1) & (complete_test['EF_Rating']==4))\n",
    "    dsc_EF5_loc = np.where((complete_test['Mode']==1) & (complete_test['EF_Rating']==5))\n",
    "    qlcs_loc = np.where((complete_test['Mode']==0))\n",
    "    qlcs_EF0_loc = np.where((complete_test['Mode']==0) & (complete_test['EF_Rating']==0))\n",
    "    qlcs_EF1_loc = np.where((complete_test['Mode']==0) & (complete_test['EF_Rating']==1))\n",
    "    qlcs_EF2_loc = np.where((complete_test['Mode']==0) & (complete_test['EF_Rating']==2))\n",
    "    qlcs_EF3_loc = np.where((complete_test['Mode']==0) & (complete_test['EF_Rating']==3))\n",
    "    qlcs_EF4_loc = np.where((complete_test['Mode']==0) & (complete_test['EF_Rating']==4))\n",
    "    qlcs_EF5_loc = np.where((complete_test['Mode']==0) & (complete_test['EF_Rating']==5))\n",
    "    mul_loc = np.where((complete_test['Mode']==2))\n",
    "    mul_EF0_loc = np.where((complete_test['Mode']==2) & (complete_test['EF_Rating']==0))\n",
    "    mul_EF1_loc = np.where((complete_test['Mode']==2) & (complete_test['EF_Rating']==1))\n",
    "    mul_EF2_loc = np.where((complete_test['Mode']==2) & (complete_test['EF_Rating']==2))\n",
    "    mul_EF3_loc = np.where((complete_test['Mode']==2) & (complete_test['EF_Rating']==3))\n",
    "    mul_EF4_loc = np.where((complete_test['Mode']==2) & (complete_test['EF_Rating']==4))\n",
    "    mul_EF5_loc = np.where((complete_test['Mode']==2) & (complete_test['EF_Rating']==5))\n",
    "    \n",
    "    \n",
    "    non_sig_loc=np.where(y_test==0)\n",
    "    sig_loc=np.where(y_test==1)\n",
    "    #violent_loc\n",
    "    scaler = preprocessing.RobustScaler()\n",
    "    scaler.fit(X_train)\n",
    "    training_predictor = scaler.transform(X_train)\n",
    "    test_predictor = scaler.transform(X_test)\n",
    "    # configure the cross-validation procedure\n",
    "    inner_cv = StratifiedKFold(3, shuffle=True, random_state=ran)\n",
    "    # define the model\n",
    "    scaler = preprocessing.RobustScaler()\n",
    "    #rfe = RFE(estimator=LogisticRegression(solver='liblinear', max_iter=1000000), n_features_to_select=2)\n",
    "    rfe = RFECV(estimator=LogisticRegression(solver='liblinear', max_iter=100000),cv=inner_cv, min_features_to_select=1)\n",
    "    model = RandomForestClassifier(max_features='sqrt', random_state=ran)\n",
    "    calibrated = CalibratedClassifierCV(model, method='sigmoid', cv=inner_cv)\n",
    "    pipeline = Pipeline(steps=[('scaler', scaler), ('model', model)])\n",
    "    # define search space\n",
    "    n_estimators = [4, 8, 16, 32, 64, 100, 200]\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = range(1,13,3)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2,3,4,5,6,7,8]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1,2,3,4,5]\n",
    "    grid = dict(model__n_estimators=n_estimators, model__max_depth=max_depth, model__min_samples_split=min_samples_split, model__min_samples_leaf=min_samples_leaf)\n",
    "    # define search\n",
    "    search = RandomizedSearchCV(estimator=pipeline, param_distributions=grid, n_iter=600, cv=inner_cv, n_jobs=-1, scoring='accuracy', refit=True, random_state=ran)\n",
    "    # execute search\n",
    "    result = search.fit(X_train, y_train)\n",
    "    #rfe_ob = rfe.fit(X_train, y_train)\n",
    "    #print(rfe_ob.support_)\n",
    "    #print(rfe_ob.ranking_)\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = result.best_estimator_\n",
    "    #print(best_model['feature_selection'].ranking_)\n",
    "    # evaluate model on the hold out dataset\n",
    "    yhat = best_model.predict(X_test)\n",
    "    yprob = best_model.predict_proba(X_test)\n",
    "    cnf_matrix = sklearn.metrics.confusion_matrix(y_test, yhat)\n",
    "    TP = cnf_matrix[1,1]\n",
    "    FP = cnf_matrix[0,1]\n",
    "    FN = cnf_matrix[1,0]\n",
    "    TN = cnf_matrix[0,0]\n",
    "    print(cnf_matrix)\n",
    "    \n",
    "    #Calibration Curve\n",
    "    fop, mpv = calibration_curve(y_test, yprob[:,1], n_bins=5)\n",
    "    \n",
    "    #Plot ROC curve\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_test, yprob[:, 1])\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "        \n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = sklearn.metrics.roc_auc_score(y_test, yprob[:,1])\n",
    "    aucs.append(roc_auc)\n",
    "    ax1.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "    i += 1\n",
    "    \n",
    "    #Precision-recall curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, yprob[:, 1])\n",
    "        \n",
    "    #Plotting each individual PR Curve\n",
    "    ax2.plot(recall, precision, lw=1, alpha=0.3, label='PR fold %d (AUC = %0.2f)' % (j, sklearn.metrics.average_precision_score(y_test, yprob[:, 1])))\n",
    "    y_real.append(y_test)\n",
    "    y_proba.append(yprob[:, 1])\n",
    "    j += 1\n",
    "    \n",
    "    #Learning Curves\n",
    "    #accuracy\n",
    "    train_sizes, train_scores1, validation_scores1 = learning_curve(estimator = best_model, X = X_train, y = y_train, train_sizes = train_sizes, cv = inner_cv, scoring = 'accuracy')\n",
    "    #Log loss\n",
    "    train_sizes, train_scores2, validation_scores2 = learning_curve(estimator = best_model, X = X_train, y = y_train, train_sizes = train_sizes, cv = inner_cv, scoring = 'neg_log_loss')\n",
    "\n",
    "    #Permutation Tests\n",
    "    #Permutation Test 1\n",
    "    #score1, perm_scores1, pvalue1 = permutation_test_score(best_model, X = X_train, y = y_train, scoring=\"accuracy\", cv=inner_cv, n_permutations=675)\n",
    "    \n",
    "    #Permutation Test 2\n",
    "    #score_rand1, perm_scores_rand1, pvalue_rand1 = permutation_test_score(best_model, X = X_train, y = y_train, scoring=\"accuracy\", cv=inner_cv, n_permutations=675)\n",
    "    \n",
    "    #Incorrect Counts\n",
    "    #pred_delta=y_test-yhat\n",
    "    #FN_loc = np.where(pred_delta==1)[0]\n",
    "    #FP_loc = np.where(pred_delta==-1)[0]\n",
    "    #locations=X_test.index\n",
    "\n",
    "    pred_delta=y_test-yhat\n",
    "    FN_loc = np.where(pred_delta==1)[0]\n",
    "    FN_EF2_loc = np.where((complete_test['EF_Rating']==2) & (pred_delta==1))[0]\n",
    "    FP_loc = np.where(pred_delta==-1)[0]\n",
    "    FP_EF1_loc = np.where((complete_test['EF_Rating']==1) & (pred_delta==-1))[0]\n",
    "    TN_loc = np.where((y_test==0) & (yhat==0))\n",
    "    TP_loc = np.where((y_test==1) & (yhat==1))\n",
    "    correct_EF0 = np.where((y_test==yhat) & (complete_test['EF_Rating']==0))\n",
    "    correct_EF1 = np.where((y_test==yhat) & (complete_test['EF_Rating']==1))\n",
    "    correct_EF2 = np.where((y_test==yhat) & (complete_test['EF_Rating']==2))\n",
    "    correct_EF3 = np.where((y_test==yhat) & (complete_test['EF_Rating']==3))\n",
    "    correct_EF4 = np.where((y_test==yhat) & (complete_test['EF_Rating']==4))\n",
    "    correct_EF5 = np.where((y_test==yhat) & (complete_test['EF_Rating']==5))\n",
    "    correct = np.where(y_test==yhat)\n",
    "    incorr_EF0 = np.where((y_test!=yhat) & (complete_test['EF_Rating']==0))\n",
    "    incorr_EF1 = np.where((y_test!=yhat) & (complete_test['EF_Rating']==1))\n",
    "    incorr_EF2 = np.where((y_test!=yhat) & (complete_test['EF_Rating']==2))\n",
    "    incorr_EF3 = np.where((y_test!=yhat) & (complete_test['EF_Rating']==3))\n",
    "    incorr_EF4 = np.where((y_test!=yhat) & (complete_test['EF_Rating']==4))\n",
    "    incorr_EF5 = np.where((y_test!=yhat) & (complete_test['EF_Rating']==5))\n",
    "    incorr = np.where(y_test!=yhat)\n",
    "    locations=X_test.index\n",
    "    EF_ratings = complete['EF_Rating'][locations]\n",
    "    EF_ratings_loc = EF_ratings.reset_index()\n",
    "    FN_EF = EF_ratings_loc['EF_Rating'][FN_loc]\n",
    "    FP_EF = EF_ratings_loc['EF_Rating'][FP_loc]\n",
    "    Modes = complete['Mode'][locations]\n",
    "    Modes_loc = Modes.reset_index()\n",
    "    FN_modes = Modes_loc['Mode'][FN_loc]\n",
    "    FP_modes = Modes_loc['Mode'][FP_loc]\n",
    "\n",
    "    #SHAP\n",
    "    explainer = shap.TreeExplainer(best_model['model'], training_predictor, model_output='probability', feature_names=names)\n",
    "    shaps = explainer(test_predictor)\n",
    "    values_s = shaps.values\n",
    "    base_s = shaps.base_values\n",
    "    pred_s = shaps.data\n",
    "    values_s_nonsig = values_s[non_sig_loc,:,:]\n",
    "    base_s_nonsig = base_s[non_sig_loc,:]\n",
    "    pred_s_nonsig = pred_s[non_sig_loc,:]\n",
    "    values_s_sig = values_s[sig_loc,:,:]\n",
    "    base_s_sig = base_s[sig_loc,:]\n",
    "    pred_s_sig = pred_s[sig_loc,:]\n",
    "    \n",
    "    values_s_vio = values_s[violent_loc,:,:]\n",
    "    base_s_vio = base_s[violent_loc,:]\n",
    "    pred_s_vio = pred_s[violent_loc,:]\n",
    "    values_s_EF0 = values_s[EF0_loc,:,:]\n",
    "    base_s_EF0 = base_s[EF0_loc,:]\n",
    "    pred_s_EF0 = pred_s[EF0_loc,:]\n",
    "    values_s_EF1 = values_s[EF1_loc,:,:]\n",
    "    base_s_EF1 = base_s[EF1_loc,:]\n",
    "    pred_s_EF1 = pred_s[EF1_loc,:]\n",
    "    values_s_EF2 = values_s[EF2_loc,:,:]\n",
    "    base_s_EF2 = base_s[EF2_loc,:]\n",
    "    pred_s_EF2 = pred_s[EF2_loc,:]\n",
    "    values_s_EF3 = values_s[EF3_loc,:,:]\n",
    "    base_s_EF3 = base_s[EF3_loc,:]\n",
    "    pred_s_EF3 = pred_s[EF3_loc,:]\n",
    "    values_s_EF4 = values_s[EF4_loc,:,:]\n",
    "    base_s_EF4 = base_s[EF4_loc,:]\n",
    "    pred_s_EF4 = pred_s[EF4_loc,:]\n",
    "    values_s_EF5 = values_s[EF5_loc,:,:]\n",
    "    base_s_EF5 = base_s[EF5_loc,:]\n",
    "    pred_s_EF5 = pred_s[EF5_loc,:]\n",
    "    values_s_cor = values_s[correct,:,:]\n",
    "    base_s_cor = base_s[correct,:]\n",
    "    pred_s_cor = pred_s[correct,:]\n",
    "    values_s_cor_EF0 = values_s[correct_EF0,:,:]\n",
    "    base_s_cor_EF0 = base_s[correct_EF0,:]\n",
    "    pred_s_cor_EF0 = pred_s[correct_EF0,:]\n",
    "    values_s_cor_EF1 = values_s[correct_EF1,:,:]\n",
    "    base_s_cor_EF1 = base_s[correct_EF1,:]\n",
    "    pred_s_cor_EF1 = pred_s[correct_EF1,:]\n",
    "    values_s_cor_EF2 = values_s[correct_EF2,:,:]\n",
    "    base_s_cor_EF2 = base_s[correct_EF2,:]\n",
    "    pred_s_cor_EF2 = pred_s[correct_EF2,:]\n",
    "    values_s_cor_EF3 = values_s[correct_EF3,:,:]\n",
    "    base_s_cor_EF3 = base_s[correct_EF3,:]\n",
    "    pred_s_cor_EF3 = pred_s[correct_EF3,:]\n",
    "    values_s_cor_EF4 = values_s[correct_EF4,:,:]\n",
    "    base_s_cor_EF4 = base_s[correct_EF4,:]\n",
    "    pred_s_cor_EF4 = pred_s[correct_EF4,:]\n",
    "    values_s_cor_EF5 = values_s[correct_EF5,:,:]\n",
    "    base_s_cor_EF5 = base_s[correct_EF5,:]\n",
    "    pred_s_cor_EF5 = pred_s[correct_EF5,:]\n",
    "    values_s_incor = values_s[incorr,:,:]\n",
    "    base_s_incor = base_s[incorr,:]\n",
    "    pred_s_incor = pred_s[incorr,:]\n",
    "    values_s_incor_EF0 = values_s[incorr_EF0,:,:]\n",
    "    base_s_incor_EF0 = base_s[incorr_EF0,:]\n",
    "    pred_s_incor_EF0 = pred_s[incorr_EF0,:]\n",
    "    values_s_incor_EF1 = values_s[incorr_EF1,:,:]\n",
    "    base_s_incor_EF1 = base_s[incorr_EF1,:]\n",
    "    pred_s_incor_EF1 = pred_s[incorr_EF1,:]\n",
    "    values_s_incor_EF2 = values_s[incorr_EF2,:,:]\n",
    "    base_s_incor_EF2 = base_s[incorr_EF2,:]\n",
    "    pred_s_incor_EF2 = pred_s[incorr_EF2,:]\n",
    "    values_s_incor_EF3 = values_s[incorr_EF3,:,:]\n",
    "    base_s_incor_EF3 = base_s[incorr_EF3,:]\n",
    "    pred_s_incor_EF3 = pred_s[incorr_EF3,:]\n",
    "    values_s_incor_EF4 = values_s[incorr_EF4,:,:]\n",
    "    base_s_incor_EF4 = base_s[incorr_EF4,:]\n",
    "    pred_s_incor_EF4 = pred_s[incorr_EF4,:]\n",
    "    values_s_incor_EF5 = values_s[incorr_EF5,:,:]\n",
    "    base_s_incor_EF5 = base_s[incorr_EF5,:]\n",
    "    pred_s_incor_EF5 = pred_s[incorr_EF5,:]\n",
    "    values_s_dsc = values_s[dsc_loc,:,:]\n",
    "    base_s_dsc = base_s[dsc_loc,:]\n",
    "    pred_s_dsc = pred_s[dsc_loc,:]\n",
    "    values_s_dsc_EF0 = values_s[dsc_EF0_loc,:,:]\n",
    "    base_s_dsc_EF0 = base_s[dsc_EF0_loc,:]\n",
    "    pred_s_dsc_EF0 = pred_s[dsc_EF0_loc,:]\n",
    "    values_s_dsc_EF1 = values_s[dsc_EF1_loc,:,:]\n",
    "    base_s_dsc_EF1 = base_s[dsc_EF1_loc,:]\n",
    "    pred_s_dsc_EF1 = pred_s[dsc_EF1_loc,:]\n",
    "    values_s_dsc_EF2 = values_s[dsc_EF2_loc,:,:]\n",
    "    base_s_dsc_EF2 = base_s[dsc_EF2_loc,:]\n",
    "    pred_s_dsc_EF2 = pred_s[dsc_EF2_loc,:]\n",
    "    values_s_dsc_EF3 = values_s[dsc_EF3_loc,:,:]\n",
    "    base_s_dsc_EF3 = base_s[dsc_EF3_loc,:]\n",
    "    pred_s_dsc_EF3 = pred_s[dsc_EF3_loc,:]\n",
    "    values_s_dsc_EF4 = values_s[dsc_EF4_loc,:,:]\n",
    "    base_s_dsc_EF4 = base_s[dsc_EF4_loc,:]\n",
    "    pred_s_dsc_EF4 = pred_s[dsc_EF4_loc,:]\n",
    "    values_s_dsc_EF5 = values_s[dsc_EF5_loc,:,:]\n",
    "    base_s_dsc_EF5 = base_s[dsc_EF5_loc,:]\n",
    "    pred_s_dsc_EF5 = pred_s[dsc_EF5_loc,:]\n",
    "    values_s_qlcs = values_s[qlcs_loc,:,:]\n",
    "    base_s_qlcs = base_s[qlcs_loc,:]\n",
    "    pred_s_qlcs = pred_s[qlcs_loc,:]\n",
    "    values_s_qlcs_EF0 = values_s[qlcs_EF0_loc,:,:]\n",
    "    base_s_qlcs_EF0 = base_s[qlcs_EF0_loc,:]\n",
    "    pred_s_qlcs_EF0 = pred_s[qlcs_EF0_loc,:]\n",
    "    values_s_qlcs_EF1 = values_s[qlcs_EF1_loc,:,:]\n",
    "    base_s_qlcs_EF1 = base_s[qlcs_EF1_loc,:]\n",
    "    pred_s_qlcs_EF1 = pred_s[qlcs_EF1_loc,:]\n",
    "    values_s_qlcs_EF2 = values_s[qlcs_EF2_loc,:,:]\n",
    "    base_s_qlcs_EF2 = base_s[qlcs_EF2_loc,:]\n",
    "    pred_s_qlcs_EF2 = pred_s[qlcs_EF2_loc,:]\n",
    "    values_s_qlcs_EF3 = values_s[qlcs_EF3_loc,:,:]\n",
    "    base_s_qlcs_EF3 = base_s[qlcs_EF3_loc,:]\n",
    "    pred_s_qlcs_EF3 = pred_s[qlcs_EF3_loc,:]\n",
    "    values_s_qlcs_EF4 = values_s[qlcs_EF4_loc,:,:]\n",
    "    base_s_qlcs_EF4 = base_s[qlcs_EF4_loc,:]\n",
    "    pred_s_qlcs_EF4 = pred_s[qlcs_EF4_loc,:]\n",
    "    values_s_qlcs_EF5 = values_s[qlcs_EF5_loc,:,:]\n",
    "    base_s_qlcs_EF5 = base_s[qlcs_EF5_loc,:]\n",
    "    pred_s_qlcs_EF5 = pred_s[qlcs_EF5_loc,:]\n",
    "    values_s_mul = values_s[mul_loc,:,:]\n",
    "    base_s_mul = base_s[mul_loc,:]\n",
    "    pred_s_mul = pred_s[mul_loc,:]\n",
    "    values_s_mul_EF0 = values_s[mul_EF0_loc,:,:]\n",
    "    base_s_mul_EF0 = base_s[mul_EF0_loc,:]\n",
    "    pred_s_mul_EF0 = pred_s[mul_EF0_loc,:]\n",
    "    values_s_mul_EF1 = values_s[mul_EF1_loc,:,:]\n",
    "    base_s_mul_EF1 = base_s[mul_EF1_loc,:]\n",
    "    pred_s_mul_EF1 = pred_s[mul_EF1_loc,:]\n",
    "    values_s_mul_EF2 = values_s[mul_EF2_loc,:,:]\n",
    "    base_s_mul_EF2 = base_s[mul_EF2_loc,:]\n",
    "    pred_s_mul_EF2 = pred_s[mul_EF2_loc,:]\n",
    "    values_s_mul_EF3 = values_s[mul_EF3_loc,:,:]\n",
    "    base_s_mul_EF3 = base_s[mul_EF3_loc,:]\n",
    "    pred_s_mul_EF3 = pred_s[mul_EF3_loc,:]\n",
    "    values_s_mul_EF4 = values_s[mul_EF4_loc,:,:]\n",
    "    base_s_mul_EF4 = base_s[mul_EF4_loc,:]\n",
    "    pred_s_mul_EF4 = pred_s[mul_EF4_loc,:]\n",
    "    values_s_mul_EF5 = values_s[mul_EF5_loc,:,:]\n",
    "    base_s_mul_EF5 = base_s[mul_EF5_loc,:]\n",
    "    pred_s_mul_EF5 = pred_s[mul_EF5_loc,:]\n",
    "    values_s_FN = values_s[FN_loc,:,:]\n",
    "    base_s_FN = base_s[FN_loc,:]\n",
    "    pred_s_FN = pred_s[FN_loc,:]\n",
    "    values_s_FN_EF2 = values_s[FN_EF2_loc,:,:]\n",
    "    base_s_FN_EF2 = base_s[FN_EF2_loc,:]\n",
    "    pred_s_FN_EF2 = pred_s[FN_EF2_loc,:]\n",
    "    values_s_FP = values_s[FP_loc,:,:]\n",
    "    base_s_FP = base_s[FP_loc,:]\n",
    "    pred_s_FP = pred_s[FP_loc,:]\n",
    "    values_s_FP_EF1 = values_s[FP_EF1_loc,:,:]\n",
    "    base_s_FP_EF1 = base_s[FP_EF1_loc,:]\n",
    "    pred_s_FP_EF1 = pred_s[FP_EF1_loc,:]\n",
    "    values_s_TN = values_s[TN_loc,:,:]\n",
    "    base_s_TN = base_s[TN_loc,:]\n",
    "    pred_s_TN = pred_s[TN_loc,:]\n",
    "    values_s_TP = values_s[TP_loc,:,:]\n",
    "    base_s_TP = base_s[TP_loc,:]\n",
    "    pred_s_TP = pred_s[TP_loc,:]\n",
    "    \n",
    "    x_nonsig = X_test.values[non_sig_loc,:]\n",
    "    x_sig = X_test.values[sig_loc,:]\n",
    "    x_vio = X_test.values[violent_loc,:]\n",
    "    x_EF0 = X_test.values[EF0_loc,:]\n",
    "    x_EF1 = X_test.values[EF1_loc,:]\n",
    "    x_EF2 = X_test.values[EF2_loc,:]\n",
    "    x_EF3 = X_test.values[EF3_loc,:]\n",
    "    x_EF4 = X_test.values[EF4_loc,:]\n",
    "    x_EF5 = X_test.values[EF5_loc,:]\n",
    "    x_cor = X_test.values[correct,:]\n",
    "    x_cor_EF0 = X_test.values[correct_EF0,:]\n",
    "    x_cor_EF1 = X_test.values[correct_EF1,:]\n",
    "    x_cor_EF2 = X_test.values[correct_EF2,:]\n",
    "    x_cor_EF3 = X_test.values[correct_EF3,:]\n",
    "    x_cor_EF4 = X_test.values[correct_EF4,:]\n",
    "    x_cor_EF5 = X_test.values[correct_EF5,:]\n",
    "    x_incor = X_test.values[incorr,:]\n",
    "    x_incor_EF0 = X_test.values[incorr_EF0,:]\n",
    "    x_incor_EF1 = X_test.values[incorr_EF1,:]\n",
    "    x_incor_EF2 = X_test.values[incorr_EF2,:]\n",
    "    x_incor_EF3 = X_test.values[incorr_EF3,:]\n",
    "    x_incor_EF4 = X_test.values[incorr_EF4,:]\n",
    "    x_incor_EF5 = X_test.values[incorr_EF5,:]\n",
    "    x_dsc = X_test.values[dsc_loc,:]\n",
    "    x_dsc_EF0 = X_test.values[dsc_EF0_loc,:]\n",
    "    x_dsc_EF1 = X_test.values[dsc_EF1_loc,:]\n",
    "    x_dsc_EF2 = X_test.values[dsc_EF2_loc,:]\n",
    "    x_dsc_EF3 = X_test.values[dsc_EF3_loc,:]\n",
    "    x_dsc_EF4 = X_test.values[dsc_EF4_loc,:]\n",
    "    x_dsc_EF5 = X_test.values[dsc_EF5_loc,:]\n",
    "    x_qlcs = X_test.values[qlcs_loc,:]\n",
    "    x_qlcs_EF0 = X_test.values[qlcs_EF0_loc,:]\n",
    "    x_qlcs_EF1 = X_test.values[qlcs_EF1_loc,:]\n",
    "    x_qlcs_EF2 = X_test.values[qlcs_EF2_loc,:]\n",
    "    x_qlcs_EF3 = X_test.values[qlcs_EF3_loc,:]\n",
    "    x_qlcs_EF4 = X_test.values[qlcs_EF4_loc,:]\n",
    "    x_qlcs_EF5 = X_test.values[qlcs_EF5_loc,:]\n",
    "    x_mul = X_test.values[mul_loc,:]\n",
    "    x_mul_EF0 = X_test.values[mul_EF0_loc,:]\n",
    "    x_mul_EF1 = X_test.values[mul_EF1_loc,:]\n",
    "    x_mul_EF2 = X_test.values[mul_EF2_loc,:]\n",
    "    x_mul_EF3 = X_test.values[mul_EF3_loc,:]\n",
    "    x_mul_EF4 = X_test.values[mul_EF4_loc,:]\n",
    "    x_mul_EF5 = X_test.values[mul_EF5_loc,:]\n",
    "    x_FN = X_test.values[FN_loc,:]\n",
    "    x_FN_EF2 = X_test.values[FN_EF2_loc,:]\n",
    "    x_FP = X_test.values[FP_loc,:]\n",
    "    x_FP_EF1 = X_test.values[FP_EF1_loc,:]\n",
    "    x_TN = X_test.values[TN_loc,:]\n",
    "    x_TP = X_test.values[TP_loc,:]\n",
    "    #shap.plots.bar(shaps[:,:,1], max_display=25)\n",
    "    \n",
    "    # evaluate the model\n",
    "    acc = sklearn.metrics.accuracy_score(y_test, yhat)\n",
    "    jac = sklearn.metrics.jaccard_score(y_test, yhat)\n",
    "    brier = sklearn.metrics.brier_score_loss(y_test, yprob[:,1])\n",
    "    ll = sklearn.metrics.log_loss(y_test, yprob[:,1])\n",
    "    mas = sklearn.metrics.mean_absolute_error(y_test, yhat)\n",
    "    mse = sklearn.metrics.mean_squared_error(y_test, yhat)\n",
    "    prec = sklearn.metrics.precision_score(y_test, yhat)\n",
    "    recall = sklearn.metrics.recall_score(y_test, yhat)\n",
    "    auc = sklearn.metrics.roc_auc_score(y_test, yprob[:,1])\n",
    "    f1 = sklearn.metrics.f1_score(y_test, yhat)\n",
    "    f2 = sklearn.metrics.fbeta_score(y_test, yhat, beta=2)\n",
    "    FPR = FP/(FP+TN)\n",
    "    FNR = FN/(TP+FN)\n",
    "    TNR = TN/(TN+FP)\n",
    "    NPV = TN/(TN+FN)\n",
    "    FDR = FP/(TP+FP)\n",
    "    cohen_kappa = sklearn.metrics.cohen_kappa_score(y_test, yhat)\n",
    "    matt_cc = sklearn.metrics.matthews_corrcoef(y_test, yhat)\n",
    "    avg_prec_auc = sklearn.metrics.average_precision_score(y_test, yprob[:,1])\n",
    "    # store the result\n",
    "    outer_acc.append(acc)\n",
    "    outer_jac.append(jac)\n",
    "    outer_brier.append(brier)\n",
    "    outer_ll.append(ll)\n",
    "    outer_mas.append(mas)\n",
    "    outer_mse.append(mse)\n",
    "    outer_prec.append(prec)\n",
    "    outer_recall.append(recall)\n",
    "    outer_auc.append(auc)\n",
    "    outer_f1.append(f1)\n",
    "    outer_f2.append(f2)\n",
    "    outer_TP.append(TP)\n",
    "    outer_FP.append(FP)\n",
    "    outer_FN.append(FN)\n",
    "    outer_TN.append(TN)\n",
    "    outer_FPR.append(FPR)\n",
    "    outer_FNR.append(FNR)\n",
    "    outer_TNR.append(TNR)\n",
    "    outer_NPV.append(NPV)\n",
    "    outer_FDR.append(FDR)\n",
    "    outer_cohen_kappa.append(cohen_kappa)\n",
    "    outer_matt_cc.append(matt_cc)\n",
    "    outer_avg_prec_auc.append(avg_prec_auc)\n",
    "    \n",
    "    outer_train_scores_LC_ac.append(train_scores1)\n",
    "    outer_train_scores_LC_ll.append(train_scores2)\n",
    "    outer_val_scores_LC_ac.append(validation_scores1)\n",
    "    outer_val_scores_LC_ll.append(validation_scores2)\n",
    "    \n",
    "    #score.append(score1)\n",
    "    #perm_scores.append(perm_scores1)\n",
    "    #pvalue.append(pvalue1)\n",
    "    \n",
    "    #score_rand.append(score_rand1)\n",
    "    #perm_scores_rand.append(perm_scores_rand1)\n",
    "    #pvalue_rand.append(pvalue_rand1)\n",
    "    \n",
    "    FN_EF_total.append(FN_EF.values)\n",
    "    FP_EF_total.append(FP_EF.values)\n",
    "    \n",
    "    FN_mode_total.append(FN_modes.values)\n",
    "    FP_mode_total.append(FP_modes.values)\n",
    "    \n",
    "    fop_.append(fop)\n",
    "    mpv_.append(mpv)\n",
    "    \n",
    "    yprobs.append(yprob)\n",
    "    \n",
    "    shap_values.append(shaps)\n",
    "    test_sets.append(test_ix)\n",
    "    values_shap.append(values_s)\n",
    "    base_shap.append(base_s)\n",
    "    pred_shap.append(pred_s)\n",
    "    \n",
    "    values_shap_nonsig.append(values_s_nonsig[0])\n",
    "    base_shap_nonsig.append(base_s_nonsig[0])\n",
    "    pred_shap_nonsig.append(pred_s_nonsig[0])\n",
    "\n",
    "    values_shap_sig.append(values_s_sig[0])\n",
    "    base_shap_sig.append(base_s_sig[0])\n",
    "    pred_shap_sig.append(pred_s_sig[0])\n",
    "    \n",
    "    values_shap_vio.append(values_s_vio[0])\n",
    "    base_shap_vio.append(base_s_vio[0])\n",
    "    pred_shap_vio.append(pred_s_vio[0])\n",
    "    \n",
    "    values_shap_EF0.append(values_s_EF0[0])\n",
    "    base_shap_EF0.append(base_s_EF0[0])\n",
    "    pred_shap_EF0.append(pred_s_EF0[0])\n",
    "    \n",
    "    values_shap_EF1.append(values_s_EF1[0])\n",
    "    base_shap_EF1.append(base_s_EF1[0])\n",
    "    pred_shap_EF1.append(pred_s_EF1[0])\n",
    "    \n",
    "    values_shap_EF2.append(values_s_EF2[0])\n",
    "    base_shap_EF2.append(base_s_EF2[0])\n",
    "    pred_shap_EF2.append(pred_s_EF2[0])\n",
    "    \n",
    "    values_shap_EF3.append(values_s_EF3[0])\n",
    "    base_shap_EF3.append(base_s_EF3[0])\n",
    "    pred_shap_EF3.append(pred_s_EF3[0])\n",
    "    \n",
    "    values_shap_EF4.append(values_s_EF4[0])\n",
    "    base_shap_EF4.append(base_s_EF4[0])\n",
    "    pred_shap_EF4.append(pred_s_EF4[0])\n",
    "    \n",
    "    values_shap_EF5.append(values_s_EF5[0])\n",
    "    base_shap_EF5.append(base_s_EF5[0])\n",
    "    pred_shap_EF5.append(pred_s_EF5[0])\n",
    "    \n",
    "    values_shap_cor.append(values_s_cor[0])\n",
    "    base_shap_cor.append(base_s_cor[0])\n",
    "    pred_shap_cor.append(pred_s_cor[0])\n",
    "    \n",
    "    values_shap_cor_EF0.append(values_s_cor_EF0[0])\n",
    "    base_shap_cor_EF0.append(base_s_cor_EF0[0])\n",
    "    pred_shap_cor_EF0.append(pred_s_cor_EF0[0])\n",
    "    \n",
    "    values_shap_cor_EF1.append(values_s_cor_EF1[0])\n",
    "    base_shap_cor_EF1.append(base_s_cor_EF1[0])\n",
    "    pred_shap_cor_EF1.append(pred_s_cor_EF1[0])\n",
    "    \n",
    "    values_shap_cor_EF2.append(values_s_cor_EF2[0])\n",
    "    base_shap_cor_EF2.append(base_s_cor_EF2[0])\n",
    "    pred_shap_cor_EF2.append(pred_s_cor_EF2[0])\n",
    "    \n",
    "    values_shap_cor_EF3.append(values_s_cor_EF3[0])\n",
    "    base_shap_cor_EF3.append(base_s_cor_EF3[0])\n",
    "    pred_shap_cor_EF3.append(pred_s_cor_EF3[0])\n",
    "    \n",
    "    values_shap_cor_EF4.append(values_s_cor_EF4[0])\n",
    "    base_shap_cor_EF4.append(base_s_cor_EF4[0])\n",
    "    pred_shap_cor_EF4.append(pred_s_cor_EF4[0])\n",
    "    \n",
    "    values_shap_cor_EF5.append(values_s_cor_EF5[0])\n",
    "    base_shap_cor_EF5.append(base_s_cor_EF5[0])\n",
    "    pred_shap_cor_EF5.append(pred_s_cor_EF5[0])\n",
    "    \n",
    "    values_shap_incor.append(values_s_incor[0])\n",
    "    base_shap_incor.append(base_s_incor[0])\n",
    "    pred_shap_incor.append(pred_s_incor[0])\n",
    "    \n",
    "    values_shap_incor_EF0.append(values_s_incor_EF0[0])\n",
    "    base_shap_incor_EF0.append(base_s_incor_EF0[0])\n",
    "    pred_shap_incor_EF0.append(pred_s_incor_EF0[0])\n",
    "    \n",
    "    values_shap_incor_EF1.append(values_s_incor_EF1[0])\n",
    "    base_shap_incor_EF1.append(base_s_incor_EF1[0])\n",
    "    pred_shap_incor_EF1.append(pred_s_incor_EF1[0])\n",
    "    \n",
    "    values_shap_incor_EF2.append(values_s_incor_EF2[0])\n",
    "    base_shap_incor_EF2.append(base_s_incor_EF2[0])\n",
    "    pred_shap_incor_EF2.append(pred_s_incor_EF2[0])\n",
    "    \n",
    "    values_shap_incor_EF3.append(values_s_incor_EF3[0])\n",
    "    base_shap_incor_EF3.append(base_s_incor_EF3[0])\n",
    "    pred_shap_incor_EF3.append(pred_s_incor_EF3[0])\n",
    "    \n",
    "    values_shap_incor_EF4.append(values_s_incor_EF4[0])\n",
    "    base_shap_incor_EF4.append(base_s_incor_EF4[0])\n",
    "    pred_shap_incor_EF4.append(pred_s_incor_EF4[0])\n",
    "    \n",
    "    values_shap_incor_EF5.append(values_s_incor_EF5[0])\n",
    "    base_shap_incor_EF5.append(base_s_incor_EF5[0])\n",
    "    pred_shap_incor_EF5.append(pred_s_incor_EF5[0])\n",
    "    \n",
    "    values_shap_dsc.append(values_s_dsc[0])\n",
    "    base_shap_dsc.append(base_s_dsc[0])\n",
    "    pred_shap_dsc.append(pred_s_dsc[0])\n",
    "    \n",
    "    values_shap_dsc_EF0.append(values_s_dsc_EF0[0])\n",
    "    base_shap_dsc_EF0.append(base_s_dsc_EF0[0])\n",
    "    pred_shap_dsc_EF0.append(pred_s_dsc_EF0[0])\n",
    "    \n",
    "    values_shap_dsc_EF1.append(values_s_dsc_EF1[0])\n",
    "    base_shap_dsc_EF1.append(base_s_dsc_EF1[0])\n",
    "    pred_shap_dsc_EF1.append(pred_s_dsc_EF1[0])\n",
    "    \n",
    "    values_shap_dsc_EF2.append(values_s_dsc_EF2[0])\n",
    "    base_shap_dsc_EF2.append(base_s_dsc_EF2[0])\n",
    "    pred_shap_dsc_EF2.append(pred_s_dsc_EF2[0])\n",
    "    \n",
    "    values_shap_dsc_EF3.append(values_s_dsc_EF3[0])\n",
    "    base_shap_dsc_EF3.append(base_s_dsc_EF3[0])\n",
    "    pred_shap_dsc_EF3.append(pred_s_dsc_EF3[0])\n",
    "    \n",
    "    values_shap_dsc_EF4.append(values_s_dsc_EF4[0])\n",
    "    base_shap_dsc_EF4.append(base_s_dsc_EF4[0])\n",
    "    pred_shap_dsc_EF4.append(pred_s_dsc_EF4[0])\n",
    "    \n",
    "    values_shap_dsc_EF5.append(values_s_dsc_EF5[0])\n",
    "    base_shap_dsc_EF5.append(base_s_dsc_EF5[0])\n",
    "    pred_shap_dsc_EF5.append(pred_s_dsc_EF5[0])\n",
    "    \n",
    "    values_shap_qlcs.append(values_s_qlcs[0])\n",
    "    base_shap_qlcs.append(base_s_qlcs[0])\n",
    "    pred_shap_qlcs.append(pred_s_qlcs[0])\n",
    "    \n",
    "    values_shap_qlcs_EF0.append(values_s_qlcs_EF0[0])\n",
    "    base_shap_qlcs_EF0.append(base_s_qlcs_EF0[0])\n",
    "    pred_shap_qlcs_EF0.append(pred_s_qlcs_EF0[0])\n",
    "    \n",
    "    values_shap_qlcs_EF1.append(values_s_qlcs_EF1[0])\n",
    "    base_shap_qlcs_EF1.append(base_s_qlcs_EF1[0])\n",
    "    pred_shap_qlcs_EF1.append(pred_s_qlcs_EF1[0])\n",
    "    \n",
    "    values_shap_qlcs_EF2.append(values_s_qlcs_EF2[0])\n",
    "    base_shap_qlcs_EF2.append(base_s_qlcs_EF2[0])\n",
    "    pred_shap_qlcs_EF2.append(pred_s_qlcs_EF2[0])\n",
    "    \n",
    "    values_shap_qlcs_EF3.append(values_s_qlcs_EF3[0])\n",
    "    base_shap_qlcs_EF3.append(base_s_qlcs_EF3[0])\n",
    "    pred_shap_qlcs_EF3.append(pred_s_qlcs_EF3[0])\n",
    "    \n",
    "    values_shap_qlcs_EF4.append(values_s_qlcs_EF4[0])\n",
    "    base_shap_qlcs_EF4.append(base_s_qlcs_EF4[0])\n",
    "    pred_shap_qlcs_EF4.append(pred_s_qlcs_EF4[0])\n",
    "    \n",
    "    values_shap_qlcs_EF5.append(values_s_qlcs_EF5[0])\n",
    "    base_shap_qlcs_EF5.append(base_s_qlcs_EF5[0])\n",
    "    pred_shap_qlcs_EF5.append(pred_s_qlcs_EF5[0])\n",
    "    \n",
    "    values_shap_mul.append(values_s_mul[0])\n",
    "    base_shap_mul.append(base_s_mul[0])\n",
    "    pred_shap_mul.append(pred_s_mul[0])\n",
    "    \n",
    "    values_shap_mul_EF0.append(values_s_mul_EF0[0])\n",
    "    base_shap_mul_EF0.append(base_s_mul_EF0[0])\n",
    "    pred_shap_mul_EF0.append(pred_s_mul_EF0[0])\n",
    "    \n",
    "    values_shap_mul_EF1.append(values_s_mul_EF1[0])\n",
    "    base_shap_mul_EF1.append(base_s_mul_EF1[0])\n",
    "    pred_shap_mul_EF1.append(pred_s_mul_EF1[0])\n",
    "    \n",
    "    values_shap_mul_EF2.append(values_s_mul_EF2[0])\n",
    "    base_shap_mul_EF2.append(base_s_mul_EF2[0])\n",
    "    pred_shap_mul_EF2.append(pred_s_mul_EF2[0])\n",
    "    \n",
    "    values_shap_mul_EF3.append(values_s_mul_EF3[0])\n",
    "    base_shap_mul_EF3.append(base_s_mul_EF3[0])\n",
    "    pred_shap_mul_EF3.append(pred_s_mul_EF3[0])\n",
    "    \n",
    "    values_shap_mul_EF4.append(values_s_mul_EF4[0])\n",
    "    base_shap_mul_EF4.append(base_s_mul_EF4[0])\n",
    "    pred_shap_mul_EF4.append(pred_s_mul_EF4[0])\n",
    "    \n",
    "    values_shap_mul_EF5.append(values_s_mul_EF5[0])\n",
    "    base_shap_mul_EF5.append(base_s_mul_EF5[0])\n",
    "    pred_shap_mul_EF5.append(pred_s_mul_EF5[0])\n",
    "    \n",
    "    if (len(values_s_FP)>0):\n",
    "        values_shap_FP.append(values_s_FP)\n",
    "        base_shap_FP.append(base_s_FP)\n",
    "        pred_shap_FP.append(pred_s_FP)\n",
    "    else:\n",
    "        values_shap_FP=values_shap_FP\n",
    "        base_shap_FP=base_shap_FP\n",
    "        pred_shap_FP=pred_shap_FP\n",
    "        \n",
    "    if (len(values_s_FP_EF1)>0):\n",
    "        values_shap_FP_EF1.append(values_s_FP_EF1)\n",
    "        base_shap_FP_EF1.append(base_s_FP_EF1)\n",
    "        pred_shap_FP_EF1.append(pred_s_FP_EF1)\n",
    "    else:\n",
    "        values_shap_FP_EF1=values_shap_FP_EF1\n",
    "        base_shap_FP_EF1=base_shap_FP_EF1\n",
    "        pred_shap_FP_EF1=pred_shap_FP_EF1\n",
    "        \n",
    "    values_shap_FN.append(values_s_FN)\n",
    "    base_shap_FN.append(base_s_FN)\n",
    "    pred_shap_FN.append(pred_s_FN)\n",
    "    \n",
    "    values_shap_FN_EF2.append(values_s_FN_EF2)\n",
    "    base_shap_FN_EF2.append(base_s_FN_EF2)\n",
    "    pred_shap_FN_EF2.append(pred_s_FN_EF2)\n",
    "    \n",
    "    values_shap_TP.append(values_s_TP[0])\n",
    "    base_shap_TP.append(base_s_TP[0])\n",
    "    pred_shap_TP.append(pred_s_TP[0])\n",
    "    \n",
    "    values_shap_TN.append(values_s_TN[0])\n",
    "    base_shap_TN.append(base_s_TN[0])\n",
    "    pred_shap_TN.append(pred_s_TN[0])\n",
    "    \n",
    "    #dict_shap[\"Explanation\"].append(shaps)\n",
    "    \n",
    "    all_data.append(X_test.values)\n",
    "    X_nonsig.append(x_nonsig[0])\n",
    "    X_sig.append(x_sig[0])\n",
    "    X_vio.append(x_vio[0])\n",
    "    X_EF0.append(x_EF0[0])\n",
    "    X_EF1.append(x_EF1[0])\n",
    "    X_EF2.append(x_EF2[0])\n",
    "    X_EF3.append(x_EF3[0])\n",
    "    X_EF4.append(x_EF4[0])\n",
    "    X_EF5.append(x_EF5[0])\n",
    "    X_cor.append(x_cor[0])\n",
    "    X_cor_EF0.append(x_cor_EF0[0])\n",
    "    X_cor_EF1.append(x_cor_EF1[0])\n",
    "    X_cor_EF2.append(x_cor_EF2[0])\n",
    "    X_cor_EF3.append(x_cor_EF3[0])\n",
    "    X_cor_EF4.append(x_cor_EF4[0])\n",
    "    X_cor_EF5.append(x_cor_EF5[0])\n",
    "    X_incor.append(x_incor[0])\n",
    "    X_incor_EF0.append(x_incor_EF0[0])\n",
    "    X_incor_EF1.append(x_incor_EF1[0])\n",
    "    X_incor_EF2.append(x_incor_EF2[0])\n",
    "    X_incor_EF3.append(x_incor_EF3[0])\n",
    "    X_incor_EF4.append(x_incor_EF4[0])\n",
    "    X_incor_EF5.append(x_incor_EF5[0])\n",
    "    X_dsc.append(x_dsc[0])\n",
    "    X_dsc_EF0.append(x_dsc_EF0[0])\n",
    "    X_dsc_EF1.append(x_dsc_EF1[0])\n",
    "    X_dsc_EF2.append(x_dsc_EF2[0])\n",
    "    X_dsc_EF3.append(x_dsc_EF3[0])\n",
    "    X_dsc_EF4.append(x_dsc_EF4[0])\n",
    "    X_dsc_EF5.append(x_dsc_EF5[0])\n",
    "    X_qlcs.append(x_qlcs[0])\n",
    "    X_qlcs_EF0.append(x_qlcs_EF0[0])\n",
    "    X_qlcs_EF1.append(x_qlcs_EF1[0])\n",
    "    X_qlcs_EF2.append(x_qlcs_EF2[0])\n",
    "    X_qlcs_EF3.append(x_qlcs_EF3[0])\n",
    "    X_qlcs_EF4.append(x_qlcs_EF4[0])\n",
    "    X_qlcs_EF5.append(x_qlcs_EF5[0])\n",
    "    X_mul.append(x_mul[0])\n",
    "    X_mul_EF0.append(x_mul_EF0[0])\n",
    "    X_mul_EF1.append(x_mul_EF1[0])\n",
    "    X_mul_EF2.append(x_mul_EF2[0])\n",
    "    X_mul_EF3.append(x_mul_EF3[0])\n",
    "    X_mul_EF4.append(x_mul_EF4[0])\n",
    "    X_mul_EF5.append(x_mul_EF5[0])\n",
    "    if (len(x_FP>0)):\n",
    "        X_FP.append(x_FP)\n",
    "    else:\n",
    "        X_FP=X_FP\n",
    "    if (len(x_FP_EF1>0)):\n",
    "        X_FP_EF1.append(x_FP_EF1)\n",
    "    else:\n",
    "        X_FP_EF1=X_FP_EF1\n",
    "    X_FN.append(x_FN)\n",
    "    X_FN_EF2.append(x_FN_EF2)\n",
    "    X_TP.append(x_TP[0])\n",
    "    X_TN.append(x_TN[0])\n",
    "    \n",
    "    \n",
    "    # report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))\n",
    "    \n",
    "# summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(outer_acc), np.std(outer_acc)))\n",
    "print(np.mean(outer_acc))\n",
    "print(np.mean(outer_jac))\n",
    "print(np.mean(outer_brier))\n",
    "print(np.mean(outer_ll))\n",
    "print(np.mean(outer_mas))\n",
    "print(np.mean(outer_mse))\n",
    "print(np.mean(outer_prec))\n",
    "print(np.mean(outer_recall))\n",
    "print(np.mean(outer_auc))\n",
    "print(np.mean(outer_f1))\n",
    "print(np.mean(outer_f2))\n",
    "print(np.mean(outer_FPR))\n",
    "print(np.mean(outer_FNR))\n",
    "print(np.mean(outer_TNR))\n",
    "print(np.mean(outer_NPV))\n",
    "print(np.mean(outer_FDR))\n",
    "print(np.mean(outer_cohen_kappa))\n",
    "print(np.mean(outer_matt_cc))\n",
    "print(np.mean(outer_avg_prec_auc))\n",
    "\n",
    "print(np.std(outer_acc))\n",
    "print(np.std(outer_jac))\n",
    "print(np.std(outer_brier))\n",
    "print(np.std(outer_ll))\n",
    "print(np.std(outer_mas))\n",
    "print(np.std(outer_mse))\n",
    "print(np.std(outer_prec))\n",
    "print(np.std(outer_recall))\n",
    "print(np.std(outer_auc))\n",
    "print(np.std(outer_f1))\n",
    "print(np.std(outer_f2))\n",
    "print(np.std(outer_FPR))\n",
    "print(np.std(outer_FNR))\n",
    "print(np.std(outer_TNR))\n",
    "print(np.std(outer_NPV))\n",
    "print(np.std(outer_FDR))\n",
    "print(np.std(outer_cohen_kappa))\n",
    "print(np.std(outer_matt_cc))\n",
    "print(np.std(outer_avg_prec_auc))\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(np.sum(outer_TP))\n",
    "print(np.sum(outer_TN))\n",
    "print(np.sum(outer_FP))\n",
    "print(np.sum(outer_FN))\n",
    "\n",
    "#Mean Learning Curves\n",
    "mean_train_ac = np.mean(np.mean(outer_train_scores_LC_ac, axis=2), axis=0)\n",
    "mean_train_ll = np.mean(np.mean(outer_train_scores_LC_ll, axis=2),axis=0)\n",
    "mean_val_ac = np.mean(np.mean(outer_val_scores_LC_ac, axis=2),axis=0)\n",
    "mean_val_ll = np.mean(np.mean(outer_val_scores_LC_ll, axis=2),axis=0)\n",
    "\n",
    "#Mean Permutation Tests\n",
    "#Test 1\n",
    "#mean_score_perm = np.mean(score)\n",
    "#mean_perm_scores_in = np.mean(perm_scores)\n",
    "#mean_pvalue_perm = np.mean(pvalue)\n",
    "\n",
    "\n",
    "#Test 2\n",
    "#mean_score_rand_perm = np.mean(score_rand)\n",
    "#mean_perm_scores_rand_in = np.mean(perm_scores_rand, axis=0)\n",
    "#mean_pvalue_rand_perm = np.mean(pvalue_rand)\n",
    "\n",
    "#Incorrect Counts\n",
    "print('EF2s:',(np.concatenate(FN_EF_total)==2).sum())\n",
    "print('EF3s:',(np.concatenate(FN_EF_total)==3).sum())\n",
    "print('EF4s:',(np.concatenate(FN_EF_total)==4).sum())\n",
    "print('EF5s:',(np.concatenate(FN_EF_total)==5).sum())\n",
    "print('EF0s:',(np.concatenate(FP_EF_total)==0).sum())\n",
    "print('EF1s:',(np.concatenate(FP_EF_total)==1).sum())\n",
    "\n",
    "print('FN QLCSs:',(np.concatenate(FN_mode_total)==0).sum())\n",
    "print('FP QLCSs:',(np.concatenate(FP_mode_total)==0).sum())\n",
    "print('FN Discrete:',(np.concatenate(FN_mode_total)==1).sum())\n",
    "print('FP Discrete:',(np.concatenate(FP_mode_total)==1).sum())\n",
    "print('FN Multicell:',(np.concatenate(FN_mode_total)==2).sum())\n",
    "print('FP Multicell:',(np.concatenate(FP_mode_total)==2).sum())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Customized SHAP plotting code\n",
    "\n",
    "import warnings\n",
    "try:\n",
    "    import matplotlib.pyplot as pl\n",
    "except ImportError:\n",
    "    warnings.warn(\"matplotlib could not be loaded!\")\n",
    "    pass\n",
    "from shap.plots import _labels\n",
    "from shap.utils import format_value, ordinal_str\n",
    "from shap.plots._utils import convert_ordering, convert_color, merge_nodes, get_sort_order, sort_inds, dendrogram_coords\n",
    "from shap.plots import colors\n",
    "import numpy as np\n",
    "import scipy\n",
    "import copy\n",
    "from shap._explanation import Explanation, Cohorts\n",
    "\n",
    "\n",
    "# TODO: improve the bar chart to look better like the waterfall plot with numbers inside the bars when they fit\n",
    "# TODO: Have the Explanation object track enough data so that we can tell (and so show) how many instances are in each cohort\n",
    "def bar(shap_valuess, max_display=10, order=Explanation.abs, clustering=None, clustering_cutoff=0.5,\n",
    "        merge_cohorts=False, show_data=\"auto\", show=True):\n",
    "    \"\"\" Create a bar plot of a set of SHAP values.\n",
    "    If a single sample is passed then we plot the SHAP values as a bar chart. If an\n",
    "    Explanation with many samples is passed then we plot the mean absolute value for\n",
    "    each feature column as a bar chart.\n",
    "    Parameters\n",
    "    ----------\n",
    "    shap_values : shap.Explanation or shap.Cohorts or dictionary of shap.Explanation objects\n",
    "        A single row of a SHAP Explanation object (i.e. shap_values[0]) or a multi-row Explanation\n",
    "        object that we want to summarize.\n",
    "    max_display : int\n",
    "        The maximum number of bars to display.\n",
    "    show : bool\n",
    "        If show is set to False then we don't call the matplotlib.pyplot.show() function. This allows\n",
    "        further customization of the plot by the caller after the bar() function is finished. \n",
    "    \"\"\"\n",
    "\n",
    "    # assert str(type(shap_values)).endswith(\"Explanation'>\"), \"The shap_values paramemter must be a shap.Explanation object!\"\n",
    "\n",
    "    # convert Explanation objects to dictionaries\n",
    "    #if isinstance(shap_valuess, dict):\n",
    "    #    cohorts = {\"\": shap_valuess}\n",
    "    #elif isinstance(shap_valuess, Cohorts):\n",
    "    #    cohorts = shap_valuess.cohorts\n",
    "    #else:\n",
    "    #    assert isinstance(shap_values, dict), \"You must pass an Explanation object, Cohorts object, or dictionary to bar plot!\"\n",
    "    cohorts = shap_valuess\n",
    "    # unpack our list of Explanation objects we need to plot\n",
    "    cohort_labels = list(cohorts.keys())\n",
    "    cohort_exps = list(cohorts.values())\n",
    "    for i in range(len(cohort_exps)):\n",
    "        if len(cohort_exps[i].shape) == 2:\n",
    "            cohort_exps[0][i] = np.mean(np.abs(cohort_exps[0][i]))\n",
    "    #    assert isinstance(cohort_exps[i], Explanation), \"The shap_values paramemter must be a Explanation object, Cohorts object, or dictionary of Explanation objects!\"\n",
    "        assert cohort_exps[0][i].shape == cohort_exps[0][0].shape, \"When passing several Explanation objects they must all have the same shape!\"\n",
    "        # TODO: check other attributes for equality? like feature names perhaps? probably clustering as well.\n",
    "\n",
    "    # unpack the Explanation object\n",
    "    features = cohort_exps[0]\n",
    "    feature_names = predictor_cols\n",
    "    if clustering is None:\n",
    "        partition_tree = getattr(cohort_exps[0], \"clustering\", None)\n",
    "    elif clustering is False:\n",
    "        partition_tree = None\n",
    "    else:\n",
    "        partition_tree = clustering\n",
    "    if partition_tree is not None:\n",
    "        assert partition_tree.shape[1] == 4, \"The clustering provided by the Explanation object does not seem to be a partition tree (which is all shap.plots.bar supports)!\"\n",
    "    #op_history = cohort_exps[0].op_history\n",
    "    values = np.array([cohort_exps[2][i] for i in range(len(cohort_exps))])\n",
    "\n",
    "    if len(values[0]) == 0:\n",
    "        raise Exception(\"The passed Explanation is empty! (so there is nothing to plot)\")\n",
    "\n",
    "    # we show the data on auto only when there are no transforms\n",
    "    #if show_data == \"auto\":\n",
    "    #    show_data = len(op_history) == 0\n",
    "\n",
    "    # TODO: Rather than just show the \"1st token\", \"2nd token\", etc. it would be better to show the \"Instance 0's 1st but\", etc\n",
    "    if issubclass(type(feature_names), str):\n",
    "        feature_names = [ordinal_str(i)+\" \"+feature_names for i in range(len(values[0]))]\n",
    "\n",
    "    # build our auto xlabel based on the transform history of the Explanation object\n",
    "    #xlabel = \"SHAP value\"\n",
    "    #for op in op_history:\n",
    "    #    if op[\"name\"] == \"abs\":\n",
    "    #        xlabel = \"|\"+xlabel+\"|\"\n",
    "    #    elif op[\"name\"] == \"__getitem__\":\n",
    "    #        pass # no need for slicing to effect our label, it will be used later to find the sizes of cohorts\n",
    "    #    else:\n",
    "    #        xlabel = str(op[\"name\"])+\"(\"+xlabel+\")\"\n",
    "\n",
    "    # find how many instances are in each cohort (if they were created from an Explanation object)\n",
    "    #cohort_sizes = []\n",
    "    #for exp in cohort_exps:\n",
    "    #    for op in exp.op_history:\n",
    "    #        if op.get(\"collapsed_instances\", False): # see if this if the first op to collapse the instances\n",
    "    #            cohort_sizes.append(op[\"prev_shape\"][0])\n",
    "    #            break\n",
    "\n",
    "\n",
    "    # unwrap any pandas series\n",
    "    if str(type(features)) == \"<class 'pandas.core.series.Series'>\":\n",
    "        if feature_names is None:\n",
    "            feature_names = list(features.index)\n",
    "        features = features.values\n",
    "\n",
    "    # ensure we at least have default feature names\n",
    "    if feature_names is None:\n",
    "        feature_names = np.array([labels['FEATURE'] % str(i) for i in range(len(values[0]))])\n",
    "\n",
    "    # determine how many top features we will plot\n",
    "    if max_display is None:\n",
    "        max_display = len(feature_names)\n",
    "    num_features = min(max_display, len(values[0]))\n",
    "    max_display = min(max_display, num_features)\n",
    "\n",
    "    # iteratively merge nodes until we can cut off the smallest feature values to stay within\n",
    "    # num_features without breaking a cluster tree\n",
    "    orig_inds = [[i] for i in range(len(values[0]))]\n",
    "    orig_values = values.copy()\n",
    "    while True:\n",
    "        feature_order = np.argsort(np.mean([np.argsort(convert_ordering(order, Explanation(values[i]))) for i in range(values.shape[0])], 0))\n",
    "        if partition_tree is not None:\n",
    "\n",
    "            # compute the leaf order if we were to show (and so have the ordering respect) the whole partition tree\n",
    "            clust_order = sort_inds(partition_tree, np.abs(values).mean(0))\n",
    "\n",
    "            # now relax the requirement to match the parition tree ordering for connections above clustering_cutoff\n",
    "            dist = scipy.spatial.distance.squareform(scipy.cluster.hierarchy.cophenet(partition_tree))\n",
    "            feature_order = get_sort_order(dist, clust_order, clustering_cutoff, feature_order)\n",
    "\n",
    "            # if the last feature we can display is connected in a tree the next feature then we can't just cut\n",
    "            # off the feature ordering, so we need to merge some tree nodes and then try again.\n",
    "            if max_display < len(feature_order) and dist[feature_order[max_display-1],feature_order[max_display-2]] <= clustering_cutoff:\n",
    "                #values, partition_tree, orig_inds = merge_nodes(values, partition_tree, orig_inds)\n",
    "                partition_tree, ind1, ind2 = merge_nodes(np.abs(values).mean(0), partition_tree)\n",
    "                for i in range(len(values)):\n",
    "                    values[:,ind1] += values[:,ind2]\n",
    "                    values = np.delete(values, ind2, 1)\n",
    "                    orig_inds[ind1] += orig_inds[ind2]\n",
    "                    del orig_inds[ind2]\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # here we build our feature names, accounting for the fact that some features might be merged together\n",
    "    feature_inds = feature_order[:max_display]\n",
    "    y_pos = np.arange(len(feature_inds), 0, -1)\n",
    "    feature_names_new = []\n",
    "    for pos,inds in enumerate(orig_inds):\n",
    "        if len(inds) == 1:\n",
    "            feature_names_new.append(feature_names[inds[0]])\n",
    "        else:\n",
    "            full_print = \" + \".join([feature_names[i] for i in inds])\n",
    "            if len(full_print) <= 40:\n",
    "                feature_names_new.append(full_print)\n",
    "            else:\n",
    "                max_ind = np.argmax(np.abs(orig_values).mean(0)[inds])\n",
    "                feature_names_new.append(feature_names[inds[max_ind]] + \" + %d other features\" % (len(inds)-1))\n",
    "    feature_names = feature_names_new\n",
    "\n",
    "    # see how many individual (vs. grouped at the end) features we are plotting\n",
    "    if num_features < len(values[0]):\n",
    "        num_cut = np.sum([len(orig_inds[feature_order[i]]) for i in range(num_features-1, len(values[0]))])\n",
    "        values[:,feature_order[num_features-1]] = np.sum([values[:,feature_order[i]] for i in range(num_features-1, len(values[0]))], 0)\n",
    "    \n",
    "    # build our y-tick labels\n",
    "    yticklabels = []\n",
    "    for i in feature_inds:\n",
    "        if features is not None and show_data:\n",
    "            yticklabels.append(format_value(features[i], \"%0.03f\") + \" = \" + feature_names[i])\n",
    "        else:\n",
    "            yticklabels.append(feature_names[i])\n",
    "    if num_features < len(values[0]):\n",
    "        yticklabels[-1] = \"Sum of %d other features\" % num_cut\n",
    "\n",
    "    # compute our figure size based on how many features we are showing\n",
    "    row_height = 0.5\n",
    "    pl.gcf().set_size_inches(8, num_features * row_height * np.sqrt(len(values)) + 1.5)\n",
    "\n",
    "    # if negative values are present then we draw a vertical line to mark 0, otherwise the axis does this for us...\n",
    "    negative_values_present = np.sum(values[:,feature_order[:num_features]] < 0) > 0\n",
    "    if negative_values_present:\n",
    "        pl.axvline(0, 0, 1, color=\"#000000\", linestyle=\"-\", linewidth=1, zorder=1)\n",
    "\n",
    "    # draw the bars\n",
    "    patterns = (None, '\\\\\\\\', '++', 'xx', '////', '*', 'o', 'O', '.', '-')\n",
    "    total_width = 0.7\n",
    "    bar_width = total_width / len(values)\n",
    "    for i in range(len(values)):\n",
    "        ypos_offset = - ((i - len(values) / 2) * bar_width + bar_width / 2)\n",
    "        pl.barh(\n",
    "            y_pos + ypos_offset, values[i,feature_inds],\n",
    "            bar_width, align='center',\n",
    "            color=[colors.blue_rgb if values[i,feature_inds[j]] <= 0 else colors.red_rgb for j in range(len(y_pos))],\n",
    "            hatch=patterns[i], edgecolor=(1,1,1,0.8), label=f\"{cohort_labels[i]} [{cohort_sizes[i] if i < len(cohort_sizes) else None}]\"\n",
    "        )\n",
    "\n",
    "    # draw the yticks (the 1e-8 is so matplotlib 3.3 doesn't try and collapse the ticks)\n",
    "    pl.yticks(list(y_pos) + list(y_pos + 1e-8), yticklabels + [l.split('=')[-1] for l in yticklabels], fontsize=13)\n",
    "\n",
    "    xlen = pl.xlim()[1] - pl.xlim()[0]\n",
    "    fig = pl.gcf()\n",
    "    ax = pl.gca()\n",
    "    #xticks = ax.get_xticks()\n",
    "    bbox = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "    width, height = bbox.width, bbox.height\n",
    "    bbox_to_xscale = xlen/width\n",
    "\n",
    "    for i in range(len(values)):\n",
    "        ypos_offset = - ((i - len(values) / 2) * bar_width + bar_width / 2)\n",
    "        for j in range(len(y_pos)):\n",
    "            ind = feature_order[j]\n",
    "            if values[i,ind] < 0:\n",
    "                pl.text(\n",
    "                    values[i,ind] - (5/72)*bbox_to_xscale, y_pos[j] + ypos_offset, format_value(values[i,ind], '%+0.02f'),\n",
    "                    horizontalalignment='right', verticalalignment='center', color=colors.blue_rgb,\n",
    "                    fontsize=12\n",
    "                )\n",
    "            else:\n",
    "                pl.text(\n",
    "                    values[i,ind] + (5/72)*bbox_to_xscale, y_pos[j] + ypos_offset, format_value(values[i,ind], '%+0.02f'),\n",
    "                    horizontalalignment='left', verticalalignment='center', color=colors.red_rgb,\n",
    "                    fontsize=12\n",
    "                )\n",
    "\n",
    "    # put horizontal lines for each feature row\n",
    "    for i in range(num_features):\n",
    "        pl.axhline(i+1, color=\"#888888\", lw=0.5, dashes=(1, 5), zorder=-1)\n",
    "    \n",
    "    if features is not None:\n",
    "        features = list(features)\n",
    "\n",
    "        # try and round off any trailing zeros after the decimal point in the feature values\n",
    "        for i in range(len(features)):\n",
    "            try:\n",
    "                if round(features[i]) == features[i]:\n",
    "                    features[i] = int(features[i])\n",
    "            except:\n",
    "                pass # features[i] must not be a number\n",
    "    \n",
    "    pl.gca().xaxis.set_ticks_position('bottom')\n",
    "    pl.gca().yaxis.set_ticks_position('none')\n",
    "    pl.gca().spines['right'].set_visible(False)\n",
    "    pl.gca().spines['top'].set_visible(False)\n",
    "    if negative_values_present:\n",
    "        pl.gca().spines['left'].set_visible(False)\n",
    "    pl.gca().tick_params('x', labelsize=11)\n",
    "\n",
    "    xmin,xmax = pl.gca().get_xlim()\n",
    "    ymin,ymax = pl.gca().get_ylim()\n",
    "    \n",
    "    if negative_values_present:\n",
    "        pl.gca().set_xlim(xmin - (xmax-xmin)*0.05, xmax + (xmax-xmin)*0.05)\n",
    "    else:\n",
    "        pl.gca().set_xlim(xmin, xmax + (xmax-xmin)*0.05)\n",
    "    \n",
    "    # if features is None:\n",
    "    #     pl.xlabel(labels[\"GLOBAL_VALUE\"], fontsize=13)\n",
    "    # else:\n",
    "    pl.xlabel(xlabel, fontsize=13)\n",
    "\n",
    "    if len(values) > 1:\n",
    "        pl.legend(fontsize=12)\n",
    "\n",
    "    # color the y tick labels that have the feature values as gray\n",
    "    # (these fall behind the black ones with just the feature name)\n",
    "    tick_labels = pl.gca().yaxis.get_majorticklabels()\n",
    "    for i in range(num_features):\n",
    "        tick_labels[i].set_color(\"#999999\")\n",
    "\n",
    "    # draw a dendrogram if we are given a partition tree\n",
    "    if partition_tree is not None:\n",
    "        \n",
    "        # compute the dendrogram line positions based on our current feature order\n",
    "        feature_pos = np.argsort(feature_order)\n",
    "        ylines,xlines = dendrogram_coords(feature_pos, partition_tree)\n",
    "        \n",
    "        # plot the distance cut line above which we don't show tree edges\n",
    "        xmin,xmax = pl.xlim()\n",
    "        xlines_min,xlines_max = np.min(xlines),np.max(xlines)\n",
    "        ct_line_pos = (clustering_cutoff / (xlines_max - xlines_min)) * 0.1 * (xmax - xmin) + xmax\n",
    "        pl.text(\n",
    "            ct_line_pos + 0.005 * (xmax - xmin), (ymax - ymin)/2, \"Clustering cutoff = \" + format_value(clustering_cutoff, '%0.02f'),\n",
    "            horizontalalignment='left', verticalalignment='center', color=\"#999999\",\n",
    "            fontsize=12, rotation=-90\n",
    "        )\n",
    "        l = pl.axvline(ct_line_pos, color=\"#dddddd\", dashes=(1, 1))\n",
    "        l.set_clip_on(False)\n",
    "        \n",
    "        for (xline, yline) in zip(xlines, ylines):\n",
    "            \n",
    "            # normalize the x values to fall between 0 and 1\n",
    "            xv = (np.array(xline) / (xlines_max - xlines_min))\n",
    "\n",
    "            # only draw if we are not going past distance threshold\n",
    "            if np.array(xline).max() <= clustering_cutoff:\n",
    "\n",
    "                # only draw if we are not going past the bottom of the plot\n",
    "                if yline.max() < max_display:\n",
    "                    l = pl.plot(\n",
    "                        xv * 0.1 * (xmax - xmin) + xmax,\n",
    "                        max_display - np.array(yline),\n",
    "                        color=\"#999999\"\n",
    "                    )\n",
    "                    for v in l:\n",
    "                        v.set_clip_on(False)\n",
    "    \n",
    "    if show:\n",
    "        pl.show()\n",
    "\n",
    "\n",
    "\n",
    "# def compute_sort_counts(partition_tree, leaf_values, pos=None):\n",
    "#     if pos is None:\n",
    "#         pos = partition_tree.shape[0]-1\n",
    "    \n",
    "#     M = partition_tree.shape[0] + 1\n",
    "        \n",
    "#     if pos < 0:\n",
    "#         return 1,leaf_values[pos + M]\n",
    "    \n",
    "#     left = int(partition_tree[pos, 0]) - M\n",
    "#     right = int(partition_tree[pos, 1]) - M\n",
    "    \n",
    "#     left_val,left_sum = compute_sort_counts(partition_tree, leaf_values, left)\n",
    "#     right_val,right_sum = compute_sort_counts(partition_tree, leaf_values, right)\n",
    "    \n",
    "#     if left_sum > right_sum:\n",
    "#         left_val = right_val + 1\n",
    "#     else:\n",
    "#         right_val = left_val + 1\n",
    "\n",
    "#     if left >= 0:\n",
    "#         partition_tree[left,3] = left_val\n",
    "#     if right >= 0:\n",
    "#         partition_tree[right,3] = right_val\n",
    "\n",
    "    \n",
    "#     return max(left_val, right_val) + 1, max(left_sum, right_sum)\n",
    "\n",
    "def bar_legacy(shap_values, features=None, feature_names=None, max_display=None, show=True):\n",
    "\n",
    "    # unwrap pandas series\n",
    "    if str(type(features)) == \"<class 'pandas.core.series.Series'>\":\n",
    "        if feature_names is None:\n",
    "            feature_names = list(features.index)\n",
    "        features = features.values\n",
    "\n",
    "    if feature_names is None:\n",
    "        feature_names = np.array([labels['FEATURE'] % str(i) for i in range(len(shap_values))])\n",
    "\n",
    "    if max_display is None:\n",
    "        max_display = 7\n",
    "    else:\n",
    "        max_display = min(len(feature_names), max_display)\n",
    "\n",
    "\n",
    "    feature_order = np.argsort(-np.abs(shap_values))\n",
    "    \n",
    "    # \n",
    "    feature_inds = feature_order[:max_display]\n",
    "    y_pos = np.arange(len(feature_inds), 0, -1)\n",
    "    pl.barh(\n",
    "        y_pos, shap_values[feature_inds],\n",
    "        0.7, align='center',\n",
    "        color=[colors.red_rgb if shap_values[feature_inds[i]] > 0 else colors.blue_rgb for i in range(len(y_pos))]\n",
    "    )\n",
    "    row_height = 0.1\n",
    "    pl.gcf().set_size_inches(8, 12)\n",
    "    pl.yticks(y_pos, fontsize=23)\n",
    "    pl.xticks(fontsize=13)\n",
    "    if features is not None:\n",
    "        features = list(features)\n",
    "\n",
    "        # try and round off any trailing zeros after the decimal point in the feature values\n",
    "        for i in range(len(features)):\n",
    "            try:\n",
    "                if round(features[i]) == features[i]:\n",
    "                    features[i] = int(features[i])\n",
    "            except TypeError:\n",
    "                pass # features[i] must not be a number\n",
    "    yticklabels = []\n",
    "    for i in feature_inds:\n",
    "        if features is not None:\n",
    "            yticklabels.append(feature_names[i])\n",
    "        else:\n",
    "            yticklabels.append(feature_names[i])\n",
    "    pl.gca().set_yticklabels(yticklabels, fontsize=13)\n",
    "    pl.gca().xaxis.set_ticks_position('bottom')\n",
    "    pl.gca().yaxis.set_ticks_position('none')\n",
    "    pl.gca().spines['right'].set_visible(False)\n",
    "    pl.gca().spines['top'].set_visible(False)\n",
    "    #pl.gca().spines['left'].set_visible(False)\n",
    "    \n",
    "    pl.xlabel(\"SHAP value (impact on model output)\")\n",
    "    pl.ylabel(\"Predictor\")\n",
    "    pl.title(\"Impact on Model Prediction\", fontsize=20)\n",
    "    \n",
    "    \n",
    "    if show:\n",
    "        #pl.savefig('/data/keeling/a/msessa2/python/notebooks/SHAP/'+str(name), bbox_inches='tight')\n",
    "        pl.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Save SHAP output by different separations and take mean across CV folds\n",
    "nonsig_shap = np.concatenate(values_shap_nonsig, axis=0)\n",
    "nonsig_base = np.concatenate(base_shap_nonsig, axis=0)\n",
    "nonsig_pred = np.concatenate(pred_shap_nonsig, axis=0)\n",
    "\n",
    "sig_shap = np.concatenate(values_shap_sig, axis=0)\n",
    "sig_base = np.concatenate(base_shap_sig, axis=0)\n",
    "sig_pred = np.concatenate(pred_shap_sig, axis=0)\n",
    "\n",
    "nonsig_no_scale = np.concatenate(X_nonsig, axis=0)\n",
    "sig_no_scale = np.concatenate(X_sig, axis=0)\n",
    "sig_lob = np.mean(np.abs(sig_shap[:,:,1]), axis=0)\n",
    "nonsig_lob = np.mean(np.abs(nonsig_shap[:,:,1]), axis=0)\n",
    "\n",
    "#Subset data for additional SHAP analysis\n",
    "vio_shap = np.concatenate(values_shap_vio, axis=0)\n",
    "vio_base = np.concatenate(base_shap_vio, axis=0)\n",
    "vio_pred = np.concatenate(pred_shap_vio, axis=0)\n",
    "vio_no_scale = np.concatenate(X_vio, axis=0)\n",
    "vio_lob = np.mean(np.abs(vio_shap[:,:,1]), axis=0)\n",
    "vio_lob_no_abs=np.mean(vio_shap[:,:,1], axis=0)\n",
    "\n",
    "EF0_shap = np.concatenate(values_shap_EF0, axis=0)\n",
    "EF0_base = np.concatenate(base_shap_EF0, axis=0)\n",
    "EF0_pred = np.concatenate(pred_shap_EF0, axis=0)\n",
    "EF0_no_scale = np.concatenate(X_EF0, axis=0)\n",
    "EF0_lob = np.mean(np.abs(EF0_shap[:,:,1]), axis=0)\n",
    "EF0_lob_no_abs=np.mean(EF0_shap[:,:,1], axis=0)\n",
    "\n",
    "EF1_shap = np.concatenate(values_shap_EF1, axis=0)\n",
    "EF1_base = np.concatenate(base_shap_EF1, axis=0)\n",
    "EF1_pred = np.concatenate(pred_shap_EF1, axis=0)\n",
    "EF1_no_scale = np.concatenate(X_EF1, axis=0)\n",
    "EF1_lob = np.mean(np.abs(EF1_shap[:,:,1]), axis=0)\n",
    "EF1_lob_no_abs=np.mean(EF1_shap[:,:,1], axis=0)\n",
    "\n",
    "EF2_shap = np.concatenate(values_shap_EF2, axis=0)\n",
    "EF2_base = np.concatenate(base_shap_EF2, axis=0)\n",
    "EF2_pred = np.concatenate(pred_shap_EF2, axis=0)\n",
    "EF2_no_scale = np.concatenate(X_EF2, axis=0)\n",
    "EF2_lob = np.mean(np.abs(EF2_shap[:,:,1]), axis=0)\n",
    "EF2_lob_no_abs=np.mean(EF2_shap[:,:,1], axis=0)\n",
    "\n",
    "EF3_shap = np.concatenate(values_shap_EF3, axis=0)\n",
    "EF3_base = np.concatenate(base_shap_EF3, axis=0)\n",
    "EF3_pred = np.concatenate(pred_shap_EF3, axis=0)\n",
    "EF3_no_scale = np.concatenate(X_EF3, axis=0)\n",
    "EF3_lob = np.mean(np.abs(EF3_shap[:,:,1]), axis=0)\n",
    "EF3_lob_no_abs=np.mean(EF3_shap[:,:,1], axis=0)\n",
    "\n",
    "EF4_shap = np.concatenate(values_shap_EF4, axis=0)\n",
    "EF4_base = np.concatenate(base_shap_EF4, axis=0)\n",
    "EF4_pred = np.concatenate(pred_shap_EF4, axis=0)\n",
    "EF4_no_scale = np.concatenate(X_EF4, axis=0)\n",
    "EF4_lob = np.mean(np.abs(EF4_shap[:,:,1]), axis=0)\n",
    "EF4_lob_no_abs=np.mean(EF4_shap[:,:,1], axis=0)\n",
    "\n",
    "EF5_shap = np.concatenate(values_shap_EF5, axis=0)\n",
    "EF5_base = np.concatenate(base_shap_EF5, axis=0)\n",
    "EF5_pred = np.concatenate(pred_shap_EF5, axis=0)\n",
    "EF5_no_scale = np.concatenate(X_EF5, axis=0)\n",
    "EF5_lob = np.mean(np.abs(EF5_shap[:,:,1]), axis=0)\n",
    "EF5_lob_no_abs=np.mean(EF5_shap[:,:,1], axis=0)\n",
    "\n",
    "cor_shap = np.concatenate(values_shap_cor, axis=0)\n",
    "cor_base = np.concatenate(base_shap_cor, axis=0)\n",
    "cor_pred = np.concatenate(pred_shap_cor, axis=0)\n",
    "cor_no_scale = np.concatenate(X_cor, axis=0)\n",
    "cor_lob = np.mean(np.abs(cor_shap[:,:,1]), axis=0)\n",
    "cor_lob_no_abs=np.mean(cor_shap[:,:,1], axis=0)\n",
    "\n",
    "EF0_cor_shap = np.concatenate(values_shap_cor_EF0, axis=0)\n",
    "EF0_cor_base = np.concatenate(base_shap_cor_EF0, axis=0)\n",
    "EF0_cor_pred = np.concatenate(pred_shap_cor_EF0, axis=0)\n",
    "EF0_cor_no_scale = np.concatenate(X_cor_EF0, axis=0)\n",
    "EF0_cor_lob = np.mean(np.abs(EF0_cor_shap[:,:,1]), axis=0)\n",
    "EF0_cor_lob_no_abs=np.mean(EF0_cor_shap[:,:,1], axis=0)\n",
    "\n",
    "EF1_cor_shap = np.concatenate(values_shap_cor_EF1, axis=0)\n",
    "EF1_cor_base = np.concatenate(base_shap_cor_EF1, axis=0)\n",
    "EF1_cor_pred = np.concatenate(pred_shap_cor_EF1, axis=0)\n",
    "EF1_cor_no_scale = np.concatenate(X_cor_EF1, axis=0)\n",
    "EF1_cor_lob = np.mean(np.abs(EF1_cor_shap[:,:,1]), axis=0)\n",
    "EF1_cor_lob_no_abs=np.mean(EF1_cor_shap[:,:,1], axis=0)\n",
    "\n",
    "EF2_cor_shap = np.concatenate(values_shap_cor_EF2, axis=0)\n",
    "EF2_cor_base = np.concatenate(base_shap_cor_EF2, axis=0)\n",
    "EF2_cor_pred = np.concatenate(pred_shap_cor_EF2, axis=0)\n",
    "EF2_cor_no_scale = np.concatenate(X_cor_EF2, axis=0)\n",
    "EF2_cor_lob = np.mean(np.abs(EF2_cor_shap[:,:,1]), axis=0)\n",
    "EF2_cor_lob_no_abs=np.mean(EF2_cor_shap[:,:,1], axis=0)\n",
    "\n",
    "EF3_cor_shap = np.concatenate(values_shap_cor_EF3, axis=0)\n",
    "EF3_cor_base = np.concatenate(base_shap_cor_EF3, axis=0)\n",
    "EF3_cor_pred = np.concatenate(pred_shap_cor_EF3, axis=0)\n",
    "EF3_cor_no_scale = np.concatenate(X_cor_EF3, axis=0)\n",
    "EF3_cor_lob = np.mean(np.abs(EF3_cor_shap[:,:,1]), axis=0)\n",
    "EF3_cor_lob_no_abs=np.mean(EF3_cor_shap[:,:,1], axis=0)\n",
    "\n",
    "EF4_cor_shap = np.concatenate(values_shap_cor_EF4, axis=0)\n",
    "EF4_cor_base = np.concatenate(base_shap_cor_EF4, axis=0)\n",
    "EF4_cor_pred = np.concatenate(pred_shap_cor_EF4, axis=0)\n",
    "EF4_cor_no_scale = np.concatenate(X_cor_EF4, axis=0)\n",
    "EF4_cor_lob = np.mean(np.abs(EF4_cor_shap[:,:,1]), axis=0)\n",
    "EF4_cor_lob_no_abs=np.mean(EF4_cor_shap[:,:,1], axis=0)\n",
    "\n",
    "EF5_cor_shap = np.concatenate(values_shap_cor_EF5, axis=0)\n",
    "EF5_cor_base = np.concatenate(base_shap_cor_EF5, axis=0)\n",
    "EF5_cor_pred = np.concatenate(pred_shap_cor_EF5, axis=0)\n",
    "EF5_cor_no_scale = np.concatenate(X_cor_EF5, axis=0)\n",
    "EF5_cor_lob = np.mean(np.abs(EF5_cor_shap[:,:,1]), axis=0)\n",
    "EF5_cor_lob_no_abs=np.mean(EF5_cor_shap[:,:,1], axis=0)\n",
    "\n",
    "incor_shap = np.concatenate(values_shap_incor, axis=0)\n",
    "incor_base = np.concatenate(base_shap_incor, axis=0)\n",
    "incor_pred = np.concatenate(pred_shap_incor, axis=0)\n",
    "incor_no_scale = np.concatenate(X_incor, axis=0)\n",
    "incor_lob = np.mean(np.abs(incor_shap[:,:,1]), axis=0)\n",
    "incor_lob_no_abs=np.mean(incor_shap[:,:,1], axis=0)\n",
    "\n",
    "EF0_incor_shap = np.concatenate(values_shap_incor_EF0, axis=0)\n",
    "EF0_incor_base = np.concatenate(base_shap_incor_EF0, axis=0)\n",
    "EF0_incor_pred = np.concatenate(pred_shap_incor_EF0, axis=0)\n",
    "EF0_incor_no_scale = np.concatenate(X_incor_EF0, axis=0)\n",
    "EF0_incor_lob = np.mean(np.abs(EF0_incor_shap[:,:,1]), axis=0)\n",
    "EF0_incor_lob_no_abs=np.mean(EF0_incor_shap[:,:,1], axis=0)\n",
    "\n",
    "EF1_incor_shap = np.concatenate(values_shap_incor_EF1, axis=0)\n",
    "EF1_incor_base = np.concatenate(base_shap_incor_EF1, axis=0)\n",
    "EF1_incor_pred = np.concatenate(pred_shap_incor_EF1, axis=0)\n",
    "EF1_incor_no_scale = np.concatenate(X_incor_EF1, axis=0)\n",
    "EF1_incor_lob = np.mean(np.abs(EF1_incor_shap[:,:,1]), axis=0)\n",
    "EF1_incor_lob_no_abs=np.mean(EF1_incor_shap[:,:,1], axis=0)\n",
    "\n",
    "EF2_incor_shap = np.concatenate(values_shap_incor_EF2, axis=0)\n",
    "EF2_incor_base = np.concatenate(base_shap_incor_EF2, axis=0)\n",
    "EF2_incor_pred = np.concatenate(pred_shap_incor_EF2, axis=0)\n",
    "EF2_incor_no_scale = np.concatenate(X_incor_EF2, axis=0)\n",
    "EF2_incor_lob = np.mean(np.abs(EF2_incor_shap[:,:,1]), axis=0)\n",
    "EF2_incor_lob_no_abs=np.mean(EF2_incor_shap[:,:,1], axis=0)\n",
    "\n",
    "EF3_incor_shap = np.concatenate(values_shap_incor_EF3, axis=0)\n",
    "EF3_incor_base = np.concatenate(base_shap_incor_EF3, axis=0)\n",
    "EF3_incor_pred = np.concatenate(pred_shap_incor_EF3, axis=0)\n",
    "EF3_incor_no_scale = np.concatenate(X_incor_EF3, axis=0)\n",
    "EF3_incor_lob = np.mean(np.abs(EF3_incor_shap[:,:,1]), axis=0)\n",
    "EF3_incor_lob_no_abs=np.mean(EF3_incor_shap[:,:,1], axis=0)\n",
    "\n",
    "EF4_incor_shap = np.concatenate(values_shap_incor_EF4, axis=0)\n",
    "EF4_incor_base = np.concatenate(base_shap_incor_EF4, axis=0)\n",
    "EF4_incor_pred = np.concatenate(pred_shap_incor_EF4, axis=0)\n",
    "EF4_incor_no_scale = np.concatenate(X_incor_EF4, axis=0)\n",
    "EF4_incor_lob = np.mean(np.abs(EF4_incor_shap[:,:,1]), axis=0)\n",
    "EF4_incor_lob_no_abs=np.mean(EF4_incor_shap[:,:,1], axis=0)\n",
    "\n",
    "EF5_incor_shap = np.concatenate(values_shap_incor_EF5, axis=0)\n",
    "EF5_incor_base = np.concatenate(base_shap_incor_EF5, axis=0)\n",
    "EF5_incor_pred = np.concatenate(pred_shap_incor_EF5, axis=0)\n",
    "EF5_incor_no_scale = np.concatenate(X_incor_EF5, axis=0)\n",
    "EF5_incor_lob = np.mean(np.abs(EF5_incor_shap[:,:,1]), axis=0)\n",
    "EF5_incor_lob_no_abs=np.mean(EF5_incor_shap[:,:,1], axis=0)\n",
    "\n",
    "dsc_shap = np.concatenate(values_shap_dsc, axis=0)\n",
    "dsc_base = np.concatenate(base_shap_dsc, axis=0)\n",
    "dsc_pred = np.concatenate(pred_shap_dsc, axis=0)\n",
    "dsc_no_scale = np.concatenate(X_dsc, axis=0)\n",
    "dsc_lob = np.mean(np.abs(dsc_shap[:,:,1]), axis=0)\n",
    "dsc_lob_no_abs=np.mean(dsc_shap[:,:,1], axis=0)\n",
    "\n",
    "EF0_dsc_shap = np.concatenate(values_shap_dsc_EF0, axis=0)\n",
    "EF0_dsc_base = np.concatenate(base_shap_dsc_EF0, axis=0)\n",
    "EF0_dsc_pred = np.concatenate(pred_shap_dsc_EF0, axis=0)\n",
    "EF0_dsc_no_scale = np.concatenate(X_dsc_EF0, axis=0)\n",
    "EF0_dsc_lob = np.mean(np.abs(EF0_dsc_shap[:,:,1]), axis=0)\n",
    "EF0_dsc_lob_no_abs=np.mean(EF0_dsc_shap[:,:,1], axis=0)\n",
    "\n",
    "EF1_dsc_shap = np.concatenate(values_shap_dsc_EF1, axis=0)\n",
    "EF1_dsc_base = np.concatenate(base_shap_dsc_EF1, axis=0)\n",
    "EF1_dsc_pred = np.concatenate(pred_shap_dsc_EF1, axis=0)\n",
    "EF1_dsc_no_scale = np.concatenate(X_dsc_EF1, axis=0)\n",
    "EF1_dsc_lob = np.mean(np.abs(EF1_dsc_shap[:,:,1]), axis=0)\n",
    "EF1_dsc_lob_no_abs=np.mean(EF1_dsc_shap[:,:,1], axis=0)\n",
    "\n",
    "EF2_dsc_shap = np.concatenate(values_shap_dsc_EF2, axis=0)\n",
    "EF2_dsc_base = np.concatenate(base_shap_dsc_EF2, axis=0)\n",
    "EF2_dsc_pred = np.concatenate(pred_shap_dsc_EF2, axis=0)\n",
    "EF2_dsc_no_scale = np.concatenate(X_dsc_EF2, axis=0)\n",
    "EF2_dsc_lob = np.mean(np.abs(EF2_dsc_shap[:,:,1]), axis=0)\n",
    "EF2_dsc_lob_no_abs=np.mean(EF2_dsc_shap[:,:,1], axis=0)\n",
    "\n",
    "EF3_dsc_shap = np.concatenate(values_shap_dsc_EF3, axis=0)\n",
    "EF3_dsc_base = np.concatenate(base_shap_dsc_EF3, axis=0)\n",
    "EF3_dsc_pred = np.concatenate(pred_shap_dsc_EF3, axis=0)\n",
    "EF3_dsc_no_scale = np.concatenate(X_dsc_EF3, axis=0)\n",
    "EF3_dsc_lob = np.mean(np.abs(EF3_dsc_shap[:,:,1]), axis=0)\n",
    "EF3_dsc_lob_no_abs=np.mean(EF3_dsc_shap[:,:,1], axis=0)\n",
    "\n",
    "EF4_dsc_shap = np.concatenate(values_shap_dsc_EF4, axis=0)\n",
    "EF4_dsc_base = np.concatenate(base_shap_dsc_EF4, axis=0)\n",
    "EF4_dsc_pred = np.concatenate(pred_shap_dsc_EF4, axis=0)\n",
    "EF4_dsc_no_scale = np.concatenate(X_dsc_EF4, axis=0)\n",
    "EF4_dsc_lob = np.mean(np.abs(EF4_dsc_shap[:,:,1]), axis=0)\n",
    "EF4_dsc_lob_no_abs=np.mean(EF4_dsc_shap[:,:,1], axis=0)\n",
    "\n",
    "EF5_dsc_shap = np.concatenate(values_shap_dsc_EF5, axis=0)\n",
    "EF5_dsc_base = np.concatenate(base_shap_dsc_EF5, axis=0)\n",
    "EF5_dsc_pred = np.concatenate(pred_shap_dsc_EF5, axis=0)\n",
    "EF5_dsc_no_scale = np.concatenate(X_dsc_EF5, axis=0)\n",
    "EF5_dsc_lob = np.mean(np.abs(EF5_dsc_shap[:,:,1]), axis=0)\n",
    "EF5_dsc_lob_no_abs=np.mean(EF5_dsc_shap[:,:,1], axis=0)\n",
    "\n",
    "qlcs_shap = np.concatenate(values_shap_qlcs, axis=0)\n",
    "qlcs_base = np.concatenate(base_shap_qlcs, axis=0)\n",
    "qlcs_pred = np.concatenate(pred_shap_qlcs, axis=0)\n",
    "qlcs_no_scale = np.concatenate(X_qlcs, axis=0)\n",
    "qlcs_lob = np.mean(np.abs(qlcs_shap[:,:,1]), axis=0)\n",
    "qlcs_lob_no_abs=np.mean(qlcs_shap[:,:,1], axis=0)\n",
    "\n",
    "EF0_qlcs_shap = np.concatenate(values_shap_qlcs_EF0, axis=0)\n",
    "EF0_qlcs_base = np.concatenate(base_shap_qlcs_EF0, axis=0)\n",
    "EF0_qlcs_pred = np.concatenate(pred_shap_qlcs_EF0, axis=0)\n",
    "EF0_qlcs_no_scale = np.concatenate(X_qlcs_EF0, axis=0)\n",
    "EF0_qlcs_lob = np.mean(np.abs(EF0_qlcs_shap[:,:,1]), axis=0)\n",
    "EF0_qlcs_lob_no_abs=np.mean(EF0_qlcs_shap[:,:,1], axis=0)\n",
    "\n",
    "EF1_qlcs_shap = np.concatenate(values_shap_qlcs_EF1, axis=0)\n",
    "EF1_qlcs_base = np.concatenate(base_shap_qlcs_EF1, axis=0)\n",
    "EF1_qlcs_pred = np.concatenate(pred_shap_qlcs_EF1, axis=0)\n",
    "EF1_qlcs_no_scale = np.concatenate(X_qlcs_EF1, axis=0)\n",
    "EF1_qlcs_lob = np.mean(np.abs(EF1_qlcs_shap[:,:,1]), axis=0)\n",
    "EF1_qlcs_lob_no_abs=np.mean(EF1_qlcs_shap[:,:,1], axis=0)\n",
    "\n",
    "EF2_qlcs_shap = np.concatenate(values_shap_qlcs_EF2, axis=0)\n",
    "EF2_qlcs_base = np.concatenate(base_shap_qlcs_EF2, axis=0)\n",
    "EF2_qlcs_pred = np.concatenate(pred_shap_qlcs_EF2, axis=0)\n",
    "EF2_qlcs_no_scale = np.concatenate(X_qlcs_EF2, axis=0)\n",
    "EF2_qlcs_lob = np.mean(np.abs(EF2_qlcs_shap[:,:,1]), axis=0)\n",
    "EF2_qlcs_lob_no_abs=np.mean(EF2_qlcs_shap[:,:,1], axis=0)\n",
    "\n",
    "EF3_qlcs_shap = np.concatenate(values_shap_qlcs_EF3, axis=0)\n",
    "EF3_qlcs_base = np.concatenate(base_shap_qlcs_EF3, axis=0)\n",
    "EF3_qlcs_pred = np.concatenate(pred_shap_qlcs_EF3, axis=0)\n",
    "EF3_qlcs_no_scale = np.concatenate(X_qlcs_EF3, axis=0)\n",
    "EF3_qlcs_lob = np.mean(np.abs(EF3_qlcs_shap[:,:,1]), axis=0)\n",
    "EF3_qlcs_lob_no_abs=np.mean(EF3_qlcs_shap[:,:,1], axis=0)\n",
    "\n",
    "EF4_qlcs_shap = np.concatenate(values_shap_qlcs_EF4, axis=0)\n",
    "EF4_qlcs_base = np.concatenate(base_shap_qlcs_EF4, axis=0)\n",
    "EF4_qlcs_pred = np.concatenate(pred_shap_qlcs_EF4, axis=0)\n",
    "EF4_qlcs_no_scale = np.concatenate(X_qlcs_EF4, axis=0)\n",
    "EF4_qlcs_lob = np.mean(np.abs(EF4_qlcs_shap[:,:,1]), axis=0)\n",
    "EF4_qlcs_lob_no_abs=np.mean(EF4_qlcs_shap[:,:,1], axis=0)\n",
    "\n",
    "#EF5_qlcs_shap = np.concatenate(values_shap_qlcs_EF5, axis=0)\n",
    "#EF5_qlcs_base = np.concatenate(base_shap_qlcs_EF5, axis=0)\n",
    "#EF5_qlcs_pred = np.concatenate(pred_shap_qlcs_EF5, axis=0)\n",
    "#EF5_qlcs_no_scale = np.concatenate(X_qlcs_EF5, axis=0)\n",
    "#EF5_qlcs_lob = np.mean(np.abs(EF5_qlcs_shap[:,:,1]), axis=0)\n",
    "#EF5_qlcs_lob_no_abs=np.mean(EF5_qlcs_shap[:,:,1], axis=0)\n",
    "\n",
    "mul_shap = np.concatenate(values_shap_mul, axis=0)\n",
    "mul_base = np.concatenate(base_shap_mul, axis=0)\n",
    "mul_pred = np.concatenate(pred_shap_mul, axis=0)\n",
    "mul_no_scale = np.concatenate(X_mul, axis=0)\n",
    "mul_lob = np.mean(np.abs(mul_shap[:,:,1]), axis=0)\n",
    "mul_lob_no_abs=np.mean(mul_shap[:,:,1], axis=0)\n",
    "\n",
    "EF0_mul_shap = np.concatenate(values_shap_mul_EF0, axis=0)\n",
    "EF0_mul_base = np.concatenate(base_shap_mul_EF0, axis=0)\n",
    "EF0_mul_pred = np.concatenate(pred_shap_mul_EF0, axis=0)\n",
    "EF0_mul_no_scale = np.concatenate(X_mul_EF0, axis=0)\n",
    "EF0_mul_lob = np.mean(np.abs(EF0_mul_shap[:,:,1]), axis=0)\n",
    "EF0_mul_lob_no_abs=np.mean(EF0_mul_shap[:,:,1], axis=0)\n",
    "\n",
    "EF1_mul_shap = np.concatenate(values_shap_mul_EF1, axis=0)\n",
    "EF1_mul_base = np.concatenate(base_shap_mul_EF1, axis=0)\n",
    "EF1_mul_pred = np.concatenate(pred_shap_mul_EF1, axis=0)\n",
    "EF1_mul_no_scale = np.concatenate(X_mul_EF1, axis=0)\n",
    "EF1_mul_lob = np.mean(np.abs(EF1_mul_shap[:,:,1]), axis=0)\n",
    "EF1_mul_lob_no_abs=np.mean(EF1_mul_shap[:,:,1], axis=0)\n",
    "\n",
    "EF2_mul_shap = np.concatenate(values_shap_mul_EF2, axis=0)\n",
    "EF2_mul_base = np.concatenate(base_shap_mul_EF2, axis=0)\n",
    "EF2_mul_pred = np.concatenate(pred_shap_mul_EF2, axis=0)\n",
    "EF2_mul_no_scale = np.concatenate(X_mul_EF2, axis=0)\n",
    "EF2_mul_lob = np.mean(np.abs(EF2_mul_shap[:,:,1]), axis=0)\n",
    "EF2_mul_lob_no_abs=np.mean(EF2_mul_shap[:,:,1], axis=0)\n",
    "\n",
    "EF3_mul_shap = np.concatenate(values_shap_mul_EF3, axis=0)\n",
    "EF3_mul_base = np.concatenate(base_shap_mul_EF3, axis=0)\n",
    "EF3_mul_pred = np.concatenate(pred_shap_mul_EF3, axis=0)\n",
    "EF3_mul_no_scale = np.concatenate(X_mul_EF3, axis=0)\n",
    "EF3_mul_lob = np.mean(np.abs(EF3_mul_shap[:,:,1]), axis=0)\n",
    "EF3_mul_lob_no_abs=np.mean(EF3_mul_shap[:,:,1], axis=0)\n",
    "\n",
    "#EF4_mul_shap = np.concatenate(values_shap_mul_EF4, axis=0)\n",
    "#EF4_mul_base = np.concatenate(base_shap_mul_EF4, axis=0)\n",
    "#EF4_mul_pred = np.concatenate(pred_shap_mul_EF4, axis=0)\n",
    "#EF4_mul_no_scale = np.concatenate(X_mul_EF4, axis=0)\n",
    "#EF4_mul_lob = np.mean(np.abs(EF4_mul_shap[:,:,1]), axis=0)\n",
    "#EF4_mul_lob_no_abs=np.mean(EF4_mul_shap[:,:,1], axis=0)\n",
    "\n",
    "#EF5_mul_shap = np.concatenate(values_shap_mul_EF5, axis=0)\n",
    "#EF5_mul_base = np.concatenate(base_shap_mul_EF5, axis=0)\n",
    "#EF5_mul_pred = np.concatenate(pred_shap_mul_EF5, axis=0)\n",
    "#EF5_mul_no_scale = np.concatenate(X_mul_EF5, axis=0)\n",
    "#EF5_mul_lob = np.mean(np.abs(EF5_mul_shap[:,:,1]), axis=0)\n",
    "#EF5_mul_lob_no_abs=np.mean(EF5_mul_shap[:,:,1], axis=0)\n",
    "\n",
    "FP_shap = np.concatenate(values_shap_FP, axis=0)\n",
    "FP_base = np.concatenate(base_shap_FP, axis=0)\n",
    "FP_pred = np.concatenate(pred_shap_FP, axis=0)\n",
    "FP_no_scale = np.concatenate(X_FP, axis=0)\n",
    "FP_lob = np.mean(np.abs(FP_shap[:,:,1]), axis=0)\n",
    "FP_lob_no_abs=np.mean(FP_shap[:,:,1], axis=0)\n",
    "\n",
    "FP_EF1_shap = np.concatenate(values_shap_FP_EF1, axis=0)\n",
    "FP_EF1_base = np.concatenate(base_shap_FP_EF1, axis=0)\n",
    "FP_EF1_pred = np.concatenate(pred_shap_FP_EF1, axis=0)\n",
    "FP_EF1_no_scale = np.concatenate(X_FP_EF1, axis=0)\n",
    "FP_EF1_lob = np.mean(np.abs(FP_EF1_shap[:,:,1]), axis=0)\n",
    "FP_EF1_lob_no_abs=np.mean(FP_EF1_shap[:,:,1], axis=0)\n",
    "\n",
    "FN_shap = np.concatenate(values_shap_FN, axis=0)\n",
    "FN_base = np.concatenate(base_shap_FN, axis=0)\n",
    "FN_pred = np.concatenate(pred_shap_FN, axis=0)\n",
    "FN_no_scale = np.concatenate(X_FN, axis=0)\n",
    "FN_lob = np.mean(np.abs(FN_shap[:,:,1]), axis=0)\n",
    "FN_lob_no_abs=np.mean(FN_shap[:,:,1], axis=0)\n",
    "\n",
    "FN_EF2_shap = np.concatenate(values_shap_FN_EF2, axis=0)\n",
    "FN_EF2_base = np.concatenate(base_shap_FN_EF2, axis=0)\n",
    "FN_EF2_pred = np.concatenate(pred_shap_FN_EF2, axis=0)\n",
    "FN_EF2_no_scale = np.concatenate(X_FN_EF2, axis=0)\n",
    "FN_EF2_lob = np.mean(np.abs(FN_EF2_shap[:,:,1]), axis=0)\n",
    "FN_EF2_lob_no_abs=np.mean(FN_EF2_shap[:,:,1], axis=0)\n",
    "\n",
    "TP_shap = np.concatenate(values_shap_TP, axis=0)\n",
    "TP_base = np.concatenate(base_shap_TP, axis=0)\n",
    "TP_pred = np.concatenate(pred_shap_TP, axis=0)\n",
    "TP_no_scale = np.concatenate(X_TP, axis=0)\n",
    "TP_lob = np.mean(np.abs(TP_shap[:,:,1]), axis=0)\n",
    "TP_lob_no_abs=np.mean(TP_shap[:,:,1], axis=0)\n",
    "\n",
    "TN_shap = np.concatenate(values_shap_TN, axis=0)\n",
    "TN_base = np.concatenate(base_shap_TN, axis=0)\n",
    "TN_pred = np.concatenate(pred_shap_TN, axis=0)\n",
    "TN_no_scale = np.concatenate(X_TN, axis=0)\n",
    "TN_lob = np.mean(np.abs(TN_shap[:,:,1]), axis=0)\n",
    "TN_lob_no_abs=np.mean(TN_shap[:,:,1], axis=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Full data SHAP separations\n",
    "wyt = np.concatenate(values_shap, axis=0)\n",
    "print(wyt.shape)\n",
    "no_scale = np.concatenate(all_data, axis=0)\n",
    "\n",
    "nyt = np.concatenate(base_shap, axis=0)\n",
    "cyt = np.concatenate(pred_shap, axis=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Mean of full SHAP analysis\n",
    "lob=np.mean(np.abs(wyt[:,:,1]), axis=0)\n",
    "\n",
    "lob_no_abs=np.mean(wyt[:,:,1], axis=0)\n",
    "lob.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#all predictors\n",
    "shap.summary_plot(shap_values=wyt[:,:,1], features=no_scale, feature_names=names, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#all predictors\n",
    "name='radar_lob_no_abs'\n",
    "bar_legacy(lob_no_abs, no_scale, predictor_cols, 15)\n",
    "#pl.savefig('/data/keeling/a/msessa2/python/notebooks/SHAP/all_lob_abs')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#all predictors\n",
    "bar_legacy(lob_no_abs, no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#all predictors\n",
    "bar_legacy(lob_no_abs, no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Violent Cases\n",
    "shap.summary_plot(shap_values=vio_shap[:,:,1], features=vio_no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')\n",
    "plt.savefig('/data/keeling/a/msessa2/python/notebooks/SHAP/radar_vio_shap', bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Violent cases\n",
    "name='radar_vio_abs'\n",
    "bar_legacy(vio_lob, vio_no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Violent cases, no abs\n",
    "name='radar_vio_no_abs'\n",
    "bar_legacy(vio_lob_no_abs, vio_no_scale, names, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#EF0 cases\n",
    "shap.summary_plot(shap_values=EF0_shap[:,:,1], features=EF0_no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')\n",
    "plt.savefig('/data/keeling/a/msessa2/python/notebooks/SHAP/radar_EF0_shap', bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#EF0 cases\n",
    "name='radar_EF0_abs'\n",
    "bar_legacy(EF0_lob, EF0_no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#EF0 cases, no abs\n",
    "name='radar_EF0_no_abs'\n",
    "bar_legacy(EF0_lob_no_abs, EF0_no_scale, names, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#EF1 cases\n",
    "shap.summary_plot(shap_values=EF1_shap[:,:,1], features=EF1_no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')\n",
    "plt.savefig('/data/keeling/a/msessa2/python/notebooks/SHAP/radar_EF1_shap', bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#EF1 cases\n",
    "name='radar_EF1_abs'\n",
    "bar_legacy(EF1_lob, EF1_no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#EF1 cases, no abs\n",
    "name='radar_EF1_no_abs'\n",
    "bar_legacy(EF1_lob_no_abs, EF1_no_scale, names, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#EF2 cases\n",
    "shap.summary_plot(shap_values=EF2_shap[:,:,1], features=EF2_no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')\n",
    "plt.savefig('/data/keeling/a/msessa2/python/notebooks/SHAP/radar_EF2_shap', bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#EF2 cases\n",
    "name='radar_EF2_abs'\n",
    "bar_legacy(EF2_lob, EF2_no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#EF2 cases, no abs\n",
    "name='radar_EF2_no_abs'\n",
    "bar_legacy(EF2_lob_no_abs, EF2_no_scale, names, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#correct cases\n",
    "shap.summary_plot(shap_values=cor_shap[:,:,1], features=cor_no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')\n",
    "plt.savefig('/data/keeling/a/msessa2/python/notebooks/SHAP/radar_cor_shap', bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#correct cases\n",
    "name='radar_cor_abs'\n",
    "bar_legacy(cor_lob, cor_no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#correct cases, no abs\n",
    "name='radar_cor_no_abs'\n",
    "bar_legacy(cor_lob_no_abs, cor_no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#incorrect cases\n",
    "shap.summary_plot(shap_values=incor_shap[:,:,1], features=incor_no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')\n",
    "plt.savefig('/data/keeling/a/msessa2/python/notebooks/SHAP/radar_incor_shap', bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#incorrect cases\n",
    "name='radar_incor_abs'\n",
    "bar_legacy(incor_lob, incor_no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#incorrect cases, no abs\n",
    "name='radar_incor_no_abs'\n",
    "bar_legacy(incor_lob_no_abs, incor_no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#DSC cases\n",
    "shap.summary_plot(shap_values=dsc_shap[:,:,1], features=dsc_no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')\n",
    "plt.savefig('/data/keeling/a/msessa2/python/notebooks/SHAP/radar_dsc_shap', bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#DSC cases\n",
    "name='radar_dsc_abs'\n",
    "bar_legacy(dsc_lob, dsc_no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#DSC cases, no abs\n",
    "name='radar_dsc_no_abs'\n",
    "bar_legacy(dsc_lob_no_abs, dsc_no_scale, names, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#QLCS cases\n",
    "shap.summary_plot(shap_values=qlcs_shap[:,:,1], features=qlcs_no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')\n",
    "plt.savefig('/data/keeling/a/msessa2/python/notebooks/SHAP/radar_qlcs_shap', bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#QLCS cases\n",
    "name='radar_qlcs_abs'\n",
    "bar_legacy(qlcs_lob, qlcs_no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#QLCS cases, no abs\n",
    "name='radar_qlcs_no_abs'\n",
    "bar_legacy(qlcs_lob_no_abs, qlcs_no_scale, names, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#MUL cases\n",
    "shap.summary_plot(shap_values=mul_shap[:,:,1], features=mul_no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')\n",
    "plt.savefig('/data/keeling/a/msessa2/python/notebooks/SHAP/radar_mul_shap', bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#MUL cases\n",
    "name='radar_mul_abs'\n",
    "bar_legacy(mul_lob, mul_no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#MUL cases, no abs\n",
    "name='radar_mul_no_abs'\n",
    "bar_legacy(mul_lob_no_abs, mul_no_scale, names, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#TP cases\n",
    "shap.summary_plot(shap_values=TP_shap[:,:,1], features=TP_no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')\n",
    "plt.savefig('/data/keeling/a/msessa2/python/notebooks/SHAP/radar_TP_shap', bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#TP cases\n",
    "name='radar_TP_abs'\n",
    "bar_legacy(TP_lob, TP_no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#TP cases, no abs\n",
    "name='radar_TP_no_abs'\n",
    "bar_legacy(TP_lob_no_abs, TP_no_scale, names, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#TN cases\n",
    "shap.summary_plot(shap_values=TN_shap[:,:,1], features=TN_no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')\n",
    "plt.savefig('/data/keeling/a/msessa2/python/notebooks/SHAP/radar_TN_shap', bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#TN cases\n",
    "name='radar_TN_abs'\n",
    "bar_legacy(TN_lob, TN_no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#TN cases, no abs\n",
    "name='radar_TN_no_abs'\n",
    "bar_legacy(TN_lob_no_abs, TN_no_scale, names, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#FP cases\n",
    "shap.summary_plot(shap_values=FP_shap[:,:,1], features=FP_no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')\n",
    "plt.savefig('/data/keeling/a/msessa2/python/notebooks/SHAP/radar_FP_shap', bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#FP cases\n",
    "name='radar_FP_abs'\n",
    "bar_legacy(FP_lob, FP_no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#FP cases, no abs\n",
    "name='radar_FP_no_abs'\n",
    "bar_legacy(FP_lob_no_abs, FP_no_scale, names, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#FP cases, EF1\n",
    "shap.summary_plot(shap_values=FP_EF1_shap[:,:,1], features=FP_EF1_no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')\n",
    "plt.savefig('/data/keeling/a/msessa2/python/notebooks/SHAP/radar_FP_EF1_shap', bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#FP cases, EF1\n",
    "name='radar_FP_EF1_abs'\n",
    "bar_legacy(FP_EF1_lob, FP_EF1_no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#FP cases, no abs, EF1\n",
    "name='radar_FP_EF1_no_abs'\n",
    "bar_legacy(FP_EF1_lob_no_abs, FP_EF1_no_scale, names, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#FN cases\n",
    "shap.summary_plot(shap_values=FN_shap[:,:,1], features=FN_no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')\n",
    "plt.savefig('/data/keeling/a/msessa2/python/notebooks/SHAP/radar_FN_shap', bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#FN cases\n",
    "name='radar_FN_abs'\n",
    "bar_legacy(FN_lob, FN_no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#FN cases, no abs\n",
    "name='radar_FN_no_abs'\n",
    "bar_legacy(FN_lob_no_abs, FN_no_scale, names, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#FN cases, EF2\n",
    "shap.summary_plot(shap_values=FN_EF2_shap[:,:,1], features=FN_EF2_no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')\n",
    "plt.savefig('/data/keeling/a/msessa2/python/notebooks/SHAP/radar_FN_EF2_shap', bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#FN cases, EF2\n",
    "name='radar_FN_EF2_abs'\n",
    "bar_legacy(FN_EF2_lob, FN_EF2_no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#FN cases, no abs, EF2\n",
    "name='radar_FN_EF2_no_abs'\n",
    "bar_legacy(FN_EF2_lob_no_abs, FN_EF2_no_scale, names, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#all predictors\n",
    "shap.summary_plot(shap_values=wyt[:,:,1], features=no_scale, feature_names=predictor_cols, max_display=55, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#all predictors\n",
    "shap.summary_plot(shap_values=wyt[:,:,1], features=no_scale, feature_names=names, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#all predictors\n",
    "bar_legacy(lob, no_scale, names, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Radar\n",
    "shap.summary_plot(shap_values=wyt[:,:,1], features=no_scale, feature_names=names, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Radar\n",
    "bar_legacy(lob, no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#env\n",
    "shap.summary_plot(shap_values=wyt[:,:,1], features=no_scale, feature_names=predictor_cols, max_display=55, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#ENV\n",
    "shap.summary_plot(shap_values=wyt[:,:,1], features=no_scale, feature_names=names, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#ENV\n",
    "bar_legacy(lob, no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Case Study env\n",
    "bar_legacy(lob, no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#All radar\n",
    "#All Env.\n",
    "shap.summary_plot(shap_values=wyt[:,:,1], features=no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bar_legacy(lob, no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#All Env. 0.703\n",
    "shap.summary_plot(shap_values=wyt[:,:,1], features=no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "shap.summary_plot(shap_values=wyt[:,:,1], features=no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')\n",
    "#log odds"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bar_legacy(lob, no_scale, predictor_cols, 15) #log odds"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bar_legacy(lob, no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Env., no comp. 0.693\n",
    "shap.summary_plot(shap_values=wyt[:,:,1], features=no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bar_legacy(lob, no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#ENV., just thermo 0.663\n",
    "shap.summary_plot(shap_values=wyt[:,:,1], features=no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bar_legacy(lob, no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#ENV. just wind 0.677\n",
    "shap.summary_plot(shap_values=wyt[:,:,1], features=no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bar_legacy(lob, no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#ENV., just comp. 0.685\n",
    "shap.summary_plot(shap_values=wyt[:,:,1], features=no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bar_legacy(lob, no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#stp-fixed, scp-fixed, tts, 0.703\n",
    "#\n",
    "shap.summary_plot(shap_values=wyt[:,:,1], features=no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bar_legacy(lob, no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "shap.summary_plot(shap_values=wyt[:,:,1], features=no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')\n",
    "#0.6955"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bar_legacy(lob, no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "shap.summary_plot(shap_values=wyt[:,:,1], features=no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')\n",
    "#0.665, 0.6588"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bar_legacy(lob, no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "shap.summary_plot(shap_values=wyt[:,:,1], features=no_scale, feature_names=predictor_cols, max_display=15, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')\n",
    "#0.665, 0.6588"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "shap.summary_plot(shap_values=wyt[:,:,1], features=no_scale, feature_names=predictor_cols, max_display=45, show=False)\n",
    "plt.title(\"Impact on Model Prediction\", fontsize=19)\n",
    "plt.ylabel('Predictors')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bar_legacy(lob, no_scale, predictor_cols, 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#for i in range(0,47):\n",
    "#    shap.plots.scatter(shap_values[:,i,1])\n",
    "#shap.plots.scatter(wyt[:,0,1])\n",
    "for i in range (0,47):\n",
    "    shap.dependence_plot(i,wyt[:,:,1], no_scale, predictor_cols)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "nonsig_shap = np.concatenate(values_shap_nonsig, axis=0)\n",
    "nonsig_base = np.concatenate(base_shap_nonsig, axis=0)\n",
    "nonsig_pred = np.concatenate(pred_shap_nonsig, axis=0)\n",
    "\n",
    "sig_shap = np.concatenate(values_shap_sig, axis=0)\n",
    "sig_base = np.concatenate(base_shap_sig, axis=0)\n",
    "sig_pred = np.concatenate(pred_shap_sig, axis=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SHAP Scatter Plots**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#SHAP Scatter Plots\n",
    "\n",
    "#predictor_cols=['Average Pretornadic Mesocyclone Width (km)', 'Discrete Mode', 'QLCS Mode', 'Multicell Mode', 'Peak Pretornadic Mesocyclone Intensity (m/s)', 'Distance from Radar (km)']\n",
    "#names=['Avg_Meso_Distance_(Km)','Discrete', 'QLCS', 'Multi', 'Peak_meso_intensity', 'Distance(km)']\n",
    "\n",
    " \n",
    "#names = ['Avg_Meso_Distance_(Km)','Discrete', 'QLCS', 'Multi', 'Peak_meso_intensity', 'Distance(km)', 'SBCAPE_MEAN', 'SBCIN_MEAN', 'MLCAPE_MEAN', 'MLCIN_MEAN', 'MUCAPE_MEAN', '03CAPE_MEAN', 'LI_MEAN', '08bulk_MEAN', '06bulk_MEAN', '03bulk_MEAN', '01bulk_MEAN', '0500bulk_MEAN', 'BR_speed_MEAN', 'meanmotion_MEAN', 'SRH03_MEAN', 'SRH01_MEAN', 'RAPSRH500_MEAN', 'eff_base_h_ma_MEAN', 'eff_top_h_ma_MEAN', 'eff_layer_depth_ma_MEAN', 'EBS_MEAN', 'ESRH_MEAN', 'srwind_02_MEAN', 'srwind_46_MEAN', 'srwind_911_MEAN', 'lapse_36_MEAN', 'lapse_03_MEAN', 'RH_36_MEAN', 'RH_03_MEAN', 'LCL_h_MEAN', 'LFC_h_MEAN', 'LCL_LFC_hdif_MEAN', 'RH_LCL_LFC_MEAN', 'EHI01_MEAN', 'EHI03_MEAN', 'SCP_fixed_MEAN', 'SCP_eff_MEAN', 'STP_fixed_MEAN', 'tor01_EHI_MEAN', 'TTS_MEAN', 'critang_MEAN']\n",
    "#predictor_cols = ['Average Pretornadic Mesocyclone Width (km)', 'Discrete Mode', 'QLCS Mode', 'Multicell Mode', 'Peak Pretornadic Mesocyclone Intensity (m/s)', 'Distance from Radar (km)', 'SBCAPE_MEAN (J/kg)', 'SBCIN_MEAN (J/kg)', 'MLCAPE_MEAN (J/kg)', 'MLCIN_MEAN (J/kg)', 'MUCAPE_MEAN (J/kg)', '03CAPE_MEAN (J/kg)', 'LI_MEAN (C)', '08bulk_MEAN (m/s)', '06bulk_MEAN (m/s)', '03bulk_MEAN (m/s)', '01bulk_MEAN (m/s)', '0500bulk_MEAN (m/s)', 'BR_speed_MEAN (m/s)', 'meanmotion_MEAN (m/s)', 'SRH03_MEAN (m^2/s^2)', 'SRH01_MEAN (m^2/s^2)', 'RAPSRH500_MEAN (m^2/s^2)', 'eff_base_h_MEAN (m)', 'eff_top_h_MEAN (m)', 'eff_layer_depth_MEAN (m)', 'EBS_MEAN (m/s)', 'ESRH_MEAN (m^2/s^2)', 'srwind_02_MEAN (m/s)', 'srwind_46_MEAN (m/s)', 'srwind_911_MEAN (m/s)', 'lapse_36_MEAN (C/km)', 'lapse_03_MEAN (C/km)', 'RH_36_MEAN (%)', 'RH_03_MEAN (%)', 'LCL_h_MEAN (m)', 'LFC_h_MEAN (m)', 'LCL_LFC_hdif_MEAN (m)', 'RH_LCL_LFC_MEAN (%)', 'EHI01_MEAN', 'EHI03_MEAN', 'SCP_fixed_MEAN', 'SCP_eff_MEAN', 'STP_fixed_MEAN', 'tor01_EHI_MEAN', 'TTS_MEAN', 'critang_MEAN ()']\n",
    "\n",
    "predictor_cols = ['Avg_Meso_Distance_(Km)','Peak_(m/s)', 'Distance(km)', 'SBCAPE_MEAN', 'MLCAPE_MEAN', 'MLCIN_MEAN', 'MUCAPE_MEAN', 'LI_MEAN', '08bulk_MEAN', '06bulk_MEAN', '03bulk_MEAN', '01bulk_MEAN', '0500bulk_MEAN', 'BR_speed_MEAN', 'meanmotion_MEAN', 'SRH03_MEAN', 'SRH01_MEAN', 'RAPSRH500_MEAN', 'eff_base_h_ma_MEAN', 'eff_layer_depth_ma_MEAN', 'EBS_MEAN', 'ESRH_MEAN', 'lapse_36_MEAN', 'lapse_03_MEAN', 'RH_36_MEAN', 'RH_03_MEAN', 'LFC_h_MEAN', 'LCL_LFC_hdif_MEAN', 'RH_LCL_LFC_MEAN', 'EHI01_MEAN', 'EHI03_MEAN', 'SCP_fixed_MEAN', 'SCP_eff_MEAN', 'STP_fixed_MEAN', 'tor01_EHI_MEAN', 'TTS_MEAN']\n",
    "names = ['Average Pretornadic Mesocyclone Width (km)', 'Peak Pretornadic Mesocyclone Intensity (m/s)', 'Distance from Radar (km)', 'SBCAPE_MEAN (J/kg)', 'MLCAPE_MEAN (J/kg)', 'MLCIN_MEAN (J/kg)', 'MUCAPE_MEAN (J/kg)', 'LI_MEAN (C)', '08bulk_MEAN (m/s)', '06bulk_MEAN (m/s)', '03bulk_MEAN (m/s)', '01bulk_MEAN (m/s)', '0500bulk_MEAN (m/s)', 'BR_speed_MEAN (m/s)', 'meanmotion_MEAN (m/s)', 'SRH03_MEAN (m^2/s^2)', 'SRH01_MEAN (m^2/s^2)', 'RAPSRH500_MEAN (m^2/s^2)', 'eff_base_h_MEAN (m)', 'eff_layer_depth_MEAN (m)', 'EBS_MEAN (m/s)', 'ESRH_MEAN (m^2/s^2)', 'lapse_36_MEAN (C/km)', 'lapse_03_MEAN (C/km)', 'RH_36_MEAN (%)', 'RH_03_MEAN (%)', 'LFC_h_MEAN (m)', 'LCL_LFC_hdif_MEAN (m)', 'RH_LCL_LFC_MEAN (%)', 'EHI01_MEAN', 'EHI03_MEAN', 'SCP_fixed_MEAN', 'SCP_eff_MEAN', 'STP_fixed_MEAN', 'tor01_EHI_MEAN', 'TTS_MEAN']\n",
    "\n",
    "\n",
    "#names = ['SBCAPE_MEAN', 'SBCIN_MEAN', 'MLCAPE_MEAN', 'MLCIN_MEAN', 'MUCAPE_MEAN', '03CAPE_MEAN', 'LI_MEAN', '08bulk_MEAN', '06bulk_MEAN', '03bulk_MEAN', '01bulk_MEAN', '0500bulk_MEAN', 'BR_speed_MEAN', 'meanmotion_MEAN', 'SRH03_MEAN', 'SRH01_MEAN', 'RAPSRH500_MEAN', 'eff_base_h_ma_MEAN', 'eff_top_h_ma_MEAN', 'eff_layer_depth_ma_MEAN', 'EBS_MEAN', 'ESRH_MEAN', 'srwind_02_MEAN', 'srwind_46_MEAN', 'srwind_911_MEAN', 'lapse_36_MEAN', 'lapse_03_MEAN', 'RH_36_MEAN', 'RH_03_MEAN', 'LCL_h_MEAN', 'LFC_h_MEAN', 'LCL_LFC_hdif_MEAN', 'RH_LCL_LFC_MEAN', 'EHI01_MEAN', 'EHI03_MEAN', 'SCP_fixed_MEAN', 'SCP_eff_MEAN', 'STP_fixed_MEAN', 'tor01_EHI_MEAN', 'TTS_MEAN', 'critang_MEAN']\n",
    "#predictor_cols = ['SBCAPE_MEAN (J/kg)', 'SBCIN_MEAN (J/kg)', 'MLCAPE_MEAN (J/kg)', 'MLCIN_MEAN (J/kg)', 'MUCAPE_MEAN (J/kg)', '03CAPE_MEAN (J/kg)', 'LI_MEAN (C)', '08bulk_MEAN (m/s)', '06bulk_MEAN (m/s)', '03bulk_MEAN (m/s)', '01bulk_MEAN (m/s)', '0500bulk_MEAN (m/s)', 'BR_speed_MEAN (m/s)', 'meanmotion_MEAN (m/s)', 'SRH03_MEAN (m^2/s^2)', 'SRH01_MEAN (m^2/s^2)', 'RAPSRH500_MEAN (m^2/s^2)', 'eff_base_h_MEAN (m)', 'eff_top_h_MEAN (m)', 'eff_layer_depth_MEAN (m)', 'EBS_MEAN (m/s)', 'ESRH_MEAN (m^2/s^2)', 'srwind_02_MEAN (m/s)', 'srwind_46_MEAN (m/s)', 'srwind_911_MEAN (m/s)', 'lapse_36_MEAN (C/km)', 'lapse_03_MEAN (C/km)', 'RH_36_MEAN (%)', 'RH_03_MEAN (%)', 'LCL_h_MEAN (m)', 'LFC_h_MEAN (m)', 'LCL_LFC_hdif_MEAN (m)', 'RH_LCL_LFC_MEAN (%)', 'EHI01_MEAN', 'EHI03_MEAN', 'SCP_fixed_MEAN', 'SCP_eff_MEAN', 'STP_fixed_MEAN', 'tor01_EHI_MEAN', 'TTS_MEAN', 'critang_MEAN ()']\n",
    "\n",
    "#predictor_cols = ['MLCAPE_MEAN (J/kg)', '06bulk_MEAN (m/s)', '01bulk_MEAN (m/s)', 'EBS_MEAN (m/s)', 'ESRH_MEAN (m^2/s^2)', 'lapse_36_MEAN (C/km)', 'RH_36_MEAN (%)', 'SCP_fixed_MEAN', 'STP_fixed_MEAN', 'TTS_MEAN']\n",
    "#names = ['MLCAPE_MEAN', '06bulk_MEAN', '01bulk_MEAN', 'EBS_MEAN', 'ESRH_MEAN', 'lapse_36_MEAN', 'RH_36_MEAN', 'SCP_fixed_MEAN', 'STP_fixed_MEAN', 'TTS_MEAN']\n",
    "\n",
    "#for i in [names.index('SBCAPE_MEAN'), names.index('SRH03_MEAN'), names.index('EBS_MEAN'), names.index('RH_36_MEAN'), names.index('RH_03_MEAN')]:\n",
    "#for i in [names.index('RH_36_MEAN')]:\n",
    "for i in range(0,36):\n",
    "#for i in [0]:\n",
    "    coefficients=np.polyfit(no_scale[:,i], wyt[:,i,1],3)\n",
    "    #coefficients=np.polyfit(no_scale[:,i], shap[:,i,1],3)\n",
    "    poly=np.poly1d(coefficients)\n",
    "    new_x = np.linspace(np.min(no_scale[:,i]),np.max(no_scale[:,i]),400)\n",
    "    new_y=poly(new_x)\n",
    "    plt.figure(figsize=(12,12))\n",
    "    EF0 = plt.scatter(EF0_cor_no_scale[:,i], EF0_cor_shap[:,i,1], c= 'blue')\n",
    "    EF1 = plt.scatter(EF1_cor_no_scale[:,i], EF1_cor_shap[:,i,1], c= 'lightblue')\n",
    "    EF2 = plt.scatter(EF2_cor_no_scale[:,i], EF2_cor_shap[:,i,1], c= 'yellow')\n",
    "    EF3 = plt.scatter(EF3_cor_no_scale[:,i], EF3_cor_shap[:,i,1], c='orange')\n",
    "    EF4 = plt.scatter(EF4_cor_no_scale[:,i], EF4_cor_shap[:,i,1], c='red')\n",
    "    EF5 = plt.scatter(EF5_cor_no_scale[:,i], EF5_cor_shap[:,i,1], c='darkred')\n",
    "    #nonsig = plt.scatter(nonsig_no_scale[:,i], nonsig_shap[:,i,1])\n",
    "    #sig = plt.scatter(sig_no_scale[:,i], sig_shap[:,i,1])\n",
    "    plt.plot(new_x, new_y, c='black')\n",
    "    plt.axhline(0, color='black')\n",
    "    plt.axvline(new_x[np.argmin(np.abs(new_y[50:]))+50], c='black', linestyle='--')\n",
    "    #plt.xlim(np.min(no_scale[:,i]-1),np.max(no_scale[:,i])+1)\n",
    "    plt.title(names[i], fontsize=19)\n",
    "    plt.xlabel(names[i], fontsize=18)\n",
    "    plt.ylabel('SHAP Values', fontsize=18)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.legend((EF0, EF1, EF2, EF3, EF4, EF5), ('EF0', 'EF1', 'EF2', 'EF3', 'EF4', 'EF5'), loc='lower right', fontsize=16)\n",
    "    #plt.savefig('/data/keeling/a/msessa2/python/notebooks/SHAP/SHAP_scatter/final_pred/env_mul_cases_'+str(names[i]), bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sig_loc=np.where(y_test==1)\n",
    "values_shap[0][sig_loc,:,1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test=np.array(yprobs)\n",
    "test[:,:,1]\n",
    "fop_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "forecasted=[]\n",
    "obser=[]\n",
    "hist=[]\n",
    "for i in range(0,15):\n",
    "    fore, obs, h=plot_reliability_curve(y_real[i], y_proba[i])\n",
    "    forecasted.append(fore)\n",
    "    obser.append(obs)\n",
    "    hist.append(h)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Calibration Curve Definition\n",
    "def plot_calibration_curve(name, fig_index, probs, fop, mpv):\n",
    "    \"\"\"Plot calibration curve for est w/o and with calibration. \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    fig1 = plt.figure(fig_index, figsize=(15, 15))\n",
    "    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "    \n",
    "    _plot_background(axes_object=ax1, observed_labels=y_real)\n",
    "    \n",
    "    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly Calibrated\")\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    \n",
    "        \n",
    "    ax1.plot(mpv_[0], fop_[0], \"s-\", label=f'{name}',c='red')\n",
    "    ax1.plot(mpv_[1], fop_[1], \"s-\", label=f'{name}', c='brown')\n",
    "    ax1.plot(mpv_[2], fop_[2], \"s-\", label=f'{name}',c='orange')\n",
    "    ax1.plot(mpv_[3], fop_[3], \"s-\", label=f'{name}', c='purple')\n",
    "    ax1.plot(mpv_[4], fop_[4], \"s-\", label=f'{name}', c='yellow')\n",
    "    ax1.plot(mpv_[5], fop_[5], \"s-\", label=f'{name}', c='blue')\n",
    "    ax1.plot(mpv_[6], fop_[6], \"s-\", label=f'{name}', c='teal')\n",
    "    ax1.plot(mpv_[7], fop_[7], \"s-\", label=f'{name}', c='green')\n",
    "    ax1.plot(mpv_[8], fop_[8], \"s-\", label=f'{name}', c='lightblue')\n",
    "    ax1.plot(mpv_[9], fop_[9], \"s-\", label=f'{name}', c='gold')\n",
    "    ax1.plot(mpv_[10], fop_[10], \"s-\", label=f'{name}', c='pink')\n",
    "    ax1.plot(mpv_[11], fop_[11], \"s-\", label=f'{name}', c='grey')\n",
    "    ax1.plot(mpv_[12], fop_[12], \"s-\", label=f'{name}', c='lightgreen')\n",
    "    ax1.plot(mpv_[13], fop_[13], \"s-\", label=f'{name}', c='darkblue')\n",
    "    ax1.plot(mpv_[14], fop_[14], \"s-\", label=f'{name}', c='darkgreen')\n",
    "    ax1.set_ylabel(\"Fraction of positives\")\n",
    "    ax1.set_ylim([-0.05, 1.05])\n",
    "    \n",
    "    ax1.set_title(f'Calibration Plot ({name})')\n",
    "    ax1.set_xlabel(\"Mean predicted value\")\n",
    "    \n",
    "    ax2.hist(probs, range=(0, 1), bins=10, label=name, histtype=\"step\", lw=2)\n",
    "    ax2.set_xlabel(\"Mean predicted value\")\n",
    "    ax2.set_ylabel(\"Count\")\n",
    "    fig1.savefig('/data/keeling/a/msessa2/python/notebooks/Random_Forest/Final_calibration_and_CSI/RF_UC_ALL_env')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Plot code for performance diagrams\n",
    "\n",
    "\n",
    "DEFAULT_LINE_COLOUR = numpy.array([228, 26, 28], dtype=float) / 255\n",
    "DEFAULT_LINE_WIDTH = 2\n",
    "DEFAULT_BIAS_LINE_COLOUR = numpy.full(3, 152. / 255)\n",
    "DEFAULT_BIAS_LINE_WIDTH = 2\n",
    "\n",
    "LEVELS_FOR_CSI_CONTOURS = numpy.linspace(0, 1, num=11, dtype=float)\n",
    "LEVELS_FOR_BIAS_CONTOURS = numpy.array(\n",
    "    [0.25, 0.5, 0.75, 1., 1.5, 2., 3., 5.])\n",
    "\n",
    "BIAS_STRING_FORMAT = '%.2f'\n",
    "BIAS_LABEL_PADDING_PX = 10\n",
    "\n",
    "FIGURE_WIDTH_INCHES = 18\n",
    "FIGURE_HEIGHT_INCHES = 18\n",
    "\n",
    "FONT_SIZE = 20\n",
    "pyplot.rc('font', size=FONT_SIZE)\n",
    "pyplot.rc('axes', titlesize=FONT_SIZE)\n",
    "pyplot.rc('axes', labelsize=FONT_SIZE)\n",
    "pyplot.rc('xtick', labelsize=FONT_SIZE)\n",
    "pyplot.rc('ytick', labelsize=FONT_SIZE)\n",
    "pyplot.rc('legend', fontsize=FONT_SIZE)\n",
    "pyplot.rc('figure', titlesize=FONT_SIZE)\n",
    "\n",
    "\n",
    "def _get_sr_pod_grid(success_ratio_spacing=0.01, pod_spacing=0.01):\n",
    "    \"\"\"Creates grid in SR-POD (success ratio / probability of detection) space.\n",
    "    M = number of rows (unique POD values) in grid\n",
    "    N = number of columns (unique success ratios) in grid\n",
    "    :param success_ratio_spacing: Spacing between grid cells in adjacent\n",
    "        columns.\n",
    "    :param pod_spacing: Spacing between grid cells in adjacent rows.\n",
    "    :return: success_ratio_matrix: M-by-N numpy array of success ratios.\n",
    "        Success ratio increases with column index.\n",
    "    :return: pod_matrix: M-by-N numpy array of POD values.  POD decreases with\n",
    "        row index.\n",
    "    \"\"\"\n",
    "\n",
    "    num_success_ratios = 1 + int(numpy.ceil(1. / success_ratio_spacing))\n",
    "    num_pod_values = 1 + int(numpy.ceil(1. / pod_spacing))\n",
    "\n",
    "    unique_success_ratios = numpy.linspace(0., 1., num=num_success_ratios)\n",
    "    unique_pod_values = numpy.linspace(0., 1., num=num_pod_values)[::-1]\n",
    "    return numpy.meshgrid(unique_success_ratios, unique_pod_values)\n",
    "\n",
    "\n",
    "def _csi_from_sr_and_pod(success_ratio_array, pod_array):\n",
    "    \"\"\"Computes CSI (critical success index) from success ratio and POD.\n",
    "    POD = probability of detection\n",
    "    :param success_ratio_array: numpy array (any shape) of success ratios.\n",
    "    :param pod_array: numpy array (same shape) of POD values.\n",
    "    :return: csi_array: numpy array (same shape) of CSI values.\n",
    "    \"\"\"\n",
    "\n",
    "    return (success_ratio_array ** -1 + pod_array ** -1 - 1.) ** -1\n",
    "\n",
    "\n",
    "def _bias_from_sr_and_pod(success_ratio_array, pod_array):\n",
    "    \"\"\"Computes frequency bias from success ratio and POD.\n",
    "    POD = probability of detection\n",
    "    :param success_ratio_array: numpy array (any shape) of success ratios.\n",
    "    :param pod_array: numpy array (same shape) of POD values.\n",
    "    :return: frequency_bias_array: numpy array (same shape) of frequency biases.\n",
    "    \"\"\"\n",
    "\n",
    "    return pod_array / success_ratio_array\n",
    "\n",
    "\n",
    "def _get_csi_colour_scheme():\n",
    "    \"\"\"Returns colour scheme for CSI (critical success index).\n",
    "    :return: colour_map_object: Colour scheme (instance of\n",
    "        `matplotlib.colors.ListedColormap`).\n",
    "    :return: colour_norm_object: Instance of `matplotlib.colors.BoundaryNorm`,\n",
    "        defining the scale of the colour map.\n",
    "    \"\"\"\n",
    "\n",
    "    this_colour_map_object = pyplot.cm.Blues\n",
    "    this_colour_norm_object = matplotlib.colors.BoundaryNorm(\n",
    "        LEVELS_FOR_CSI_CONTOURS, this_colour_map_object.N)\n",
    "\n",
    "    rgba_matrix = this_colour_map_object(this_colour_norm_object(\n",
    "        LEVELS_FOR_CSI_CONTOURS))\n",
    "    colour_list = [\n",
    "        rgba_matrix[i, ..., :-1] for i in range(rgba_matrix.shape[0])\n",
    "    ]\n",
    "\n",
    "    colour_map_object = matplotlib.colors.ListedColormap(colour_list)\n",
    "    colour_map_object.set_under(numpy.array([1, 1, 1]))\n",
    "    colour_norm_object = matplotlib.colors.BoundaryNorm(\n",
    "        LEVELS_FOR_CSI_CONTOURS, colour_map_object.N)\n",
    "\n",
    "    return colour_map_object, colour_norm_object\n",
    "\n",
    "\n",
    "def _add_colour_bar(\n",
    "        axes_object, colour_map_object, values_to_colour, min_colour_value,\n",
    "        max_colour_value, colour_norm_object=None,\n",
    "        orientation_string='vertical', extend_min=True, extend_max=True,\n",
    "        fraction_of_axis_length=1., font_size=FONT_SIZE):\n",
    "    \"\"\"Adds colour bar to existing axes.\n",
    "    :param axes_object: Existing axes (instance of\n",
    "        `matplotlib.axes._subplots.AxesSubplot`).\n",
    "    :param colour_map_object: Colour scheme (instance of\n",
    "        `matplotlib.pyplot.cm`).\n",
    "    :param values_to_colour: numpy array of values to colour.\n",
    "    :param min_colour_value: Minimum value in colour map.\n",
    "    :param max_colour_value: Max value in colour map.\n",
    "    :param colour_norm_object: Instance of `matplotlib.colors.BoundaryNorm`,\n",
    "        defining the scale of the colour map.  If `colour_norm_object is None`,\n",
    "        will assume that scale is linear.\n",
    "    :param orientation_string: Orientation of colour bar (\"vertical\" or\n",
    "        \"horizontal\").\n",
    "    :param extend_min: Boolean flag.  If True, the bottom of the colour bar will\n",
    "        have an arrow.  If False, it will be a flat line, suggesting that lower\n",
    "        values are not possible.\n",
    "    :param extend_max: Same but for top of colour bar.\n",
    "    :param fraction_of_axis_length: Fraction of axis length (y-axis if\n",
    "        orientation is \"vertical\", x-axis if orientation is \"horizontal\")\n",
    "        occupied by colour bar.\n",
    "    :param font_size: Font size for labels on colour bar.\n",
    "    :return: colour_bar_object: Colour bar (instance of\n",
    "        `matplotlib.pyplot.colorbar`) created by this method.\n",
    "    \"\"\"\n",
    "\n",
    "    if colour_norm_object is None:\n",
    "        colour_norm_object = matplotlib.colors.Normalize(\n",
    "            vmin=min_colour_value, vmax=max_colour_value, clip=False)\n",
    "\n",
    "    scalar_mappable_object = pyplot.cm.ScalarMappable(\n",
    "        cmap=colour_map_object, norm=colour_norm_object)\n",
    "    scalar_mappable_object.set_array(values_to_colour)\n",
    "\n",
    "    if extend_min and extend_max:\n",
    "        extend_string = 'both'\n",
    "    elif extend_min:\n",
    "        extend_string = 'min'\n",
    "    elif extend_max:\n",
    "        extend_string = 'max'\n",
    "    else:\n",
    "        extend_string = 'neither'\n",
    "\n",
    "    if orientation_string == 'horizontal':\n",
    "        padding = 0.075\n",
    "    else:\n",
    "        padding = 0.05\n",
    "\n",
    "    colour_bar_object = pyplot.colorbar(\n",
    "        ax=axes_object, mappable=scalar_mappable_object,\n",
    "        orientation=orientation_string, pad=padding, extend=extend_string,\n",
    "        shrink=fraction_of_axis_length)\n",
    "\n",
    "    colour_bar_object.ax.tick_params(labelsize=font_size)\n",
    "    return colour_bar_object\n",
    "\n",
    "\n",
    "def get_points_in_perf_diagram(observed_labels, forecast_probabilities):\n",
    "    \"\"\"Creates points for performance diagram.\n",
    "    E = number of examples\n",
    "    T = number of binarization thresholds\n",
    "    :param observed_labels: length-E numpy array of class labels (integers in\n",
    "        0...1).\n",
    "    :param forecast_probabilities: length-E numpy array with forecast\n",
    "        probabilities of label = 1.\n",
    "    :return: pod_by_threshold: length-T numpy array of POD (probability of\n",
    "        detection) values.\n",
    "    :return: success_ratio_by_threshold: length-T numpy array of success ratios.\n",
    "    \"\"\"\n",
    "\n",
    "    assert numpy.all(numpy.logical_or(\n",
    "        observed_labels == 0, observed_labels == 1\n",
    "    ))\n",
    "\n",
    "    assert numpy.all(numpy.logical_and(\n",
    "        forecast_probabilities >= 0, forecast_probabilities <= 1\n",
    "    ))\n",
    "\n",
    "    observed_labels = observed_labels.astype(int)\n",
    "    binarization_thresholds = numpy.linspace(0, 1, num=1001, dtype=float)\n",
    "\n",
    "    num_thresholds = len(binarization_thresholds)\n",
    "    pod_by_threshold = numpy.full(num_thresholds, numpy.nan)\n",
    "    success_ratio_by_threshold = numpy.full(num_thresholds, numpy.nan)\n",
    "\n",
    "    for k in range(num_thresholds):\n",
    "        these_forecast_labels = (\n",
    "            forecast_probabilities >= binarization_thresholds[k]\n",
    "        ).astype(int)\n",
    "\n",
    "        this_num_hits = numpy.sum(numpy.logical_and(\n",
    "            these_forecast_labels == 1, observed_labels == 1\n",
    "        ))\n",
    "\n",
    "        this_num_false_alarms = numpy.sum(numpy.logical_and(\n",
    "            these_forecast_labels == 1, observed_labels == 0\n",
    "        ))\n",
    "\n",
    "        this_num_misses = numpy.sum(numpy.logical_and(\n",
    "            these_forecast_labels == 0, observed_labels == 1\n",
    "        ))\n",
    "\n",
    "        try:\n",
    "            pod_by_threshold[k] = (\n",
    "                float(this_num_hits) / (this_num_hits + this_num_misses)\n",
    "            )\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            success_ratio_by_threshold[k] = (\n",
    "                float(this_num_hits) / (this_num_hits + this_num_false_alarms)\n",
    "            )\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "\n",
    "    pod_by_threshold = numpy.array([1.] + pod_by_threshold.tolist() + [0.])\n",
    "    success_ratio_by_threshold = numpy.array(\n",
    "        [0.] + success_ratio_by_threshold.tolist() + [1.]\n",
    "    )\n",
    "\n",
    "    return pod_by_threshold, success_ratio_by_threshold\n",
    "\n",
    "\n",
    "def plot_performance_diagram(\n",
    "        observed_labels, forecast_probabilities,\n",
    "        line_colour=DEFAULT_LINE_COLOUR, line_width=DEFAULT_LINE_WIDTH,\n",
    "        bias_line_colour=DEFAULT_BIAS_LINE_COLOUR,\n",
    "        bias_line_width=DEFAULT_BIAS_LINE_WIDTH, axes_object=None):\n",
    "    \"\"\"Plots performance diagram.\n",
    "    E = number of examples\n",
    "    :param observed_labels: length-E numpy array of class labels (integers in\n",
    "        0...1).\n",
    "    :param forecast_probabilities: length-E numpy array with forecast\n",
    "        probabilities of label = 1.\n",
    "    :param line_colour: Colour (in any format accepted by `matplotlib.colors`).\n",
    "    :param line_width: Line width (real positive number).\n",
    "    :param bias_line_colour: Colour of contour lines for frequency bias.\n",
    "    :param bias_line_width: Width of contour lines for frequency bias.\n",
    "    :param axes_object: Will plot on these axes (instance of\n",
    "        `matplotlib.axes._subplots.AxesSubplot`).  If `axes_object is None`,\n",
    "        will create new axes.\n",
    "    :return: pod_by_threshold: See doc for `get_points_in_perf_diagram`.\n",
    "        detection) values.\n",
    "    :return: success_ratio_by_threshold: Same.\n",
    "    \"\"\"\n",
    "    pod_bt=[]\n",
    "    sr_bt=[]\n",
    "    for i in range(0,15):\n",
    "        pod_by_threshold, success_ratio_by_threshold = get_points_in_perf_diagram(\n",
    "        observed_labels=observed_labels[i],\n",
    "        forecast_probabilities=forecast_probabilities[i])\n",
    "        pod_bt.append(pod_by_threshold)\n",
    "        sr_bt.append(success_ratio_by_threshold)\n",
    "    \n",
    "\n",
    "    if axes_object is None:\n",
    "        _, axes_object = pyplot.subplots(\n",
    "            1, 1, figsize=(FIGURE_WIDTH_INCHES, FIGURE_HEIGHT_INCHES)\n",
    "        )\n",
    "\n",
    "    success_ratio_matrix, pod_matrix = _get_sr_pod_grid()\n",
    "    csi_matrix = _csi_from_sr_and_pod(success_ratio_matrix, pod_matrix)\n",
    "    frequency_bias_matrix = _bias_from_sr_and_pod(\n",
    "        success_ratio_matrix, pod_matrix)\n",
    "\n",
    "    this_colour_map_object, this_colour_norm_object = _get_csi_colour_scheme()\n",
    "\n",
    "    pyplot.contourf(\n",
    "        success_ratio_matrix, pod_matrix, csi_matrix, LEVELS_FOR_CSI_CONTOURS,\n",
    "        cmap=this_colour_map_object, norm=this_colour_norm_object, vmin=0.,\n",
    "        vmax=1., axes=axes_object)\n",
    "\n",
    "    colour_bar_object = _add_colour_bar(\n",
    "        axes_object=axes_object, colour_map_object=this_colour_map_object,\n",
    "        colour_norm_object=this_colour_norm_object,\n",
    "        values_to_colour=csi_matrix, min_colour_value=0.,\n",
    "        max_colour_value=1., orientation_string='vertical',\n",
    "        extend_min=False, extend_max=False)\n",
    "    colour_bar_object.set_label('CSI (critical success index)')\n",
    "\n",
    "    bias_colour_tuple = ()\n",
    "    for _ in range(len(LEVELS_FOR_BIAS_CONTOURS)):\n",
    "        bias_colour_tuple += (bias_line_colour,)\n",
    "\n",
    "    bias_contour_object = pyplot.contour(\n",
    "        success_ratio_matrix, pod_matrix, frequency_bias_matrix,\n",
    "        LEVELS_FOR_BIAS_CONTOURS, colors=bias_colour_tuple,\n",
    "        linewidths=bias_line_width, linestyles='dashed', axes=axes_object)\n",
    "    pyplot.clabel(\n",
    "        bias_contour_object, inline=True, inline_spacing=BIAS_LABEL_PADDING_PX,\n",
    "        fmt=BIAS_STRING_FORMAT, fontsize=FONT_SIZE)\n",
    "    nan_flags=[]\n",
    "    for i in range(0,15):\n",
    "        nan_flag = numpy.logical_or(\n",
    "            numpy.isnan(sr_bt[i]), numpy.isnan(pod_bt[i]))\n",
    "        nan_flags.append(nan_flag)\n",
    "    \n",
    "    real_ind=[]\n",
    "    final_pod=[]\n",
    "    final_sr=[]\n",
    "    length=[]\n",
    "    for i in range(0,15):\n",
    "        if not numpy.all(nan_flags[i]):\n",
    "            real_indices = numpy.where(numpy.invert(nan_flags[i]))[0]\n",
    "            real_ind.append(real_indices)\n",
    "            #print(pod_bt[i][real_indices].shape)\n",
    "            axes_object.plot(\n",
    "                sr_bt[i][real_indices],\n",
    "                pod_bt[i][real_indices], color=line_colour,\n",
    "               linestyle='solid', linewidth=line_width, alpha=0.4)\n",
    "        pod2=pod_bt[i][real_indices]\n",
    "        sr2=sr_bt[i][real_indices]\n",
    "        lengths=len(sr_bt[i][real_indices])\n",
    "        length.append(lengths)\n",
    "        final_pod.append(pod2)\n",
    "        final_sr.append(sr2)\n",
    "\n",
    "    print(length)\n",
    "    equal_pod=[]\n",
    "    equal_sr=[]\n",
    "    for i in range(0,15):\n",
    "        if len(final_pod[i])==max(length):\n",
    "            equal_pod.append(final_pod[i])\n",
    "            equal_sr.append(final_sr[i])\n",
    "    mean_pod=np.mean(equal_pod, axis=0)\n",
    "    mean_sr=np.mean(equal_sr, axis=0)\n",
    "    print(len(equal_sr))\n",
    "    print(max(length))\n",
    "\n",
    "    \n",
    "    #axes_object.plot(mean_sr, mean_pod, color='black',\n",
    "    #         label='Mean Curve',\n",
    "    #         lw=3)\n",
    "\n",
    "    axes_object.set_xlabel('Success ratio (1 - FAR)')\n",
    "    axes_object.set_ylabel('POD (probability of detection)')\n",
    "    #axes_object.legend(loc=\"lower right\", fontsize=19)\n",
    "    axes_object.set_xlim(0., 1.)\n",
    "    axes_object.set_ylim(0., 1.)\n",
    "    pyplot.savefig('/data/keeling/a/msessa2/python/notebooks/Random_Forest/Final_calibration_and_CSI/RF_CSI_ALL_radar_nomean')\n",
    "\n",
    "    return pod_by_threshold, success_ratio_by_threshold"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "plot_performance_diagram(\n",
    "        y_real, y_proba,\n",
    "        line_colour=DEFAULT_LINE_COLOUR, line_width=DEFAULT_LINE_WIDTH,\n",
    "        bias_line_colour=DEFAULT_BIAS_LINE_COLOUR,\n",
    "        bias_line_width=DEFAULT_BIAS_LINE_WIDTH, axes_object=None)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_calibration_curve('Random Forest', 1, test[:,:,1], fop_, mpv_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_calibration_curve('Random Forest', 1, test[:,:,1], fop_, mpv_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#ROC Curve Plot\n",
    "ax1.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Luck', alpha=.8)\n",
    "    \n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = sklearn.metrics.auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax1.plot(mean_fpr, mean_tpr, color='b',\n",
    "             label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "             lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax1.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                     label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "ax1.set_xlim([-0.05, 1.05])\n",
    "ax1.set_ylim([-0.05, 1.05])\n",
    "ax1.set_xlabel('False Positive Rate', fontsize=25)\n",
    "ax1.set_ylabel('True Positive Rate', fontsize=25)\n",
    "ax1.set_title('Cross-Validated ROC Curve', fontsize=42)\n",
    "ax1.legend(loc=\"lower right\", fontsize=19)\n",
    "\n",
    "\n",
    "#Precision-Recall Curve Plot\n",
    "y_real = np.concatenate(y_real)\n",
    "y_proba = np.concatenate(y_proba)\n",
    "    \n",
    "precision, recall, _ = precision_recall_curve(y_real, y_proba)\n",
    "\n",
    "ax2.plot(recall, precision, color='b', label=r'Precision-Recall (AUC = %0.2f)' % (sklearn.metrics.average_precision_score(y_real, y_proba)), lw=2, alpha=.8)\n",
    "ax2.set_xlim([-0.05, 1.05])\n",
    "ax2.set_ylim([-0.05, 1.05])\n",
    "ax2.set_xlabel('Recall', fontsize=25)\n",
    "ax2.set_ylabel('Precision', fontsize=25)\n",
    "ax2.set_title('Cross-Validated Precision Recall Curve', fontsize=42)\n",
    "ax2.legend(loc=\"lower left\", fontsize=19)\n",
    "\n",
    "fig\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig.savefig('ROC_PR_RF_radar_best')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(train_sizes, -mean_train_ll, label = 'Training score')\n",
    "plt.plot(train_sizes, -mean_val_ll, label = 'Validation score')\n",
    "plt.ylabel('Log-Loss', fontsize = 14)\n",
    "plt.xlabel('Training set size', fontsize = 14)\n",
    "plt.title('Learning Curve', fontsize = 18, y = 1.03)\n",
    "plt.legend()\n",
    "plt.savefig('LC_ll_RF_radar_best')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(train_sizes, mean_train_ac, label = 'Training score')\n",
    "plt.plot(train_sizes, mean_val_ac, label = 'Validation score')\n",
    "plt.ylabel('Accuracy', fontsize = 14)\n",
    "plt.xlabel('Training set size', fontsize = 14)\n",
    "plt.title('Learning Curve', fontsize = 18, y = 1.03)\n",
    "plt.legend()\n",
    "plt.savefig('LC_acc_RF_radar_best')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Permutation Test Plots\n",
    "\n",
    "#Test 1\n",
    "fig, ax = plt.subplots(figsize=[10,10])\n",
    "\n",
    "ax.hist(perm_scores, bins=20, density=True)\n",
    "ax.axvline(score, ls='--', color='r')\n",
    "score_label = (f\"Score on original data: {score:.2f}\\n\"\n",
    "               f\"p-value: {pvalue:.3f}\")\n",
    "ax.set_title('Permutation Test (original data)', fontsize=25)\n",
    "ax.text(0.65, 14, score_label, fontsize=15)\n",
    "ax.set_xlabel(\"Accuracy score\", fontsize=15)\n",
    "_ = ax.set_ylabel(\"Probability\", fontsize=15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig.savefig('PT1_RF_radar_best')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Test 2\n",
    "fig, ax = plt.subplots(figsize=[10,10])\n",
    "\n",
    "ax.hist(perm_scores_rand, bins=20, density=True)\n",
    "ax.set_xlim(0.13)\n",
    "ax.axvline(score_rand, ls=\"--\", color=\"r\")\n",
    "score_label = f\"Score on original\\ndata: {score_rand:.2f}\\n(p-value: {pvalue_rand:.3f})\"\n",
    "ax.text(0.14, 7.5, score_label, fontsize=15)\n",
    "ax.set_title('Permutation Test (random data)', fontsize=25)\n",
    "ax.set_xlabel(\"Accuracy score\", fontsize=15)\n",
    "ax.set_ylabel(\"Probability\", fontsize=15)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig.save_fig('PT2_DT_radar_best')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "result.best_score_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 193,
     "status": "ok",
     "timestamp": 1634057527618,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "fFy8Eqtg6X2q",
    "outputId": "efda9734-03a8-4f79-85bf-576cc721ed1f"
   },
   "source": [
    "result.best_params_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 990,
     "status": "ok",
     "timestamp": 1634057537768,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "C2I6sTl2c7kE",
    "outputId": "f199425e-ef19-4275-ff6e-d49aaf6062ec"
   },
   "source": [
    "scaler = preprocessing.RobustScaler()\n",
    "scaler.fit(X)\n",
    "training_predictor = scaler.transform(X)\n",
    "rfe.fit(training_predictor, Y.values.ravel())\n",
    "# summarize all features\n",
    "for i in range(X.shape[1]):\n",
    "\tprint('Column: %d, Selected %s, Rank: %.3f' % (i, rfe.support_[i], rfe.ranking_[i]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LIME Exploration**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16040152,
     "status": "ok",
     "timestamp": 1611106437471,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 360
    },
    "id": "MpDBqZufx1C9",
    "outputId": "8920092e-0656-467b-a850-10549c860b73"
   },
   "source": [
    "pip install lime"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DM0q2mb1x4It"
   },
   "source": [
    "import lime\n",
    "from lime import lime_tabular\n",
    "import matplotlib"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nYY6xvRdx6JC"
   },
   "source": [
    "explainer = lime_tabular.LimeTabularExplainer(training_data=training_predictor, feature_names=predictor_cols, class_names=['0', '1'], mode='classification')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "executionInfo": {
     "elapsed": 16041002,
     "status": "ok",
     "timestamp": 1611106438331,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 360
    },
    "id": "4KMNW8Cwx8aI",
    "outputId": "da06eadf-c6b0-470a-d647-42849931bb7a"
   },
   "source": [
    "i=43\n",
    "exp = explainer.explain_instance(data_row=test_predictor[i], predict_fn=rf_model.predict_proba)\n",
    "\n",
    "exp.show_in_notebook(show_table=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "executionInfo": {
     "elapsed": 16040996,
     "status": "ok",
     "timestamp": 1611106438331,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 360
    },
    "id": "5aaAK_82yB3h",
    "outputId": "12706ddb-e3b0-4404-ef03-43b4e23d9a39"
   },
   "source": [
    "fig = exp.as_pyplot_figure()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1-4AjmbrAooOZVA-F9ng4MX1ThTaRswvJ"
    },
    "executionInfo": {
     "elapsed": 16090357,
     "status": "ok",
     "timestamp": 1611106487698,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 360
    },
    "id": "xnSWwNZZyE85",
    "outputId": "470c9f5e-9918-456a-f766-c2249b4d3634"
   },
   "source": [
    "# Code for SP-LIME\n",
    "import warnings\n",
    "from lime import submodular_pick\n",
    "\n",
    "# Remember to convert the dataframe to matrix values\n",
    "# SP-LIME returns exaplanations on a sample set to provide a non redundant global decision boundary of original model\n",
    "sp_obj = submodular_pick.SubmodularPick(explainer, training_predictor, rf_model.predict_proba, num_exps_desired=5)\n",
    "\n",
    "\n",
    "[exp.show_in_notebook() for exp in sp_obj.sp_explanations];\n",
    "#[exp.as_pyplot_figure(label=exp.available_labels()[0]) for exp in sp_obj.sp_explanations];"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aZmVcxH54H90"
   },
   "source": [
    "# Make it into a dataframe\n",
    "W_pick=pd.DataFrame([dict(this.as_list(this.available_labels()[0])) for this in sp_obj.sp_explanations]).fillna(0)\n",
    " \n",
    "W_pick['prediction'] = [this.available_labels()[0] for this in sp_obj.sp_explanations]\n",
    " \n",
    "#Making a dataframe of all the explanations of sampled points\n",
    "W=pd.DataFrame([dict(this.as_list(this.available_labels()[0])) for this in sp_obj.explanations]).fillna(0)\n",
    "W['prediction'] = [this.available_labels()[0] for this in sp_obj.explanations]\n",
    " \n",
    "#Plotting the aggregate importances\n",
    "l = np.abs(W.drop(\"prediction\", axis=1)).mean(axis=0).sort_values(ascending=False).head(25).sort_values(ascending=True)\n",
    "\n",
    "#Aggregate importances split by classes\n",
    "grped_coeff = W.groupby(\"prediction\").mean()\n",
    " \n",
    "grped_coeff = grped_coeff.T\n",
    "grped_coeff[\"abs\"] = np.abs(grped_coeff.iloc[:, 0])\n",
    "grped_coeff.sort_values(\"abs\", inplace=True, ascending=False)\n",
    "f = grped_coeff.head(25).sort_values(\"abs\", ascending=True).drop(\"abs\", axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "executionInfo": {
     "elapsed": 16090351,
     "status": "ok",
     "timestamp": 1611106487702,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 360
    },
    "id": "71lDHctt4L3a",
    "outputId": "f0c3ac43-f43b-4053-806c-7c0cc969e03e"
   },
   "source": [
    "l.plot(kind='barh')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "executionInfo": {
     "elapsed": 16090661,
     "status": "ok",
     "timestamp": 1611106488019,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 360
    },
    "id": "_3xuB_yS4MyQ",
    "outputId": "7b82dc00-d1d7-4def-fed1-2e39c36ae9a4"
   },
   "source": [
    "f.plot(kind='barh')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BHqGMbYLL8aw"
   },
   "source": [
    "from sklearn.model_selection import permutation_test_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BxH6-aCBL-Xl"
   },
   "source": [
    "clf = RandomForestClassifier()\n",
    "score_iris, perm_scores_iris, pvalue_iris = permutation_test_score(\n",
    "    clf, X_scaled, Y, scoring=\"accuracy\", cv=5, n_permutations=1000)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 631,
     "status": "ok",
     "timestamp": 1623186283623,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "nWCZNVdyL_iW",
    "outputId": "036d1ff5-1b86-471f-b6c2-c2c2d4a54227"
   },
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(perm_scores_iris, bins=20, density=True)\n",
    "ax.axvline(score_iris, ls='--', color='r')\n",
    "score_label = (f\"Score on original\\ndata: {score_iris:.2f}\\n\"\n",
    "               f\"(p-value: {pvalue_iris:.3f})\")\n",
    "ax.text(0.7, 260, score_label, fontsize=12)\n",
    "ax.set_xlabel(\"Accuracy score\")\n",
    "_ = ax.set_ylabel(\"Probability\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYM5OGoY343q"
   },
   "source": [
    "# **Case Study**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IxgOwVgO39gF"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#Case Study\n",
    "#predictor_cols = ['Avg_Meso_Distance_(Km)','Discrete', 'QLCS', 'Multi',\t'Peak_(m/s)', 'Distance(km)']\n",
    "\n",
    "#predictor_cols = ['Avg_Meso_Distance_(Km)','Discrete', 'QLCS', 'Multi',\t'Peak_(m/s)', 'Distance(km)', 'SBCAPE_MEAN', 'SBCIN_MEAN', 'MLCAPE_MEAN', 'MLCIN_MEAN', 'MUCAPE_MEAN', '03CAPE_MEAN', 'LI_MEAN', '08bulk_MEAN', '06bulk_MEAN', '03bulk_MEAN', '01bulk_MEAN', '0500bulk_MEAN', 'BR_speed_MEAN', 'meanmotion_MEAN', 'SRH03_MEAN', 'SRH01_MEAN', 'RAPSRH500_MEAN', 'eff_base_h_ma_MEAN', 'eff_top_h_ma_MEAN', 'eff_layer_depth_ma_MEAN', 'EBS_MEAN', 'ESRH_MEAN', 'srwind_02_MEAN', 'srwind_46_MEAN', 'srwind_911_MEAN', 'lapse_36_MEAN', 'lapse_03_MEAN', 'RH_36_MEAN', 'RH_03_MEAN', 'LCL_h_MEAN', 'LFC_h_MEAN', 'LCL_LFC_hdif_MEAN', 'RH_LCL_LFC_MEAN', 'EHI01_MEAN', 'EHI03_MEAN', 'SCP_fixed_MEAN', 'SCP_eff_MEAN', 'STP_fixed_MEAN', 'tor01_EHI_MEAN', 'TTS_MEAN', 'critang_MEAN']\n",
    "#predictor_cols = ['SCP_fixed_MEAN', 'STP_fixed_MEAN', 'TTS_MEAN', 'EBS_MEAN', 'ESRH_MEAN', '06bulk_MEAN', 'lapse_36_MEAN', 'RH_36_MEAN', 'MLCAPE_MEAN']\n",
    "#predictor_cols = ['Avg_Meso_Distance_(Km)','Discrete', 'QLCS', 'Multi',\t'Peak_(m/s)', 'Distance(km)', 'SCP_fixed_MEAN', 'STP_fixed_MEAN', 'TTS_MEAN', 'EBS_MEAN', 'ESRH_MEAN', '06bulk_MEAN', '01bulk_MEAN', 'lapse_36_MEAN', 'RH_36_MEAN', 'MLCAPE_MEAN']\n",
    "\n",
    "#names = ['Average Pretornadic Mesocyclone Width (km)', 'Discrete Mode', 'QLCS Mode', 'Multicell Mode', 'Peak Pretornadic Mesocyclone Intensity (m/s)', 'Distance from Radar (km)']\n",
    "#name = ['Avg_Meso_Distance','Discrete', 'QLCS', 'Multi', 'Peak', 'Distance(km)']\n",
    "\n",
    "#predictor_cols = ['Avg_Meso_Distance_(Km)','Discrete', 'QLCS', 'Multi',\t'Peak_(m/s)', 'Distance(km)']\n",
    "\n",
    "#name = ['MLCAPE_MEAN', '01bulk_MEAN', 'EBS_MEAN', 'ESRH_MEAN', 'RH_36_MEAN', 'SCP_fixed_MEAN', 'STP_fixed_MEAN', 'TTS_MEAN']\n",
    "names = ['MLCAPE_MEAN (J/kg)', '01bulk_MEAN (m/s)', 'EBS_MEAN (m/s)', 'ESRH_MEAN (m^2/s^2)', 'RH_36_MEAN (%)', 'SCP_fixed_MEAN', 'STP_fixed_MEAN', 'TTS_MEAN']\n",
    "predictor_cols = ['MLCAPE_MEAN', '01bulk_MEAN', 'EBS_MEAN', 'ESRH_MEAN', 'RH_36_MEAN', 'SCP_fixed_MEAN', 'STP_fixed_MEAN', 'TTS_MEAN']\n",
    "\n",
    "\n",
    "target_col = ['Binary_EF']\n",
    "cs_test_x = case_study[predictor_cols]\n",
    "cs_test_y = case_study[target_col]\n",
    "\n",
    "train_x_cs = complete[predictor_cols]\n",
    "train_y_cs = complete[target_col]\n",
    "\n",
    "scaler = preprocessing.RobustScaler()\n",
    "scaler.fit(train_x_cs)\n",
    "training_predictor_cs = scaler.transform(train_x_cs)\n",
    "test_predictor_cs = scaler.transform(cs_test_x)\n",
    "\n",
    "#grid_search\n",
    "cv = StratifiedKFold(5, shuffle=True, random_state=4)\n",
    "\n",
    "# define search space\n",
    "n_estimators = [4, 8, 16, 32, 64, 100, 200]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = range(1,13,3)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2,3,4,5,6,7,8]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1,2,3,4,5]\n",
    "\n",
    "grid = dict(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "model= RandomForestClassifier(random_state=4)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, cv=cv, scoring=('jaccard', 'neg_brier_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'precision', 'recall', 'roc_auc', 'accuracy', 'f1'), refit='accuracy')\n",
    "\n",
    "#fit model to data\n",
    "grid_result = grid_search.fit(training_predictor_cs, train_y_cs.values.ravel())\n",
    "\n",
    "cs_best_model = grid_result.best_estimator_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Case Study\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_accuracy']\n",
    "stds = grid_result.cv_results_['std_test_accuracy']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#View RF feature importances\n",
    "cs_best_model.feature_importances_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Evaluate case study\n",
    "yhat = cs_best_model.predict(test_predictor_cs)\n",
    "yprob = cs_best_model.predict_proba(test_predictor_cs)\n",
    "cnf_matrix = sklearn.metrics.confusion_matrix(cs_test_y, yhat)\n",
    "TP = cnf_matrix[1,1]\n",
    "FP = cnf_matrix[0,1]\n",
    "FN = cnf_matrix[1,0]\n",
    "TN = cnf_matrix[0,0]\n",
    "print(cnf_matrix)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(yhat)\n",
    "print(yprob)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Custom waterfall definition to allow for unscaled feature labels\n",
    "import warnings\n",
    "try:\n",
    "    import matplotlib.pyplot as pl\n",
    "except ImportError:\n",
    "    warnings.warn(\"matplotlib could not be loaded!\")\n",
    "    pass\n",
    "from shap.plots import _labels\n",
    "from shap.utils import safe_isinstance, format_value, ordinal_str\n",
    "from shap.plots._utils import convert_ordering, convert_color, merge_nodes, get_sort_order, sort_inds, dendrogram_coords\n",
    "from shap.plots import colors\n",
    "import numpy as np\n",
    "import scipy\n",
    "import copy\n",
    "from shap._explanation import Explanation, Cohorts\n",
    "\n",
    "\n",
    "def waterfall(shap_values, features, max_display=10, show=True):\n",
    "    \"\"\" Plots an explantion of a single prediction as a waterfall plot.\n",
    "    The SHAP value of a feature represents the impact of the evidence provided by that feature on the model's\n",
    "    output. The waterfall plot is designed to visually display how the SHAP values (evidence) of each feature\n",
    "    move the model output from our prior expectation under the background data distribution, to the final model\n",
    "    prediction given the evidence of all the features. Features are sorted by the magnitude of their SHAP values\n",
    "    with the smallest magnitude features grouped together at the bottom of the plot when the number of features\n",
    "    in the models exceeds the max_display parameter.\n",
    "    Parameters\n",
    "    ----------\n",
    "    shap_values : Explanation\n",
    "        A one-dimensional Explanation object that contains the feature values and SHAP values to plot.\n",
    "    max_display : str\n",
    "        The maximum number of features to plot.\n",
    "    show : bool\n",
    "        Whether matplotlib.pyplot.show() is called before returning. Setting this to False allows the plot\n",
    "        to be customized further after it has been created.\n",
    "    \"\"\"\n",
    "\n",
    "    # Turn off interactive plot\n",
    "    if show is False:\n",
    "        plt.ioff()\n",
    "\n",
    "    base_values = shap_values.base_values\n",
    "\n",
    "    features = features\n",
    "    feature_names = shap_values.feature_names\n",
    "    lower_bounds = getattr(shap_values, \"lower_bounds\", None)\n",
    "    upper_bounds = getattr(shap_values, \"upper_bounds\", None)\n",
    "    values = shap_values.values\n",
    "\n",
    "    # make sure we only have a single output to explain\n",
    "    if (type(base_values) == np.ndarray and len(base_values) > 0) or type(base_values) == list:\n",
    "        raise Exception(\"waterfall_plot requires a scalar base_values of the model output as the first \"\n",
    "                        \"parameter, but you have passed an array as the first parameter! \"\n",
    "                        \"Try shap.waterfall_plot(explainer.base_values[0], values[0], X[0]) or \"\n",
    "                        \"for multi-output models try \"\n",
    "                        \"shap.waterfall_plot(explainer.base_values[0], values[0][0], X[0]).\")\n",
    "\n",
    "    # make sure we only have a single explanation to plot\n",
    "    if len(values.shape) == 2:\n",
    "        raise Exception(\n",
    "            \"The waterfall_plot can currently only plot a single explanation but a matrix of explanations was passed!\")\n",
    "\n",
    "    # unwrap pandas series\n",
    "    if safe_isinstance(features, \"pandas.core.series.Series\"):\n",
    "        if feature_names is None:\n",
    "            feature_names = list(features.index)\n",
    "        features = features.values\n",
    "\n",
    "    # fallback feature names\n",
    "    if feature_names is None:\n",
    "        feature_names = np.array([labels['FEATURE'] % str(i) for i in range(len(values))])\n",
    "\n",
    "    # init variables we use for tracking the plot locations\n",
    "    num_features = min(max_display, len(values))\n",
    "    row_height = 0.5\n",
    "    rng = range(num_features - 1, -1, -1)\n",
    "    order = np.argsort(-np.abs(values))\n",
    "    pos_lefts = []\n",
    "    pos_inds = []\n",
    "    pos_widths = []\n",
    "    pos_low = []\n",
    "    pos_high = []\n",
    "    neg_lefts = []\n",
    "    neg_inds = []\n",
    "    neg_widths = []\n",
    "    neg_low = []\n",
    "    neg_high = []\n",
    "    loc = base_values + values.sum()\n",
    "    yticklabels = [\"\" for i in range(num_features + 1)]\n",
    "\n",
    "    # size the plot based on how many features we are plotting\n",
    "    plt.gcf().set_size_inches(8, num_features * row_height + 1.5)\n",
    "\n",
    "    # see how many individual (vs. grouped at the end) features we are plotting\n",
    "    if num_features == len(values):\n",
    "        num_individual = num_features\n",
    "    else:\n",
    "        num_individual = num_features - 1\n",
    "\n",
    "    # compute the locations of the individual features and plot the dashed connecting lines\n",
    "    for i in range(num_individual):\n",
    "        sval = values[order[i]]\n",
    "        loc -= sval\n",
    "        if sval >= 0:\n",
    "            pos_inds.append(rng[i])\n",
    "            pos_widths.append(sval)\n",
    "            if lower_bounds is not None:\n",
    "                pos_low.append(lower_bounds[order[i]])\n",
    "                pos_high.append(upper_bounds[order[i]])\n",
    "            pos_lefts.append(loc)\n",
    "        else:\n",
    "            neg_inds.append(rng[i])\n",
    "            neg_widths.append(sval)\n",
    "            if lower_bounds is not None:\n",
    "                neg_low.append(lower_bounds[order[i]])\n",
    "                neg_high.append(upper_bounds[order[i]])\n",
    "            neg_lefts.append(loc)\n",
    "        if num_individual != num_features or i + 4 < num_individual:\n",
    "            plt.plot([loc, loc], [rng[i] - 1 - 0.4, rng[i] + 0.4],\n",
    "                     color=\"#bbbbbb\", linestyle=\"--\", linewidth=0.5, zorder=-1)\n",
    "        if features is None:\n",
    "            yticklabels[rng[i]] = feature_names[order[i]]\n",
    "        else:\n",
    "            if isinstance(features[order[i]], int) or isinstance(features[order[i]], float):\n",
    "                yticklabels[rng[i]] = format_value(features[order[i]], \"%0.03f\") + \" = \" + feature_names[order[i]]\n",
    "            else:\n",
    "                yticklabels[rng[i]] = features[order[i]] + \" = \" + feature_names[order[i]]\n",
    "\n",
    "    # add a last grouped feature to represent the impact of all the features we didn't show\n",
    "    if num_features < len(values):\n",
    "        yticklabels[0] = \"%d other features\" % (len(values) - num_features + 1)\n",
    "        remaining_impact = base_values - loc\n",
    "        if remaining_impact < 0:\n",
    "            pos_inds.append(0)\n",
    "            pos_widths.append(-remaining_impact)\n",
    "            pos_lefts.append(loc + remaining_impact)\n",
    "            c = colors.red_rgb\n",
    "        else:\n",
    "            neg_inds.append(0)\n",
    "            neg_widths.append(-remaining_impact)\n",
    "            neg_lefts.append(loc + remaining_impact)\n",
    "            c = colors.blue_rgb\n",
    "\n",
    "    points = pos_lefts + list(np.array(pos_lefts) + np.array(pos_widths)) + neg_lefts + \\\n",
    "        list(np.array(neg_lefts) + np.array(neg_widths))\n",
    "    dataw = np.max(points) - np.min(points)\n",
    "\n",
    "    # draw invisible bars just for sizing the axes\n",
    "    label_padding = np.array([0.1*dataw if w < 1 else 0 for w in pos_widths])\n",
    "    plt.barh(pos_inds, np.array(pos_widths) + label_padding + 0.02*dataw,\n",
    "             left=np.array(pos_lefts) - 0.01*dataw, color=colors.red_rgb, alpha=0)\n",
    "    label_padding = np.array([-0.1*dataw if -w < 1 else 0 for w in neg_widths])\n",
    "    plt.barh(neg_inds, np.array(neg_widths) + label_padding - 0.02*dataw,\n",
    "             left=np.array(neg_lefts) + 0.01*dataw, color=colors.blue_rgb, alpha=0)\n",
    "\n",
    "    # define variable we need for plotting the arrows\n",
    "    head_length = 0.08\n",
    "    bar_width = 0.8\n",
    "    xlen = plt.xlim()[1] - plt.xlim()[0]\n",
    "    fig = plt.gcf()\n",
    "    ax = plt.gca()\n",
    "    xticks = ax.get_xticks()\n",
    "    bbox = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "    width, height = bbox.width, bbox.height\n",
    "    bbox_to_xscale = xlen/width\n",
    "    hl_scaled = bbox_to_xscale * head_length\n",
    "    renderer = fig.canvas.get_renderer()\n",
    "\n",
    "    # draw the positive arrows\n",
    "    for i in range(len(pos_inds)):\n",
    "        dist = pos_widths[i]\n",
    "        arrow_obj = plt.arrow(\n",
    "            pos_lefts[i], pos_inds[i], max(dist-hl_scaled, 0.000001), 0,\n",
    "            head_length=min(dist, hl_scaled),\n",
    "            color=colors.red_rgb, width=bar_width,\n",
    "            head_width=bar_width\n",
    "        )\n",
    "\n",
    "        if pos_low is not None and i < len(pos_low):\n",
    "            plt.errorbar(\n",
    "                pos_lefts[i] + pos_widths[i], pos_inds[i],\n",
    "                xerr=np.array([[pos_widths[i] - pos_low[i]], [pos_high[i] - pos_widths[i]]]),\n",
    "                ecolor=colors.light_red_rgb\n",
    "            )\n",
    "\n",
    "        txt_obj = plt.text(\n",
    "            pos_lefts[i] + 0.5*dist, pos_inds[i], format_value(pos_widths[i], '%+0.02f'),\n",
    "            horizontalalignment='center', verticalalignment='center', color=\"white\",\n",
    "            fontsize=12\n",
    "        )\n",
    "        text_bbox = txt_obj.get_window_extent(renderer=renderer)\n",
    "        arrow_bbox = arrow_obj.get_window_extent(renderer=renderer)\n",
    "\n",
    "        # if the text overflows the arrow then draw it after the arrow\n",
    "        if text_bbox.width > arrow_bbox.width:\n",
    "            txt_obj.remove()\n",
    "\n",
    "            txt_obj = plt.text(\n",
    "                pos_lefts[i] + (5/72)*bbox_to_xscale + dist, pos_inds[i], format_value(pos_widths[i], '%+0.02f'),\n",
    "                horizontalalignment='left', verticalalignment='center', color=colors.red_rgb,\n",
    "                fontsize=12\n",
    "            )\n",
    "\n",
    "    # draw the negative arrows\n",
    "    for i in range(len(neg_inds)):\n",
    "        dist = neg_widths[i]\n",
    "\n",
    "        arrow_obj = plt.arrow(\n",
    "            neg_lefts[i], neg_inds[i], -max(-dist-hl_scaled, 0.000001), 0,\n",
    "            head_length=min(-dist, hl_scaled),\n",
    "            color=colors.blue_rgb, width=bar_width,\n",
    "            head_width=bar_width\n",
    "        )\n",
    "\n",
    "        if neg_low is not None and i < len(neg_low):\n",
    "            plt.errorbar(\n",
    "                neg_lefts[i] + neg_widths[i], neg_inds[i],\n",
    "                xerr=np.array([[neg_widths[i] - neg_low[i]], [neg_high[i] - neg_widths[i]]]),\n",
    "                ecolor=colors.light_blue_rgb\n",
    "            )\n",
    "\n",
    "        txt_obj = plt.text(\n",
    "            neg_lefts[i] + 0.5*dist, neg_inds[i], format_value(neg_widths[i], '%+0.02f'),\n",
    "            horizontalalignment='center', verticalalignment='center', color=\"white\",\n",
    "            fontsize=12\n",
    "        )\n",
    "        text_bbox = txt_obj.get_window_extent(renderer=renderer)\n",
    "        arrow_bbox = arrow_obj.get_window_extent(renderer=renderer)\n",
    "\n",
    "        # if the text overflows the arrow then draw it after the arrow\n",
    "        if text_bbox.width > arrow_bbox.width:\n",
    "            txt_obj.remove()\n",
    "\n",
    "            txt_obj = plt.text(\n",
    "                neg_lefts[i] - (5/72)*bbox_to_xscale + dist, neg_inds[i], format_value(neg_widths[i], '%+0.02f'),\n",
    "                horizontalalignment='right', verticalalignment='center', color=colors.blue_rgb,\n",
    "                fontsize=12\n",
    "            )\n",
    "\n",
    "    # draw the y-ticks twice, once in gray and then again with just the feature names in black\n",
    "    # The 1e-8 is so matplotlib 3.3 doesn't try and collapse the ticks\n",
    "    ytick_pos = list(range(num_features)) + list(np.arange(num_features)+1e-8)\n",
    "    plt.yticks(ytick_pos, yticklabels[:-1] + [l.split('=')[-1] for l in yticklabels[:-1]], fontsize=13)\n",
    "\n",
    "    # put horizontal lines for each feature row\n",
    "    for i in range(num_features):\n",
    "        plt.axhline(i, color=\"#cccccc\", lw=0.5, dashes=(1, 5), zorder=-1)\n",
    "\n",
    "    # mark the prior expected value and the model prediction\n",
    "    plt.axvline(base_values, 0, 1/num_features, color=\"#bbbbbb\", linestyle=\"--\", linewidth=0.5, zorder=-1)\n",
    "    fx = base_values + values.sum()\n",
    "    plt.axvline(fx, 0, 1, color=\"#bbbbbb\", linestyle=\"--\", linewidth=0.5, zorder=-1)\n",
    "\n",
    "    # clean up the main axis\n",
    "    plt.gca().xaxis.set_ticks_position('bottom')\n",
    "    plt.gca().yaxis.set_ticks_position('none')\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['left'].set_visible(False)\n",
    "    ax.tick_params(labelsize=13)\n",
    "    #plt.xlabel(\"\\nModel output\", fontsize=12)\n",
    "\n",
    "    # draw the E[f(X)] tick mark\n",
    "    xmin, xmax = ax.get_xlim()\n",
    "    ax2 = ax.twiny()\n",
    "    ax2.set_xlim(xmin, xmax)\n",
    "    ax2.set_xticks([base_values, base_values+1e-8])  # The 1e-8 is so matplotlib 3.3 doesn't try and collapse the ticks\n",
    "    ax2.set_xticklabels([\"\\n$E[f(X)]$\", \"\\n$ = \"+format_value(base_values, \"%0.03f\")+\"$\"], fontsize=12, ha=\"left\")\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['left'].set_visible(False)\n",
    "\n",
    "    # draw the f(x) tick mark\n",
    "    ax3 = ax2.twiny()\n",
    "    ax3.set_xlim(xmin, xmax)\n",
    "    # The 1e-8 is so matplotlib 3.3 doesn't try and collapse the ticks\n",
    "    ax3.set_xticks([base_values + values.sum(), base_values + values.sum() + 1e-8])\n",
    "    ax3.set_xticklabels([\"$f(x)$\", \"$ = \"+format_value(fx, \"%0.03f\")+\"$\"], fontsize=12, ha=\"left\")\n",
    "    tick_labels = ax3.xaxis.get_majorticklabels()\n",
    "    tick_labels[0].set_transform(tick_labels[0].get_transform(\n",
    "    ) + matplotlib.transforms.ScaledTranslation(-10/72., 0, fig.dpi_scale_trans))\n",
    "    tick_labels[1].set_transform(tick_labels[1].get_transform(\n",
    "    ) + matplotlib.transforms.ScaledTranslation(12/72., 0, fig.dpi_scale_trans))\n",
    "    tick_labels[1].set_color(\"#999999\")\n",
    "    ax3.spines['right'].set_visible(False)\n",
    "    ax3.spines['top'].set_visible(False)\n",
    "    ax3.spines['left'].set_visible(False)\n",
    "\n",
    "    # adjust the position of the E[f(X)] = x.xx label\n",
    "    tick_labels = ax2.xaxis.get_majorticklabels()\n",
    "    tick_labels[0].set_transform(tick_labels[0].get_transform(\n",
    "    ) + matplotlib.transforms.ScaledTranslation(-20/72., 0, fig.dpi_scale_trans))\n",
    "    tick_labels[1].set_transform(tick_labels[1].get_transform(\n",
    "    ) + matplotlib.transforms.ScaledTranslation(22/72., -1/72., fig.dpi_scale_trans))\n",
    "\n",
    "    tick_labels[1].set_color(\"#999999\")\n",
    "\n",
    "    # color the y tick labels that have the feature values as gray\n",
    "    # (these fall behind the black ones with just the feature name)\n",
    "    tick_labels = ax.yaxis.get_majorticklabels()\n",
    "    for i in range(num_features):\n",
    "        tick_labels[i].set_color(\"#999999\")\n",
    "\n",
    "    if show:\n",
    "        #plt.savefig('/data/keeling/a/msessa2/python/notebooks/SHAP/Case_Study/radar_case_'+str([b]), bbox_inches='tight')\n",
    "        plt.show()\n",
    "    else:\n",
    "        return plt.gcf()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "explainer = shap.TreeExplainer(cs_best_model, training_predictor_cs, model_output='probability', feature_names=names)\n",
    "shap_values = explainer(test_predictor_cs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "shap_values.data.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cs_test_x.values[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#SHAP plots for case study, all predictors\n",
    "for b in range(0,7):\n",
    "    waterfall(shap_values[b,:,1], features=cs_test_x.values[b], max_display=11)\n",
    "    #plt.savefig('/data/keeling/a/msessa2/python/notebooks/SHAP/Case_Study/radar_case_'+str(i), bbox_inches='tight')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "shap.summary_plot(shap_values[:,:,1], max_display=45)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "shap.plots.bar(shap_values[:,:,1], max_display=15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Extra code for single train-test split and method exploration**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gBQ9BhBlqUgo"
   },
   "source": [
    "# Initialize and train for single train, test split\n",
    "rf_model = RandomForestClassifier(n_estimators=100).fit(training_predictor, training_target_y.values.ravel())\n",
    "\n",
    "# Predict\n",
    "intensity_predict = rf_model.predict(test_predictor)\n",
    "predsprob = rf_model.predict_proba(test_predictor)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 142,
     "status": "ok",
     "timestamp": 1623175062156,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "l4azom5258tt",
    "outputId": "640c73d3-beb9-48c5-ea4b-07980c9b05b3"
   },
   "source": [
    "# Check skill with confusion matrix\n",
    "from sklearn import metrics\n",
    "cnf_matrix = metrics.confusion_matrix(test_target_y['Binary_EF'], intensity_predict)\n",
    "cnf_matrix"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1623175075017,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "Jb4BBJBZ6Cpg",
    "outputId": "04d87172-fe50-4051-eb83-914948882031"
   },
   "source": [
    "a = cnf_matrix[0,0]\n",
    "b = cnf_matrix[0,1]\n",
    "c = cnf_matrix[1,0]\n",
    "d = cnf_matrix[1,1]\n",
    "print('number of true positives (forecast = label = \"yes\"):',a)\n",
    "print('number of false positives (forecast = \"yes\" but label = \"no\"):',b)\n",
    "print('number of false negatives (forecast = \"no\" but label = \"yes\"):',c)\n",
    "print('number of true negatives (forecast = label = \"no\"):',d)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gq9Rpow27run"
   },
   "source": [
    "# Probability of Detection\n",
    "POD = a/(a+c)\n",
    "\n",
    "# Probability of False Detection\n",
    "POFD = b/(b+d)\n",
    "\n",
    "# False Alarm Ratio\n",
    "FAR = b/(a+b)\n",
    "\n",
    "# Success Ratio\n",
    "SR = 1-FAR\n",
    "\n",
    "# Frequency of Correct Nulls\n",
    "FOCN = d/(c+d)\n",
    "\n",
    "# Accuracy\n",
    "Accuracy = (a+d)/(a+b+c+d)\n",
    "\n",
    "# Critical Success Index\n",
    "CSI = a/(a+b+c)\n",
    "\n",
    "# Frequency Bias\n",
    "FB = (a+b)/(c+d)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 140,
     "status": "ok",
     "timestamp": 1623175093413,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "c7j7J9zz4yCE",
    "outputId": "9536ac23-18dc-4501-884d-54bbbe5fd7a2"
   },
   "source": [
    "print(POD)\n",
    "print(POFD)\n",
    "print(FAR)\n",
    "print(SR)\n",
    "print(FOCN)\n",
    "print(Accuracy)\n",
    "print(CSI)\n",
    "print(FB)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "executionInfo": {
     "elapsed": 479,
     "status": "ok",
     "timestamp": 1623175122090,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "_wDBI2kF7u3j",
    "outputId": "855b9926-3fb7-41d9-af39-178e3f51cd5b"
   },
   "source": [
    "# Plot confusion matrix\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ws7ObpRIl01l"
   },
   "source": [
    "import math"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 140,
     "status": "ok",
     "timestamp": 1623175125002,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "VPAFSz8H7yoP",
    "outputId": "8acfb261-2222-4a31-9bc1-afedb7ea0e3f"
   },
   "source": [
    "# Other performance metrics\n",
    "\n",
    "prob0 = []\n",
    "prob1 = []\n",
    "for i in range(len(predsprob)):\n",
    "    prob0.append(predsprob[i][0])\n",
    "    prob1.append(predsprob[i][1])\n",
    "\n",
    "#Jaccard Index, want this to be close to one\n",
    "from sklearn.metrics import jaccard_score\n",
    "j_index = jaccard_score(y_true=test_target['Binary_EF'],y_pred=intensity_predict)\n",
    "round(j_index,2)\n",
    "print('j_index:',j_index)\n",
    "\n",
    "# F1-score\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(test_target['Binary_EF'], intensity_predict)\n",
    "print('f1 score',f1)\n",
    "\n",
    "# Brier skill score\n",
    "##from sklearn.metrics import brier_score_loss\n",
    "##log_score = brier_score_loss((test_target['fzn_or_liq'].values).reshape(-1,1), predsprob[:][0])\n",
    "##print('Brier:',log_score)\n",
    "from sklearn.metrics import brier_score_loss\n",
    "brier_score = brier_score_loss(test_target['Binary_EF'],prob1)\n",
    "print('Brier:',brier_score)\n",
    "\n",
    "# Precision score\n",
    "from sklearn.metrics import precision_score\n",
    "precision_score = precision_score(test_target['Binary_EF'], intensity_predict)\n",
    "print('precision score:', precision_score)\n",
    "\n",
    "#RMSE\n",
    "print(math.sqrt(metrics.mean_squared_error(y_true=test_target['Binary_EF'],y_pred=intensity_predict)))\n",
    "\n",
    "\n",
    "#Accuracy\n",
    "print(rf_model.score(test_predictor,test_target))\n",
    "\n",
    "#Recall score\n",
    "from sklearn.metrics import recall_score\n",
    "print(recall_score(test_target['Binary_EF'], intensity_predict))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1623175169050,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "HxgyAQJFmBMT",
    "outputId": "b941a0de-a785-4d30-f39a-13f431bedb28"
   },
   "source": [
    "false_pos_rate, true_pos_rate, thresholds = metrics.roc_curve(test_target['Binary_EF'].values, prob1)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='-.', color='g')\n",
    "plt.plot(false_pos_rate, true_pos_rate)\n",
    "plt.title('Validation ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 151,
     "status": "ok",
     "timestamp": 1623175171523,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "dzUDZd7MnYIs",
    "outputId": "9582b4f4-d3d7-470f-9e50-90c5838ffcfd"
   },
   "source": [
    "metrics.roc_auc_score(test_target['Binary_EF'].values, predsprob[:,1])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!git clone https://github.com/alburke/ams-2020-ml-python-course/Advanced_Topics_In_Machine_Learning"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from ams-2020-ml-python-course/ import roc_curves\n",
    "import attr_diagrams\n",
    "import performance_diagrams "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"Methods for plotting attributes diagram.\"\"\"\n",
    "\n",
    "import numpy\n",
    "from descartes import PolygonPatch\n",
    "import shapely.geometry\n",
    "import matplotlib.colors\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "DEFAULT_NUM_BINS = 20\n",
    "RELIABILITY_LINE_COLOUR = numpy.array([228, 26, 28], dtype=float) / 255\n",
    "RELIABILITY_LINE_WIDTH = 3\n",
    "PERFECT_LINE_COLOUR = numpy.full(3, 152. / 255)\n",
    "PERFECT_LINE_WIDTH = 2\n",
    "\n",
    "NO_SKILL_LINE_COLOUR = numpy.array([31, 120, 180], dtype=float) / 255\n",
    "NO_SKILL_LINE_WIDTH = 2\n",
    "SKILL_AREA_TRANSPARENCY = 0.2\n",
    "CLIMATOLOGY_LINE_COLOUR = numpy.full(3, 152. / 255)\n",
    "CLIMATOLOGY_LINE_WIDTH = 2\n",
    "\n",
    "HISTOGRAM_FACE_COLOUR = numpy.array([228, 26, 28], dtype=float) / 255\n",
    "HISTOGRAM_EDGE_COLOUR = numpy.full(3, 0.)\n",
    "HISTOGRAM_EDGE_WIDTH = 2\n",
    "\n",
    "HISTOGRAM_LEFT_EDGE_COORD = 0.575\n",
    "HISTOGRAM_BOTTOM_EDGE_COORD = 0.175\n",
    "HISTOGRAM_WIDTH = 0.3\n",
    "HISTOGRAM_HEIGHT = 0.3\n",
    "\n",
    "HISTOGRAM_X_TICK_VALUES = numpy.linspace(0, 1, num=6, dtype=float)\n",
    "HISTOGRAM_Y_TICK_SPACING = 0.1\n",
    "\n",
    "FIGURE_WIDTH_INCHES = 15\n",
    "FIGURE_HEIGHT_INCHES = 15\n",
    "\n",
    "FONT_SIZE = 30\n",
    "pyplot.rc('font', size=FONT_SIZE)\n",
    "pyplot.rc('axes', titlesize=FONT_SIZE)\n",
    "pyplot.rc('axes', labelsize=FONT_SIZE)\n",
    "pyplot.rc('xtick', labelsize=FONT_SIZE)\n",
    "pyplot.rc('ytick', labelsize=FONT_SIZE)\n",
    "pyplot.rc('legend', fontsize=FONT_SIZE)\n",
    "pyplot.rc('figure', titlesize=FONT_SIZE)\n",
    "\n",
    "\n",
    "def _get_histogram(input_values, num_bins, min_value, max_value):\n",
    "    \"\"\"Creates histogram with uniform bin-spacing.\n",
    "    E = number of input values\n",
    "    B = number of bins\n",
    "    :param input_values: length-E numpy array of values to bin.\n",
    "    :param num_bins: Number of bins (B).\n",
    "    :param min_value: Minimum value.  Any input value < `min_value` will be\n",
    "        assigned to the first bin.\n",
    "    :param max_value: Max value.  Any input value > `max_value` will be\n",
    "        assigned to the last bin.\n",
    "    :return: inputs_to_bins: length-E numpy array of bin indices (integers).\n",
    "    \"\"\"\n",
    "\n",
    "    bin_cutoffs = numpy.linspace(min_value, max_value, num=num_bins + 1)\n",
    "    inputs_to_bins = numpy.digitize(\n",
    "        input_values, bin_cutoffs, right=False) - 1\n",
    "\n",
    "    inputs_to_bins[inputs_to_bins < 0] = 0\n",
    "    inputs_to_bins[inputs_to_bins > num_bins - 1] = num_bins - 1\n",
    "\n",
    "    return inputs_to_bins\n",
    "\n",
    "\n",
    "def _get_points_in_relia_curve(\n",
    "        observed_labels, forecast_probabilities, num_bins):\n",
    "    \"\"\"Creates points for reliability curve.\n",
    "    The reliability curve is the main component of the attributes diagram.\n",
    "    E = number of examples\n",
    "    B = number of bins\n",
    "    :param observed_labels: length-E numpy array of class labels (integers in\n",
    "        0...1).\n",
    "    :param forecast_probabilities: length-E numpy array with forecast\n",
    "        probabilities of label = 1.\n",
    "    :param num_bins: Number of bins for forecast probability.\n",
    "    :return: mean_forecast_probs: length-B numpy array of mean forecast\n",
    "        probabilities.\n",
    "    :return: mean_event_frequencies: length-B numpy array of conditional mean\n",
    "        event frequencies.  mean_event_frequencies[j] = frequency of label 1\n",
    "        when forecast probability is in the [j]th bin.\n",
    "    :return: num_examples_by_bin: length-B numpy array with number of examples\n",
    "        in each forecast bin.\n",
    "    \"\"\"\n",
    "\n",
    "    assert numpy.all(numpy.logical_or(\n",
    "        observed_labels == 0, observed_labels == 1\n",
    "    ))\n",
    "\n",
    "    assert numpy.all(numpy.logical_and(\n",
    "        forecast_probabilities >= 0, forecast_probabilities <= 1\n",
    "    ))\n",
    "\n",
    "    assert num_bins > 1\n",
    "\n",
    "    inputs_to_bins = _get_histogram(\n",
    "        input_values=forecast_probabilities, num_bins=num_bins, min_value=0.,\n",
    "        max_value=1.)\n",
    "\n",
    "    mean_forecast_probs = numpy.full(num_bins, numpy.nan)\n",
    "    mean_event_frequencies = numpy.full(num_bins, numpy.nan)\n",
    "    num_examples_by_bin = numpy.full(num_bins, -1, dtype=int)\n",
    "\n",
    "    for k in range(num_bins):\n",
    "        these_example_indices = numpy.where(inputs_to_bins == k)[0]\n",
    "        num_examples_by_bin[k] = len(these_example_indices)\n",
    "\n",
    "        mean_forecast_probs[k] = numpy.mean(\n",
    "            forecast_probabilities[these_example_indices])\n",
    "\n",
    "        mean_event_frequencies[k] = numpy.mean(\n",
    "            observed_labels[these_example_indices].astype(float)\n",
    "        )\n",
    "\n",
    "    return mean_forecast_probs, mean_event_frequencies, num_examples_by_bin\n",
    "\n",
    "\n",
    "def _vertices_to_polygon_object(x_vertices, y_vertices):\n",
    "    \"\"\"Converts two arrays of vertices to `shapely.geometry.Polygon` object.\n",
    "    V = number of vertices\n",
    "    This method allows for simple polygons only (no disjoint polygons, no\n",
    "    holes).\n",
    "    :param x_vertices: length-V numpy array of x-coordinates.\n",
    "    :param y_vertices: length-V numpy array of y-coordinates.\n",
    "    :return: polygon_object: Instance of `shapely.geometry.Polygon`.\n",
    "    \"\"\"\n",
    "\n",
    "    list_of_vertices = []\n",
    "    for i in range(len(x_vertices)):\n",
    "        list_of_vertices.append((x_vertices[i], y_vertices[i]))\n",
    "\n",
    "    return shapely.geometry.Polygon(shell=list_of_vertices)\n",
    "\n",
    "\n",
    "def _plot_background(axes_object, observed_labels):\n",
    "    \"\"\"Plots background of attributes diagram.\n",
    "    E = number of examples\n",
    "    :param axes_object: Instance of `matplotlib.axes._subplots.AxesSubplot`.\n",
    "        Will plot on these axes.\n",
    "    :param observed_labels: length-E numpy array of class labels (integers in\n",
    "        0...1).\n",
    "    \"\"\"\n",
    "\n",
    "    # Plot positive-skill area.\n",
    "    climatology = numpy.mean(observed_labels.astype(float))\n",
    "    skill_area_colour = matplotlib.colors.to_rgba(\n",
    "        NO_SKILL_LINE_COLOUR, SKILL_AREA_TRANSPARENCY)\n",
    "\n",
    "    x_vertices_left = numpy.array([0, climatology, climatology, 0, 0])\n",
    "    y_vertices_left = numpy.array([0, 0, climatology, climatology / 2, 0])\n",
    "\n",
    "    left_polygon_object = _vertices_to_polygon_object(\n",
    "        x_vertices=x_vertices_left, y_vertices=y_vertices_left)\n",
    "    left_polygon_patch = PolygonPatch(\n",
    "        left_polygon_object, lw=0, ec=skill_area_colour, fc=skill_area_colour)\n",
    "    axes_object.add_patch(left_polygon_patch)\n",
    "\n",
    "    x_vertices_right = numpy.array(\n",
    "        [climatology, 1, 1, climatology, climatology])\n",
    "    y_vertices_right = numpy.array(\n",
    "        [climatology, (1 + climatology) / 2, 1, 1, climatology])\n",
    "\n",
    "    right_polygon_object = _vertices_to_polygon_object(\n",
    "        x_vertices=x_vertices_right, y_vertices=y_vertices_right)\n",
    "    right_polygon_patch = PolygonPatch(\n",
    "        right_polygon_object, lw=0, ec=skill_area_colour, fc=skill_area_colour)\n",
    "    axes_object.add_patch(right_polygon_patch)\n",
    "\n",
    "    # Plot no-skill line (at edge of positive-skill area).\n",
    "    no_skill_x_coords = numpy.array([0, 1], dtype=float)\n",
    "    no_skill_y_coords = numpy.array([climatology, 1 + climatology]) / 2\n",
    "    axes_object.plot(\n",
    "        no_skill_x_coords, no_skill_y_coords, color=NO_SKILL_LINE_COLOUR,\n",
    "        linestyle='solid', linewidth=NO_SKILL_LINE_WIDTH)\n",
    "\n",
    "    # Plot climatology line (vertical).\n",
    "    climo_line_x_coords = numpy.full(2, climatology)\n",
    "    climo_line_y_coords = numpy.array([0, 1], dtype=float)\n",
    "    axes_object.plot(\n",
    "        climo_line_x_coords, climo_line_y_coords, color=CLIMATOLOGY_LINE_COLOUR,\n",
    "        linestyle='dashed', linewidth=CLIMATOLOGY_LINE_WIDTH)\n",
    "\n",
    "    # Plot no-resolution line (horizontal).\n",
    "    no_resolution_x_coords = climo_line_y_coords + 0.\n",
    "    no_resolution_y_coords = climo_line_x_coords + 0.\n",
    "    axes_object.plot(\n",
    "        no_resolution_x_coords, no_resolution_y_coords,\n",
    "        color=CLIMATOLOGY_LINE_COLOUR, linestyle='dashed',\n",
    "        linewidth=CLIMATOLOGY_LINE_WIDTH)\n",
    "\n",
    "\n",
    "def _floor_to_nearest(input_value_or_array, increment):\n",
    "    \"\"\"Rounds number(s) down to the nearest multiple of `increment`.\n",
    "    :param input_value_or_array: Input (either scalar or numpy array).\n",
    "    :param increment: Increment (or rounding base -- whatever you want to call\n",
    "        it).\n",
    "    :return: output_value_or_array: Rounded version of `input_value_or_array`.\n",
    "    \"\"\"\n",
    "\n",
    "    return increment * numpy.floor(input_value_or_array / increment)\n",
    "\n",
    "\n",
    "def _plot_forecast_histogram(figure_object, num_examples_by_bin):\n",
    "    \"\"\"Plots forecast histogram as inset in the attributes diagram.\n",
    "    B = number of bins\n",
    "    :param figure_object: Instance of `matplotlib.figure.Figure`.  Will plot in\n",
    "        this figure.\n",
    "    :param num_examples_by_bin: length-B numpy array, where\n",
    "        num_examples_by_bin[j] = number of examples in [j]th forecast bin.\n",
    "    \"\"\"\n",
    "\n",
    "    num_bins = len(num_examples_by_bin)\n",
    "    bin_frequencies = (\n",
    "        num_examples_by_bin.astype(float) / numpy.sum(num_examples_by_bin)\n",
    "    )\n",
    "\n",
    "    forecast_bin_edges = numpy.linspace(0, 1, num=num_bins + 1, dtype=float)\n",
    "    forecast_bin_width = forecast_bin_edges[1] - forecast_bin_edges[0]\n",
    "    forecast_bin_centers = forecast_bin_edges[:-1] + forecast_bin_width / 2\n",
    "\n",
    "    inset_axes_object = figure_object.add_axes(\n",
    "        [HISTOGRAM_LEFT_EDGE_COORD, HISTOGRAM_BOTTOM_EDGE_COORD,\n",
    "         HISTOGRAM_WIDTH, HISTOGRAM_HEIGHT]\n",
    "    )\n",
    "\n",
    "    inset_axes_object.bar(\n",
    "        forecast_bin_centers, bin_frequencies, forecast_bin_width,\n",
    "        color=HISTOGRAM_FACE_COLOUR, edgecolor=HISTOGRAM_EDGE_COLOUR,\n",
    "        linewidth=HISTOGRAM_EDGE_WIDTH)\n",
    "\n",
    "    max_y_tick_value = _floor_to_nearest(\n",
    "        1.05 * numpy.max(bin_frequencies), HISTOGRAM_Y_TICK_SPACING)\n",
    "    num_y_ticks = 1 + int(numpy.round(\n",
    "        max_y_tick_value / HISTOGRAM_Y_TICK_SPACING\n",
    "    ))\n",
    "\n",
    "    y_tick_values = numpy.linspace(0, max_y_tick_value, num=num_y_ticks)\n",
    "    pyplot.yticks(y_tick_values, axes=inset_axes_object)\n",
    "    pyplot.xticks(HISTOGRAM_X_TICK_VALUES, axes=inset_axes_object)\n",
    "\n",
    "    inset_axes_object.set_xlim(0, 1)\n",
    "    inset_axes_object.set_ylim(0, 1.05 * numpy.max(bin_frequencies))\n",
    "\n",
    "\n",
    "def plot_reliability_curve(\n",
    "        observed_labels, forecast_probabilities, num_bins=DEFAULT_NUM_BINS,\n",
    "        axes_object=None):\n",
    "    \"\"\"Plots reliability curve.\n",
    "    E = number of examples\n",
    "    :param observed_labels: length-E numpy array of class labels (integers in\n",
    "        0...1).\n",
    "    :param forecast_probabilities: length-E numpy array with forecast\n",
    "        probabilities of label = 1.\n",
    "    :param num_bins: Number of bins for forecast probability.\n",
    "    :param axes_object: Instance of `matplotlib.axes._subplots.AxesSubplot`.\n",
    "        Will plot on these axes.\n",
    "    :return: mean_forecast_probs: See doc for `_get_points_in_relia_curve`.\n",
    "    :return: mean_event_frequencies: Same.\n",
    "    :return: num_examples_by_bin: Same.\n",
    "    \"\"\"\n",
    "\n",
    "    mean_forecast_probs, mean_event_frequencies, num_examples_by_bin = (\n",
    "        _get_points_in_relia_curve(\n",
    "            observed_labels=observed_labels,\n",
    "            forecast_probabilities=forecast_probabilities, num_bins=num_bins)\n",
    "    )\n",
    "\n",
    "    if axes_object is None:\n",
    "        _, axes_object = pyplot.subplots(\n",
    "            1, 1, figsize=(FIGURE_WIDTH_INCHES, FIGURE_HEIGHT_INCHES)\n",
    "        )\n",
    "\n",
    "    perfect_x_coords = numpy.array([0, 1], dtype=float)\n",
    "    perfect_y_coords = perfect_x_coords + 0.\n",
    "    axes_object.plot(\n",
    "        perfect_x_coords, perfect_y_coords, color=PERFECT_LINE_COLOUR,\n",
    "        linestyle='dashed', linewidth=PERFECT_LINE_WIDTH)\n",
    "\n",
    "    real_indices = numpy.where(numpy.invert(numpy.logical_or(\n",
    "        numpy.isnan(mean_forecast_probs), numpy.isnan(mean_event_frequencies)\n",
    "    )))[0]\n",
    "\n",
    "    axes_object.plot(\n",
    "        mean_forecast_probs[real_indices], mean_event_frequencies[real_indices],\n",
    "        color=RELIABILITY_LINE_COLOUR,\n",
    "        linestyle='solid', linewidth=RELIABILITY_LINE_WIDTH)\n",
    "\n",
    "    axes_object.set_xlabel('Forecast probability')\n",
    "    axes_object.set_ylabel('Conditional event frequency')\n",
    "    axes_object.set_xlim(0., 1.)\n",
    "    axes_object.set_ylim(0., 1.)\n",
    "\n",
    "    return mean_forecast_probs, mean_event_frequencies, num_examples_by_bin\n",
    "\n",
    "\n",
    "def plot_attributes_diagram(\n",
    "        observed_labels, forecast_probabilities, num_bins=DEFAULT_NUM_BINS, figure_object=None, axes_object=None):\n",
    "    \"\"\"Plots attributes diagram.\n",
    "    :param observed_labels: See doc for `plot_reliability_curve`.\n",
    "    :param forecast_probabilities: Same.\n",
    "    :param num_bins: Same.\n",
    "    :return: mean_forecast_probs: See doc for `_get_points_in_relia_curve`.\n",
    "    :return: mean_event_frequencies: Same.\n",
    "    :return: num_examples_by_bin: Same.\n",
    "    \"\"\"\n",
    "\n",
    "    mean_forecast_probs, mean_event_frequencies, num_examples_by_bin = (\n",
    "        _get_points_in_relia_curve(\n",
    "            observed_labels=observed_labels,\n",
    "            forecast_probabilities=forecast_probabilities, num_bins=num_bins)\n",
    "    )\n",
    "\n",
    "    figure_object, axes_object = pyplot.subplots(\n",
    "        1, 1, figsize=(FIGURE_WIDTH_INCHES, FIGURE_HEIGHT_INCHES)\n",
    "    )\n",
    "\n",
    "    _plot_background(axes_object=axes_object, observed_labels=observed_labels)\n",
    "    #_plot_forecast_histogram(figure_object=figure_object,\n",
    "    #                         num_examples_by_bin=num_examples_by_bin)\n",
    "\n",
    "    plot_reliability_curve(\n",
    "        observed_labels=observed_labels,\n",
    "        forecast_probabilities=forecast_probabilities, num_bins=num_bins,\n",
    "        axes_object=axes_object)\n",
    "\n",
    "    return mean_forecast_probs, mean_event_frequencies, num_examples_by_bin\n",
    "\n",
    "import numpy\n",
    "import matplotlib.colors\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "DEFAULT_LINE_COLOUR = numpy.array([228, 26, 28], dtype=float) / 255\n",
    "DEFAULT_LINE_WIDTH = 3\n",
    "DEFAULT_BIAS_LINE_COLOUR = numpy.full(3, 152. / 255)\n",
    "DEFAULT_BIAS_LINE_WIDTH = 2\n",
    "\n",
    "LEVELS_FOR_CSI_CONTOURS = numpy.linspace(0, 1, num=11, dtype=float)\n",
    "LEVELS_FOR_BIAS_CONTOURS = numpy.array(\n",
    "    [0.25, 0.5, 0.75, 1., 1.5, 2., 3., 5.])\n",
    "\n",
    "BIAS_STRING_FORMAT = '%.2f'\n",
    "BIAS_LABEL_PADDING_PX = 10\n",
    "\n",
    "FIGURE_WIDTH_INCHES = 10\n",
    "FIGURE_HEIGHT_INCHES = 10\n",
    "\n",
    "FONT_SIZE = 20\n",
    "pyplot.rc('font', size=FONT_SIZE)\n",
    "pyplot.rc('axes', titlesize=FONT_SIZE)\n",
    "pyplot.rc('axes', labelsize=FONT_SIZE)\n",
    "pyplot.rc('xtick', labelsize=FONT_SIZE)\n",
    "pyplot.rc('ytick', labelsize=FONT_SIZE)\n",
    "pyplot.rc('legend', fontsize=FONT_SIZE)\n",
    "pyplot.rc('figure', titlesize=FONT_SIZE)\n",
    "\n",
    "\n",
    "def _get_sr_pod_grid(success_ratio_spacing=0.01, pod_spacing=0.01):\n",
    "    \"\"\"Creates grid in SR-POD (success ratio / probability of detection) space.\n",
    "    M = number of rows (unique POD values) in grid\n",
    "    N = number of columns (unique success ratios) in grid\n",
    "    :param success_ratio_spacing: Spacing between grid cells in adjacent\n",
    "        columns.\n",
    "    :param pod_spacing: Spacing between grid cells in adjacent rows.\n",
    "    :return: success_ratio_matrix: M-by-N numpy array of success ratios.\n",
    "        Success ratio increases with column index.\n",
    "    :return: pod_matrix: M-by-N numpy array of POD values.  POD decreases with\n",
    "        row index.\n",
    "    \"\"\"\n",
    "\n",
    "    num_success_ratios = 1 + int(numpy.ceil(1. / success_ratio_spacing))\n",
    "    num_pod_values = 1 + int(numpy.ceil(1. / pod_spacing))\n",
    "\n",
    "    unique_success_ratios = numpy.linspace(0., 1., num=num_success_ratios)\n",
    "    unique_pod_values = numpy.linspace(0., 1., num=num_pod_values)[::-1]\n",
    "    return numpy.meshgrid(unique_success_ratios, unique_pod_values)\n",
    "\n",
    "\n",
    "def _csi_from_sr_and_pod(success_ratio_array, pod_array):\n",
    "    \"\"\"Computes CSI (critical success index) from success ratio and POD.\n",
    "    POD = probability of detection\n",
    "    :param success_ratio_array: numpy array (any shape) of success ratios.\n",
    "    :param pod_array: numpy array (same shape) of POD values.\n",
    "    :return: csi_array: numpy array (same shape) of CSI values.\n",
    "    \"\"\"\n",
    "\n",
    "    return (success_ratio_array ** -1 + pod_array ** -1 - 1.) ** -1\n",
    "\n",
    "\n",
    "def _bias_from_sr_and_pod(success_ratio_array, pod_array):\n",
    "    \"\"\"Computes frequency bias from success ratio and POD.\n",
    "    POD = probability of detection\n",
    "    :param success_ratio_array: numpy array (any shape) of success ratios.\n",
    "    :param pod_array: numpy array (same shape) of POD values.\n",
    "    :return: frequency_bias_array: numpy array (same shape) of frequency biases.\n",
    "    \"\"\"\n",
    "\n",
    "    return pod_array / success_ratio_array\n",
    "\n",
    "\n",
    "def _get_csi_colour_scheme():\n",
    "    \"\"\"Returns colour scheme for CSI (critical success index).\n",
    "    :return: colour_map_object: Colour scheme (instance of\n",
    "        `matplotlib.colors.ListedColormap`).\n",
    "    :return: colour_norm_object: Instance of `matplotlib.colors.BoundaryNorm`,\n",
    "        defining the scale of the colour map.\n",
    "    \"\"\"\n",
    "\n",
    "    this_colour_map_object = pyplot.cm.Blues\n",
    "    this_colour_norm_object = matplotlib.colors.BoundaryNorm(\n",
    "        LEVELS_FOR_CSI_CONTOURS, this_colour_map_object.N)\n",
    "\n",
    "    rgba_matrix = this_colour_map_object(this_colour_norm_object(\n",
    "        LEVELS_FOR_CSI_CONTOURS))\n",
    "    colour_list = [\n",
    "        rgba_matrix[i, ..., :-1] for i in range(rgba_matrix.shape[0])\n",
    "    ]\n",
    "\n",
    "    colour_map_object = matplotlib.colors.ListedColormap(colour_list)\n",
    "    colour_map_object.set_under(numpy.array([1, 1, 1]))\n",
    "    colour_norm_object = matplotlib.colors.BoundaryNorm(\n",
    "        LEVELS_FOR_CSI_CONTOURS, colour_map_object.N)\n",
    "\n",
    "    return colour_map_object, colour_norm_object\n",
    "\n",
    "\n",
    "def _add_colour_bar(\n",
    "        axes_object, colour_map_object, values_to_colour, min_colour_value,\n",
    "        max_colour_value, colour_norm_object=None,\n",
    "        orientation_string='vertical', extend_min=True, extend_max=True,\n",
    "        fraction_of_axis_length=1., font_size=FONT_SIZE):\n",
    "    \"\"\"Adds colour bar to existing axes.\n",
    "    :param axes_object: Existing axes (instance of\n",
    "        `matplotlib.axes._subplots.AxesSubplot`).\n",
    "    :param colour_map_object: Colour scheme (instance of\n",
    "        `matplotlib.pyplot.cm`).\n",
    "    :param values_to_colour: numpy array of values to colour.\n",
    "    :param min_colour_value: Minimum value in colour map.\n",
    "    :param max_colour_value: Max value in colour map.\n",
    "    :param colour_norm_object: Instance of `matplotlib.colors.BoundaryNorm`,\n",
    "        defining the scale of the colour map.  If `colour_norm_object is None`,\n",
    "        will assume that scale is linear.\n",
    "    :param orientation_string: Orientation of colour bar (\"vertical\" or\n",
    "        \"horizontal\").\n",
    "    :param extend_min: Boolean flag.  If True, the bottom of the colour bar will\n",
    "        have an arrow.  If False, it will be a flat line, suggesting that lower\n",
    "        values are not possible.\n",
    "    :param extend_max: Same but for top of colour bar.\n",
    "    :param fraction_of_axis_length: Fraction of axis length (y-axis if\n",
    "        orientation is \"vertical\", x-axis if orientation is \"horizontal\")\n",
    "        occupied by colour bar.\n",
    "    :param font_size: Font size for labels on colour bar.\n",
    "    :return: colour_bar_object: Colour bar (instance of\n",
    "        `matplotlib.pyplot.colorbar`) created by this method.\n",
    "    \"\"\"\n",
    "\n",
    "    if colour_norm_object is None:\n",
    "        colour_norm_object = matplotlib.colors.Normalize(\n",
    "            vmin=min_colour_value, vmax=max_colour_value, clip=False)\n",
    "\n",
    "    scalar_mappable_object = pyplot.cm.ScalarMappable(\n",
    "        cmap=colour_map_object, norm=colour_norm_object)\n",
    "    scalar_mappable_object.set_array(values_to_colour)\n",
    "\n",
    "    if extend_min and extend_max:\n",
    "        extend_string = 'both'\n",
    "    elif extend_min:\n",
    "        extend_string = 'min'\n",
    "    elif extend_max:\n",
    "        extend_string = 'max'\n",
    "    else:\n",
    "        extend_string = 'neither'\n",
    "\n",
    "    if orientation_string == 'horizontal':\n",
    "        padding = 0.075\n",
    "    else:\n",
    "        padding = 0.05\n",
    "\n",
    "    colour_bar_object = pyplot.colorbar(\n",
    "        ax=axes_object, mappable=scalar_mappable_object,\n",
    "        orientation=orientation_string, pad=padding, extend=extend_string,\n",
    "        shrink=fraction_of_axis_length)\n",
    "\n",
    "    colour_bar_object.ax.tick_params(labelsize=font_size)\n",
    "    return colour_bar_object\n",
    "\n",
    "\n",
    "def get_points_in_perf_diagram(observed_labels, forecast_probabilities):\n",
    "    \"\"\"Creates points for performance diagram.\n",
    "    E = number of examples\n",
    "    T = number of binarization thresholds\n",
    "    :param observed_labels: length-E numpy array of class labels (integers in\n",
    "        0...1).\n",
    "    :param forecast_probabilities: length-E numpy array with forecast\n",
    "        probabilities of label = 1.\n",
    "    :return: pod_by_threshold: length-T numpy array of POD (probability of\n",
    "        detection) values.\n",
    "    :return: success_ratio_by_threshold: length-T numpy array of success ratios.\n",
    "    \"\"\"\n",
    "\n",
    "    assert numpy.all(numpy.logical_or(\n",
    "        observed_labels == 0, observed_labels == 1\n",
    "    ))\n",
    "\n",
    "    assert numpy.all(numpy.logical_and(\n",
    "        forecast_probabilities >= 0, forecast_probabilities <= 1\n",
    "    ))\n",
    "\n",
    "    observed_labels = observed_labels.astype(int)\n",
    "    binarization_thresholds = numpy.linspace(0, 1, num=1001, dtype=float)\n",
    "\n",
    "    num_thresholds = len(binarization_thresholds)\n",
    "    pod_by_threshold = numpy.full(num_thresholds, numpy.nan)\n",
    "    success_ratio_by_threshold = numpy.full(num_thresholds, numpy.nan)\n",
    "\n",
    "    for k in range(num_thresholds):\n",
    "        these_forecast_labels = (\n",
    "            forecast_probabilities >= binarization_thresholds[k]\n",
    "        ).astype(int)\n",
    "\n",
    "        this_num_hits = numpy.sum(numpy.logical_and(\n",
    "            these_forecast_labels == 1, observed_labels == 1\n",
    "        ))\n",
    "\n",
    "        this_num_false_alarms = numpy.sum(numpy.logical_and(\n",
    "            these_forecast_labels == 1, observed_labels == 0\n",
    "        ))\n",
    "\n",
    "        this_num_misses = numpy.sum(numpy.logical_and(\n",
    "            these_forecast_labels == 0, observed_labels == 1\n",
    "        ))\n",
    "\n",
    "        try:\n",
    "            pod_by_threshold[k] = (\n",
    "                float(this_num_hits) / (this_num_hits + this_num_misses)\n",
    "            )\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            success_ratio_by_threshold[k] = (\n",
    "                float(this_num_hits) / (this_num_hits + this_num_false_alarms)\n",
    "            )\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "\n",
    "    pod_by_threshold = numpy.array([1.] + pod_by_threshold.tolist() + [0.])\n",
    "    success_ratio_by_threshold = numpy.array(\n",
    "        [0.] + success_ratio_by_threshold.tolist() + [1.]\n",
    "    )\n",
    "\n",
    "    return pod_by_threshold, success_ratio_by_threshold\n",
    "\n",
    "\n",
    "def plot_performance_diagram(\n",
    "        observed_labels, forecast_probabilities,\n",
    "        line_colour=DEFAULT_LINE_COLOUR, line_width=DEFAULT_LINE_WIDTH,\n",
    "        bias_line_colour=DEFAULT_BIAS_LINE_COLOUR,\n",
    "        bias_line_width=DEFAULT_BIAS_LINE_WIDTH, axes_object=None):\n",
    "    \"\"\"Plots performance diagram.\n",
    "    E = number of examples\n",
    "    :param observed_labels: length-E numpy array of class labels (integers in\n",
    "        0...1).\n",
    "    :param forecast_probabilities: length-E numpy array with forecast\n",
    "        probabilities of label = 1.\n",
    "    :param line_colour: Colour (in any format accepted by `matplotlib.colors`).\n",
    "    :param line_width: Line width (real positive number).\n",
    "    :param bias_line_colour: Colour of contour lines for frequency bias.\n",
    "    :param bias_line_width: Width of contour lines for frequency bias.\n",
    "    :param axes_object: Will plot on these axes (instance of\n",
    "        `matplotlib.axes._subplots.AxesSubplot`).  If `axes_object is None`,\n",
    "        will create new axes.\n",
    "    :return: pod_by_threshold: See doc for `get_points_in_perf_diagram`.\n",
    "        detection) values.\n",
    "    :return: success_ratio_by_threshold: Same.\n",
    "    \"\"\"\n",
    "\n",
    "    pod_by_threshold, success_ratio_by_threshold = get_points_in_perf_diagram(\n",
    "        observed_labels=observed_labels,\n",
    "        forecast_probabilities=forecast_probabilities)\n",
    "\n",
    "    if axes_object is None:\n",
    "        _, axes_object = pyplot.subplots(\n",
    "            1, 1, figsize=(FIGURE_WIDTH_INCHES, FIGURE_HEIGHT_INCHES)\n",
    "        )\n",
    "\n",
    "    success_ratio_matrix, pod_matrix = _get_sr_pod_grid()\n",
    "    csi_matrix = _csi_from_sr_and_pod(success_ratio_matrix, pod_matrix)\n",
    "    frequency_bias_matrix = _bias_from_sr_and_pod(\n",
    "        success_ratio_matrix, pod_matrix)\n",
    "\n",
    "    this_colour_map_object, this_colour_norm_object = _get_csi_colour_scheme()\n",
    "\n",
    "    pyplot.contourf(\n",
    "        success_ratio_matrix, pod_matrix, csi_matrix, LEVELS_FOR_CSI_CONTOURS,\n",
    "        cmap=this_colour_map_object, norm=this_colour_norm_object, vmin=0.,\n",
    "        vmax=1., axes=axes_object)\n",
    "\n",
    "    colour_bar_object = _add_colour_bar(\n",
    "        axes_object=axes_object, colour_map_object=this_colour_map_object,\n",
    "        colour_norm_object=this_colour_norm_object,\n",
    "        values_to_colour=csi_matrix, min_colour_value=0.,\n",
    "        max_colour_value=1., orientation_string='vertical',\n",
    "        extend_min=False, extend_max=False)\n",
    "    colour_bar_object.set_label('CSI (critical success index)')\n",
    "\n",
    "    bias_colour_tuple = ()\n",
    "    for _ in range(len(LEVELS_FOR_BIAS_CONTOURS)):\n",
    "        bias_colour_tuple += (bias_line_colour,)\n",
    "\n",
    "    bias_contour_object = pyplot.contour(\n",
    "        success_ratio_matrix, pod_matrix, frequency_bias_matrix,\n",
    "        LEVELS_FOR_BIAS_CONTOURS, colors=bias_colour_tuple,\n",
    "        linewidths=bias_line_width, linestyles='dashed', axes=axes_object)\n",
    "    pyplot.clabel(\n",
    "        bias_contour_object, inline=True, inline_spacing=BIAS_LABEL_PADDING_PX,\n",
    "        fmt=BIAS_STRING_FORMAT, fontsize=FONT_SIZE)\n",
    "\n",
    "    nan_flags = numpy.logical_or(\n",
    "        numpy.isnan(success_ratio_by_threshold), numpy.isnan(pod_by_threshold)\n",
    "    )\n",
    "\n",
    "    if not numpy.all(nan_flags):\n",
    "        real_indices = numpy.where(numpy.invert(nan_flags))[0]\n",
    "        axes_object.plot(\n",
    "            success_ratio_by_threshold[real_indices],\n",
    "            pod_by_threshold[real_indices], color=line_colour,\n",
    "            linestyle='solid', linewidth=line_width)\n",
    "\n",
    "    axes_object.set_xlabel('Success ratio (1 - FAR)')\n",
    "    axes_object.set_ylabel('POD (probability of detection)')\n",
    "    axes_object.set_xlim(0., 1.)\n",
    "    axes_object.set_ylim(0., 1.)\n",
    "\n",
    "    return pod_by_threshold, success_ratio_by_threshold\n",
    "\n",
    "import numpy\n",
    "import matplotlib.colors\n",
    "import matplotlib.pyplot as pyplot\n",
    "#import performance_diagrams\n",
    "\n",
    "DEFAULT_LINE_COLOUR = numpy.array([228, 26, 28], dtype=float) / 255\n",
    "DEFAULT_LINE_WIDTH = 3\n",
    "DEFAULT_RANDOM_LINE_COLOUR = numpy.full(3, 152. / 255)\n",
    "DEFAULT_RANDOM_LINE_WIDTH = 2\n",
    "\n",
    "LEVELS_FOR_PEIRCE_CONTOURS = numpy.linspace(0, 1, num=11, dtype=float)\n",
    "\n",
    "FIGURE_WIDTH_INCHES = 10\n",
    "FIGURE_HEIGHT_INCHES = 10\n",
    "\n",
    "FONT_SIZE = 20\n",
    "pyplot.rc('font', size=FONT_SIZE)\n",
    "pyplot.rc('axes', titlesize=FONT_SIZE)\n",
    "pyplot.rc('axes', labelsize=FONT_SIZE)\n",
    "pyplot.rc('xtick', labelsize=FONT_SIZE)\n",
    "pyplot.rc('ytick', labelsize=FONT_SIZE)\n",
    "pyplot.rc('legend', fontsize=FONT_SIZE)\n",
    "pyplot.rc('figure', titlesize=FONT_SIZE)\n",
    "\n",
    "\n",
    "def _get_pofd_pod_grid(pofd_spacing=0.01, pod_spacing=0.01):\n",
    "    \"\"\"Creates grid in POFD-POD space.\n",
    "    M = number of rows (unique POD values) in grid\n",
    "    N = number of columns (unique POFD values) in grid\n",
    "    :param pofd_spacing: Spacing between grid cells in adjacent columns.\n",
    "    :param pod_spacing: Spacing between grid cells in adjacent rows.\n",
    "    :return: pofd_matrix: M-by-N numpy array of POFD values.\n",
    "    :return: pod_matrix: M-by-N numpy array of POD values.\n",
    "    \"\"\"\n",
    "\n",
    "    num_pofd_values = 1 + int(numpy.ceil(1. / pofd_spacing))\n",
    "    num_pod_values = 1 + int(numpy.ceil(1. / pod_spacing))\n",
    "\n",
    "    unique_pofd_values = numpy.linspace(0., 1., num=num_pofd_values)\n",
    "    unique_pod_values = numpy.linspace(0., 1., num=num_pod_values)[::-1]\n",
    "    return numpy.meshgrid(unique_pofd_values, unique_pod_values)\n",
    "\n",
    "\n",
    "def _get_peirce_colour_scheme():\n",
    "    \"\"\"Returns colour scheme for Peirce score.\n",
    "    :return: colour_map_object: Colour scheme (instance of\n",
    "        `matplotlib.colors.ListedColormap`).\n",
    "    :return: colour_norm_object: Instance of `matplotlib.colors.BoundaryNorm`,\n",
    "        defining the scale of the colour map.\n",
    "    \"\"\"\n",
    "\n",
    "    this_colour_map_object = pyplot.cm.Blues\n",
    "    this_colour_norm_object = matplotlib.colors.BoundaryNorm(\n",
    "        LEVELS_FOR_PEIRCE_CONTOURS, this_colour_map_object.N)\n",
    "\n",
    "    rgba_matrix = this_colour_map_object(this_colour_norm_object(\n",
    "        LEVELS_FOR_PEIRCE_CONTOURS\n",
    "    ))\n",
    "\n",
    "    colour_list = [\n",
    "        rgba_matrix[i, ..., :-1] for i in range(rgba_matrix.shape[0])\n",
    "    ]\n",
    "\n",
    "    colour_map_object = matplotlib.colors.ListedColormap(colour_list)\n",
    "    colour_map_object.set_under(numpy.array([1, 1, 1]))\n",
    "    colour_norm_object = matplotlib.colors.BoundaryNorm(\n",
    "        LEVELS_FOR_PEIRCE_CONTOURS, colour_map_object.N)\n",
    "\n",
    "    return colour_map_object, colour_norm_object\n",
    "\n",
    "\n",
    "def get_points_in_roc_curve(observed_labels, forecast_probabilities):\n",
    "    \"\"\"Creates points for ROC curve.\n",
    "    E = number of examples\n",
    "    T = number of binarization thresholds\n",
    "    :param observed_labels: length-E numpy array of class labels (integers in\n",
    "        0...1).\n",
    "    :param forecast_probabilities: length-E numpy array with forecast\n",
    "        probabilities of label = 1.\n",
    "    :return: pofd_by_threshold: length-T numpy array of POFD (probability of\n",
    "        false detection) values.\n",
    "    :return: pod_by_threshold: length-T numpy array of POD (probability of\n",
    "        detection) values.\n",
    "    \"\"\"\n",
    "\n",
    "    assert numpy.all(numpy.logical_or(\n",
    "        observed_labels == 0, observed_labels == 1\n",
    "    ))\n",
    "\n",
    "    assert numpy.all(numpy.logical_and(\n",
    "        forecast_probabilities >= 0, forecast_probabilities <= 1\n",
    "    ))\n",
    "\n",
    "    observed_labels = observed_labels.astype(int)\n",
    "    binarization_thresholds = numpy.linspace(0, 1, num=1001, dtype=float)\n",
    "\n",
    "    num_thresholds = len(binarization_thresholds)\n",
    "    pofd_by_threshold = numpy.full(num_thresholds, numpy.nan)\n",
    "    pod_by_threshold = numpy.full(num_thresholds, numpy.nan)\n",
    "\n",
    "    for k in range(num_thresholds):\n",
    "        these_forecast_labels = (\n",
    "            forecast_probabilities >= binarization_thresholds[k]\n",
    "        ).astype(int)\n",
    "\n",
    "        this_num_hits = numpy.sum(numpy.logical_and(\n",
    "            these_forecast_labels == 1, observed_labels == 1\n",
    "        ))\n",
    "\n",
    "        this_num_false_alarms = numpy.sum(numpy.logical_and(\n",
    "            these_forecast_labels == 1, observed_labels == 0\n",
    "        ))\n",
    "\n",
    "        this_num_misses = numpy.sum(numpy.logical_and(\n",
    "            these_forecast_labels == 0, observed_labels == 1\n",
    "        ))\n",
    "\n",
    "        this_num_correct_nulls = numpy.sum(numpy.logical_and(\n",
    "            these_forecast_labels == 0, observed_labels == 0\n",
    "        ))\n",
    "\n",
    "        try:\n",
    "            pofd_by_threshold[k] = (\n",
    "                float(this_num_false_alarms) /\n",
    "                (this_num_false_alarms + this_num_correct_nulls)\n",
    "            )\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            pod_by_threshold[k] = (\n",
    "                float(this_num_hits) / (this_num_hits + this_num_misses)\n",
    "            )\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "\n",
    "    pod_by_threshold = numpy.array([1.] + pod_by_threshold.tolist() + [0.])\n",
    "    pofd_by_threshold = numpy.array([1.] + pofd_by_threshold.tolist() + [0.])\n",
    "\n",
    "    return pofd_by_threshold, pod_by_threshold\n",
    "\n",
    "\n",
    "def plot_roc_curve(\n",
    "        observed_labels, forecast_probabilities,\n",
    "        line_colour=DEFAULT_LINE_COLOUR, line_width=DEFAULT_LINE_WIDTH,\n",
    "        random_line_colour=DEFAULT_RANDOM_LINE_COLOUR,\n",
    "        random_line_width=DEFAULT_RANDOM_LINE_WIDTH, axes_object=None):\n",
    "    \"\"\"Plots ROC curve.\n",
    "    E = number of examples\n",
    "    :param observed_labels: length-E numpy array of class labels (integers in\n",
    "        0...1).\n",
    "    :param forecast_probabilities: length-E numpy array with forecast\n",
    "        probabilities of label = 1.\n",
    "    :param line_colour: Colour (in any format accepted by `matplotlib.colors`).\n",
    "    :param line_width: Line width (real positive number).\n",
    "    :param random_line_colour: Colour of reference line (ROC curve for random\n",
    "        predictor).\n",
    "    :param random_line_width: Width of reference line (ROC curve for random\n",
    "        predictor).\n",
    "    :param axes_object: Will plot on these axes (instance of\n",
    "        `matplotlib.axes._subplots.AxesSubplot`).  If `axes_object is None`,\n",
    "        will create new axes.\n",
    "    :return: pofd_by_threshold: See doc for `get_points_in_roc_curve`.\n",
    "    :return: pod_by_threshold: Same.\n",
    "    \"\"\"\n",
    "\n",
    "    pofd_by_threshold, pod_by_threshold = get_points_in_roc_curve(\n",
    "        observed_labels=observed_labels,\n",
    "        forecast_probabilities=forecast_probabilities)\n",
    "\n",
    "    if axes_object is None:\n",
    "        _, axes_object = pyplot.subplots(\n",
    "            1, 1, figsize=(FIGURE_WIDTH_INCHES, FIGURE_HEIGHT_INCHES)\n",
    "        )\n",
    "\n",
    "    pofd_matrix, pod_matrix = _get_pofd_pod_grid()\n",
    "    peirce_score_matrix = pod_matrix - pofd_matrix\n",
    "\n",
    "    colour_map_object, colour_norm_object = _get_peirce_colour_scheme()\n",
    "\n",
    "    pyplot.contourf(\n",
    "        pofd_matrix, pod_matrix, peirce_score_matrix,\n",
    "        LEVELS_FOR_PEIRCE_CONTOURS, cmap=colour_map_object,\n",
    "        norm=colour_norm_object, vmin=0., vmax=1., axes=axes_object)\n",
    "\n",
    "    # TODO(thunderhoser): Calling private method is a HACK.\n",
    "    colour_bar_object = _add_colour_bar(\n",
    "        axes_object=axes_object, colour_map_object=colour_map_object,\n",
    "        colour_norm_object=colour_norm_object,\n",
    "        values_to_colour=peirce_score_matrix, min_colour_value=0.,\n",
    "        max_colour_value=1., orientation_string='vertical',\n",
    "        extend_min=False, extend_max=False)\n",
    "\n",
    "    print(colour_bar_object)\n",
    "    colour_bar_object.set_label('Peirce score')\n",
    "\n",
    "    random_x_coords = numpy.array([0., 1.])\n",
    "    random_y_coords = numpy.array([0., 1.])\n",
    "    axes_object.plot(\n",
    "        random_x_coords, random_y_coords, color=random_line_colour,\n",
    "        linestyle='dashed', linewidth=random_line_width)\n",
    "\n",
    "    nan_flags = numpy.logical_or(\n",
    "        numpy.isnan(pofd_by_threshold), numpy.isnan(pod_by_threshold)\n",
    "    )\n",
    "\n",
    "    if not numpy.all(nan_flags):\n",
    "        real_indices = numpy.where(numpy.invert(nan_flags))[0]\n",
    "        axes_object.plot(\n",
    "            pofd_by_threshold[real_indices], pod_by_threshold[real_indices],\n",
    "            color=line_colour, linestyle='solid', linewidth=line_width)\n",
    "\n",
    "    axes_object.set_xlabel('POFD (probability of false detection)')\n",
    "    axes_object.set_ylabel('POD (probability of detection)')\n",
    "    axes_object.set_xlim(0., 1.)\n",
    "    axes_object.set_ylim(0., 1.)\n",
    "\n",
    "    return pofd_by_threshold, pod_by_threshold"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_attributes_diagram(test_target_y['Binary_EF'].values,predsprob[:,1])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9907,
     "status": "ok",
     "timestamp": 1623166625339,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "g_u1fg9578-j",
    "outputId": "75e35c24-ff97-4646-b931-4e9e28d713cd"
   },
   "source": [
    "# Performance Diagram\n",
    "!pip install keras scikit-image netcdf4 pyproj scikit-learn opencv-python matplotlib shapely geopy metpy descartes\n",
    "!rm -rf course_repository\n",
    "!git clone -b add-ML-code https://github.com/swnesbitt/ams-2020-ml-python-course course_repository"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E_J68H1K8ddf"
   },
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "#sys.path.append('/content/course_repository/Introduction_To_Machine_Learning/Lecture_2/')\n",
    "sys.path.append('/content/course_repository/Introduction_To_Machine_Learning/Supervised_Learning_Algorithims/')\n",
    "\n",
    "#sys.path.append('/content/course_repository/')\n",
    "import copy\n",
    "import warnings\n",
    "import numpy\n",
    "import matplotlib.pyplot as pyplot\n",
    "import utils\n",
    "import roc_curves\n",
    "import attr_diagrams\n",
    "import performance_diagrams \n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "DEFAULT_FEATURE_DIR_NAME = ('./data/track_data_ncar_ams_3km_csv_small')\n",
    "SEPARATOR_STRING = '\\n\\n' + '*' * 50 + '\\n\\n'\n",
    "MINOR_SEPARATOR_STRING = '\\n\\n' + '-' * 50 + '\\n\\n'\n",
    "\n",
    "MODULE2_DIR_NAME = '.'\n",
    "SHORT_COURSE_DIR_NAME = '..'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tAJoiv1W8H25"
   },
   "source": [
    "training_event_frequency = np.mean(\n",
    "    training_target_y['Binary_EF'].values)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "training_event_frequency"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qS966Uez8Tas"
   },
   "source": [
    "MAX_PEIRCE_SCORE_KEY = 'max_peirce_score'\n",
    "AUC_KEY = 'area_under_roc_curve'\n",
    "MAX_CSI_KEY = 'max_csi'\n",
    "BRIER_SCORE_KEY = 'brier_score'\n",
    "BRIER_SKILL_SCORE_KEY = 'brier_skill_score'\n",
    "DEFAULT_FIG_WIDTH_INCHES = 10\n",
    "DEFAULT_FIG_HEIGHT_INCHES = 10\n",
    "SMALL_FIG_WIDTH_INCHES = 10\n",
    "SMALL_FIG_HEIGHT_INCHES = 10\n",
    "FIGURE_RESOLUTION_DPI = 300\n",
    "\n",
    "BAR_GRAPH_COLOUR = numpy.array([27, 158, 119], dtype=float) / 255\n",
    "BAR_GRAPH_EDGE_WIDTH = 2\n",
    "BAR_GRAPH_FONT_SIZE = 14\n",
    "BAR_GRAPH_FONT_COLOUR = numpy.array([217, 95, 2], dtype=float) / 255\n",
    "\n",
    "FONT_SIZE = 20\n",
    "def eval_binary_classifn2(\n",
    "        observed_labels, forecast_probabilities, training_event_frequency,\n",
    "        verbose=True, create_plots=True, dataset_name=None):\n",
    "    \"\"\"Evaluates binary-classification model.\n",
    "\n",
    "    E = number of examples\n",
    "\n",
    "    :param observed_labels: length-E numpy array of observed labels (integers in\n",
    "        0...1, where 1 means that event occurred).\n",
    "    :param forecast_probabilities: length-E numpy array with forecast\n",
    "        probabilities of event (positive class).\n",
    "    :param training_event_frequency: Frequency of event in training data.\n",
    "    :param verbose: Boolean flag.  If True, will print results to command\n",
    "        window.\n",
    "    :param create_plots: Boolean flag.  If True, will create plots.\n",
    "    :param dataset_name: Dataset name (e.g., \"validation\").  Used only if\n",
    "        `create_plots == True or verbose == True`.\n",
    "    \"\"\"\n",
    "\n",
    "    pofd_by_threshold, pod_by_threshold = get_points_in_roc_curve(\n",
    "        observed_labels=observed_labels,\n",
    "        forecast_probabilities=forecast_probabilities)\n",
    "\n",
    "    max_peirce_score = numpy.nanmax(pod_by_threshold - pofd_by_threshold)\n",
    "    area_under_roc_curve = sklearn.metrics.auc(\n",
    "        x=pofd_by_threshold, y=pod_by_threshold)\n",
    "\n",
    "    pod_by_threshold, success_ratio_by_threshold = (\n",
    "        get_points_in_perf_diagram(\n",
    "            observed_labels=observed_labels,\n",
    "            forecast_probabilities=forecast_probabilities)\n",
    "    )\n",
    "\n",
    "    csi_by_threshold = (\n",
    "        (pod_by_threshold ** -1 + success_ratio_by_threshold ** -1 - 1) ** -1\n",
    "    )\n",
    "    max_csi = numpy.nanmax(csi_by_threshold)\n",
    "\n",
    "    mean_forecast_by_bin, event_freq_by_bin, num_examples_by_bin = (\n",
    "        _get_points_in_relia_curve(\n",
    "            observed_labels=observed_labels,\n",
    "            forecast_probabilities=forecast_probabilities, num_bins=20)\n",
    "    )\n",
    "\n",
    "    uncertainty = training_event_frequency * (1. - training_event_frequency)\n",
    "\n",
    "    this_numerator = numpy.nansum(\n",
    "        num_examples_by_bin *\n",
    "        (mean_forecast_by_bin - event_freq_by_bin) ** 2\n",
    "    )\n",
    "    reliability = this_numerator / numpy.sum(num_examples_by_bin)\n",
    "\n",
    "    this_numerator = numpy.nansum(\n",
    "        num_examples_by_bin *\n",
    "        (event_freq_by_bin - training_event_frequency) ** 2\n",
    "    )\n",
    "    resolution = this_numerator / numpy.sum(num_examples_by_bin)\n",
    "\n",
    "    brier_score = uncertainty + reliability - resolution\n",
    "    brier_skill_score = (resolution - reliability) / uncertainty\n",
    "\n",
    "    evaluation_dict = {\n",
    "        MAX_PEIRCE_SCORE_KEY: max_peirce_score,\n",
    "        AUC_KEY: area_under_roc_curve,\n",
    "        MAX_CSI_KEY: max_csi,\n",
    "        BRIER_SCORE_KEY: brier_score,\n",
    "        BRIER_SKILL_SCORE_KEY: brier_skill_score\n",
    "    }\n",
    "\n",
    "    if verbose or create_plots:\n",
    "        dataset_name = dataset_name[0].upper() + dataset_name[1:]\n",
    "\n",
    "    if verbose:\n",
    "        print('{0:s} Max Peirce score (POD - POFD) = {1:.3f}'.format(\n",
    "            dataset_name, evaluation_dict[MAX_PEIRCE_SCORE_KEY]\n",
    "        ))\n",
    "        print('{0:s} AUC (area under ROC curve) = {1:.3f}'.format(\n",
    "            dataset_name, evaluation_dict[AUC_KEY]\n",
    "        ))\n",
    "        print('{0:s} Max CSI (critical success index) = {1:.3f}'.format(\n",
    "            dataset_name, evaluation_dict[MAX_CSI_KEY]\n",
    "        ))\n",
    "        print('{0:s} Brier score = {1:.3f}'.format(\n",
    "            dataset_name, evaluation_dict[BRIER_SCORE_KEY]\n",
    "        ))\n",
    "\n",
    "        message_string = (\n",
    "            '{0:s} Brier skill score (improvement over climatology) = {1:.3f}'\n",
    "        ).format(dataset_name, evaluation_dict[BRIER_SKILL_SCORE_KEY])\n",
    "        print(message_string)\n",
    "\n",
    "    if not create_plots:\n",
    "        return evaluation_dict\n",
    "\n",
    "    #_, axes_object = pyplot.subplots(\n",
    "    #    1, 1, figsize=(SMALL_FIG_WIDTH_INCHES, SMALL_FIG_HEIGHT_INCHES)\n",
    "    #)\n",
    "\n",
    "    #plot_roc_curve(\n",
    "    #    observed_labels=observed_labels,\n",
    "    #    forecast_probabilities=forecast_probabilities,\n",
    "    #    axes_object=axes_object)\n",
    "\n",
    "    #title_string = '{0:s} ROC curve (AUC = {1:.3f})'.format(\n",
    "    #    dataset_name, evaluation_dict[AUC_KEY]\n",
    "    #)\n",
    "\n",
    "    #pyplot.title(title_string)\n",
    "    #pyplot.show()\n",
    "\n",
    "    _, axes_object = pyplot.subplots(\n",
    "        1, 1, figsize=(SMALL_FIG_WIDTH_INCHES, SMALL_FIG_HEIGHT_INCHES)\n",
    "    \n",
    "    )\n",
    "\n",
    "    plot_performance_diagram(\n",
    "        observed_labels=observed_labels,\n",
    "        forecast_probabilities=forecast_probabilities,\n",
    "        axes_object=axes_object)\n",
    "\n",
    "    title_string = '{0:s} performance diagram (max CSI = {1:.3f})'.format(\n",
    "        dataset_name, evaluation_dict[MAX_CSI_KEY]\n",
    "    )\n",
    "\n",
    "    #pyplot.title(title_string)\n",
    "    #pyplot.show()\n",
    "\n",
    "    figure_object, axes_object = pyplot.subplots(\n",
    "        1, 1, figsize=(SMALL_FIG_WIDTH_INCHES, SMALL_FIG_HEIGHT_INCHES)\n",
    "    )\n",
    "\n",
    "    plot_attributes_diagram(\n",
    "        observed_labels=observed_labels,\n",
    "        forecast_probabilities=forecast_probabilities, num_bins=20,\n",
    "        figure_object=figure_object, axes_object=axes_object)\n",
    "\n",
    "    #title_string = (\n",
    "    #    '{0:s} attributes diagram (Brier skill score = {1:.3f})'\n",
    "    #).format(dataset_name, evaluation_dict[BRIER_SKILL_SCORE_KEY])\n",
    "\n",
    "    axes_object.set_title(title_string)\n",
    "    #pyplot.show()\n",
    "\n",
    "    return evaluation_dict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1510,
     "status": "ok",
     "timestamp": 1623175189669,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "TiT32OkT8iNx",
    "outputId": "934b8f88-1958-4948-c2cb-7bf35d9eeadf",
    "scrolled": false
   },
   "source": [
    "import sklearn\n",
    "eval_binary_classifn2(test_target_y['Binary_EF'].values,predsprob[:,1], training_event_frequency, dataset_name='test_data')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jIDslPuD84_u"
   },
   "source": [
    "# Training Data\n",
    "\n",
    "#predictor_cols = ['0.5_Tilt_(m)',\t'Peak_Avg.',\t'Peak_Width',\t'Avg_Meso_Distance_(Km)',\t'Avg_Meso_V',\t'Binary_Mode',\t'Peak_(m/s)',\t'PA_(m/s)',\t'Tavg_(m/s)',\t'peak_time_m/s',\t'Increase']\n",
    "#predictor_cols = ['0.5_Tilt_(m)',\t'Peak_Tilt_1.3',\t'Peak_Tilt_0.9',\t'Peak_Tilt_0.5',\t'Peak_Avg.',\t'Peak_Width',\t'Avg_Tilt_0.5',\t'Avg_Tilt_0.9',\t'Avg_Tilt_1.3',\t'Avg_Meso_Distance_(Km)',\t'Avg_Meso_V',\t'Binary_Mode',\t'Peak_(m/s)',\t'PA_(m/s)',\t'Tavg_(m/s)',\t'tilt1_peak_(m/s',\t'tilt2_peak_(m/s)',\t'tilt3_peak_(m/s)',\t'peak_time_m/s',\t'Increase']\n",
    "\n",
    "#1st iteration\n",
    "#predictor_cols = ['Avg_Meso_Distance_(Km)',\t'Mode',\t'Peak_(m/s)', 'Distance(km)', 'tr1meso', 'tr2meso', 'tr3meso', 'tr1dV', 'tr2dV', 'tr3dV']\n",
    "#2nd iteration\n",
    "#predictor_cols = ['Avg_Meso_Distance_(Km)',\t'Mode',\t'Peak_(m/s)', 'tr3meso', 'tr2dV', 'tr3dV']\n",
    "\n",
    "#3rd iteration\n",
    "predictor_cols = ['Avg_Meso_Distance_(Km)',\t'Mode',\t'Peak_(m/s)']\n",
    "\n",
    "target_col = ['Binary_EF']\n",
    "X = reduced_data[predictor_cols]\n",
    "Y = reduced_data[target_col]\n",
    "\n",
    "X_normal = preprocessing.normalize(X)\n",
    "X_scaled = preprocessing.scale(X)\n",
    "\n",
    "training_predictor, test_predictor, training_target, test_target = train_test_split(X_scaled, Y, test_size=0.3, random_state=25, stratify=Y)\n",
    "\n",
    "# Initiate the model\n",
    "rf_model_t = RandomForestClassifier(n_estimators=100).fit(training_predictor, training_target.values.ravel())\n",
    "\n",
    "# Train the Model\n",
    "#p = logreg_t.fit(X_scaled, Y.values.ravel())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eMXsfvWWoekB"
   },
   "source": [
    "# Test model with training data\n",
    "training_event_frequency_t = numpy.mean(\n",
    "    training_target['Binary_EF'].values)\n",
    "intensity_predict_t = rf_model_t.predict(training_predictor)\n",
    "predsprob_t = rf_model_t.predict_proba(training_predictor)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1315,
     "status": "ok",
     "timestamp": 1623175225689,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "1BpqmzJFotWF",
    "outputId": "2ca3b121-6ef9-4ee6-8c5b-997c33778153"
   },
   "source": [
    "# Evaluate training data\n",
    "eval_binary_classifn2(training_target['Binary_EF'].values,predsprob_t[:,1], training_event_frequency_t, dataset_name='training_data')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 167,
     "status": "ok",
     "timestamp": 1623175256979,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "uYSiFcrS-Qd6",
    "outputId": "a546cec9-cce7-4ffc-a937-d5b71b9ba010"
   },
   "source": [
    "# Check skill with confusion matrix\n",
    "from sklearn import metrics\n",
    "cnf_matrix = metrics.confusion_matrix(training_target['Binary_EF'], intensity_predict_t)\n",
    "cnf_matrix"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 143,
     "status": "ok",
     "timestamp": 1623175272173,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "2yNJ9CDk-YOW",
    "outputId": "b7526b5d-5b01-47ad-ac58-6addfe04da3c"
   },
   "source": [
    "a = cnf_matrix[0,0]\n",
    "b = cnf_matrix[0,1]\n",
    "c = cnf_matrix[1,0]\n",
    "d = cnf_matrix[1,1]\n",
    "print('number of true positives (forecast = label = \"yes\"):',a)\n",
    "print('number of false positives (forecast = \"yes\" but label = \"no\"):',b)\n",
    "print('number of false negatives (forecast = \"no\" but label = \"yes\"):',c)\n",
    "print('number of true negatives (forecast = label = \"no\"):',d)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IFdTZEcA-aaL"
   },
   "source": [
    "# Probability of Detection\n",
    "POD = a/(a+c)\n",
    "\n",
    "# Probability of False Detection\n",
    "POFD = b/(b+d)\n",
    "\n",
    "# False Alarm Ratio\n",
    "FAR = b/(a+b)\n",
    "\n",
    "# Success Ratio\n",
    "SR = 1-FAR\n",
    "\n",
    "# Frequency of Correct Nulls\n",
    "FOCN = d/(c+d)\n",
    "\n",
    "# Accuracy\n",
    "Accuracy = (a+d)/(a+b+c+d)\n",
    "\n",
    "# Critical Success Index\n",
    "CSI = a/(a+b+c)\n",
    "\n",
    "# Frequency Bias\n",
    "FB = (a+b)/(c+d)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 143,
     "status": "ok",
     "timestamp": 1623175274913,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "Lj6iQIfu-cu0",
    "outputId": "66197d64-1972-404e-dd6f-2ee7744c490e"
   },
   "source": [
    "print(POD)\n",
    "print(POFD)\n",
    "print(FAR)\n",
    "print(SR)\n",
    "print(FOCN)\n",
    "print(Accuracy)\n",
    "print(CSI)\n",
    "print(FB)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 146,
     "status": "ok",
     "timestamp": 1623175288106,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "glb2REfl-fRL",
    "outputId": "47a39dbd-1f20-40f8-8c89-9041e3617c32"
   },
   "source": [
    "#import sklearn\n",
    "# Other performance metrics\n",
    "\n",
    "prob0 = []\n",
    "prob1 = []\n",
    "for i in range(len(predsprob_t)):\n",
    "    prob0.append(predsprob_t[i][0])\n",
    "    prob1.append(predsprob_t[i][1])\n",
    "\n",
    "#Jaccard Index, want this to be close to one\n",
    "from sklearn.metrics import jaccard_score\n",
    "j_index = jaccard_score(y_true=training_target['Binary_EF'],y_pred=intensity_predict_t)\n",
    "round(j_index,2)\n",
    "print('j_index:',j_index)\n",
    "\n",
    "# F1-score\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(training_target['Binary_EF'], intensity_predict_t)\n",
    "print('f1 score',f1)\n",
    "\n",
    "# Brier skill score\n",
    "##from sklearn.metrics import brier_score_loss\n",
    "##log_score = brier_score_loss((test_target['fzn_or_liq'].values).reshape(-1,1), predsprob[:][0])\n",
    "##print('Brier:',log_score)\n",
    "from sklearn.metrics import brier_score_loss\n",
    "brier_score = brier_score_loss(training_target['Binary_EF'],prob1)\n",
    "print('Brier:',brier_score)\n",
    "\n",
    "# Precision score\n",
    "from sklearn.metrics import precision_score\n",
    "precision_score = precision_score(training_target['Binary_EF'], intensity_predict_t)\n",
    "print('precision score:', precision_score)\n",
    "\n",
    "#RMSE\n",
    "print(math.sqrt(sklearn.metrics.mean_squared_error(y_true=training_target['Binary_EF'],y_pred=intensity_predict_t)))\n",
    "\n",
    "#Accuracy\n",
    "print(rf_model_t.score(training_predictor,training_target))\n",
    "\n",
    "#Recall score\n",
    "from sklearn.metrics import recall_score\n",
    "print(recall_score(training_target['Binary_EF'], intensity_predict_t))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WQLviy8NpHZN"
   },
   "source": [
    "# Testing Training Sizes\n",
    "train_sizes = [50, 80, 110, 140 , 170, 210]\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import learning_curve\n",
    "train_sizes, train_scores, validation_scores = learning_curve(\n",
    "estimator = RandomForestClassifier(),\n",
    "X = X_scaled,\n",
    "y = Y, train_sizes = train_sizes, cv = 5,\n",
    "scoring = 'neg_root_mean_squared_error')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DTcUJ-2CrjV6"
   },
   "source": [
    "sorted(sklearn.metrics.SCORERS.keys())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 132,
     "status": "ok",
     "timestamp": 1623185096264,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "gzXDjKzhpxtj",
    "outputId": "04403cd2-c151-44f2-b17a-c7af4545a85d"
   },
   "source": [
    "train_scores_mean = -train_scores.mean(axis = 1)\n",
    "validation_scores_mean = -validation_scores.mean(axis = 1)\n",
    "print('Mean training scores\\n\\n', pd.Series(train_scores_mean, index = train_sizes))\n",
    "print('\\n', '-' * 20) # separator\n",
    "print('\\nMean validation scores\\n\\n',pd.Series(validation_scores_mean, index = train_sizes))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "executionInfo": {
     "elapsed": 522,
     "status": "ok",
     "timestamp": 1623185106894,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "gcj5EmMcp0XM",
    "outputId": "efb9fcf9-e764-4ed8-e114-ff4ba6a84b1e"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(train_sizes, train_scores_mean, label = 'Training score')\n",
    "plt.plot(train_sizes, validation_scores_mean, label = 'Validation score')\n",
    "plt.ylabel('RMSE', fontsize = 14)\n",
    "plt.xlabel('Training set size', fontsize = 14)\n",
    "plt.title('Learning curves', fontsize = 18, y = 1.03)\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "plt.savefig('/content/drive/MyDrive/Colab Notebooks/Machine_Learning/Images/RF_rmse_LC')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1059,
     "status": "ok",
     "timestamp": 1623175573141,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "Tdgpb_GWu_yH",
    "outputId": "6b3b07ca-c74e-4686-e86f-c407b1acb2ae"
   },
   "source": [
    "#Cross-validation\n",
    "from sklearn.model_selection import cross_validate # STRATIFEID K-FOLD IS USED AUTOMATICALLY WHEN GIVEN AN INTEGER\n",
    "import numpy as np\n",
    "#create a new KNN model\n",
    "log_reg_cv = RandomForestClassifier()\n",
    "#train model with cv of 5 \n",
    "cv_scores = cross_validate(log_reg_cv, X_scaled, Y.values.ravel(), cv=5, scoring=('jaccard', 'max_error', 'neg_brier_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'precision', 'recall', 'roc_auc', 'accuracy', 'f1'), return_estimator=True)\n",
    "#print each cv score (accuracy) and average them\n",
    "print(cv_scores)\n",
    "print('jaccard_scores mean:{}'.format(np.mean(cv_scores['test_jaccard'])))\n",
    "print('jaccard_scores max:{}'.format(np.max(cv_scores['test_jaccard'])))\n",
    "print(np.std(cv_scores['test_jaccard']))\n",
    "print('_neg_brier_score_scores mean:{}'.format(np.mean(cv_scores['test_neg_brier_score'])))\n",
    "print('_neg_brier_score_scores max:{}'.format(np.max(cv_scores['test_neg_brier_score'])))\n",
    "print(np.std(cv_scores['test_neg_brier_score']))\n",
    "print('neg_log_loss_scores mean:{}'.format(np.mean(cv_scores['test_neg_log_loss'])))\n",
    "print('neg_log_loss_scores max:{}'.format(np.max(cv_scores['test_neg_log_loss'])))\n",
    "print(np.std(cv_scores['test_neg_log_loss']))\n",
    "print('neg_mean_abs_error_scores mean:{}'.format(np.mean(cv_scores['test_neg_mean_absolute_error'])))\n",
    "print('neg_mean_abs_error_scores max:{}'.format(np.max(cv_scores['test_neg_mean_absolute_error'])))\n",
    "print(np.std(cv_scores['test_neg_mean_absolute_error']))\n",
    "print('neg_mean_sqr_error_scores mean:{}'.format(np.mean(cv_scores['test_neg_mean_squared_error'])))\n",
    "print('neg_mean_sqr_error_scores max:{}'.format(np.max(cv_scores['test_neg_mean_squared_error'])))\n",
    "print(np.std(cv_scores['test_neg_mean_squared_error']))\n",
    "print('precision_scores mean:{}'.format(np.mean(cv_scores['test_precision'])))\n",
    "print('precision_scores max:{}'.format(np.max(cv_scores['test_precision'])))\n",
    "print(np.std(cv_scores['test_precision']))\n",
    "print('recall_scores mean:{}'.format(np.mean(cv_scores['test_recall'])))\n",
    "print('recall_scores max:{}'.format(np.max(cv_scores['test_recall'])))\n",
    "print(np.std(cv_scores['test_recall']))\n",
    "print('roc_auc_scores mean:{}'.format(np.mean(cv_scores['test_roc_auc'])))\n",
    "print('roc_auc_scores max:{}'.format(np.max(cv_scores['test_roc_auc'])))\n",
    "print(np.std(cv_scores['test_roc_auc']))\n",
    "print('accuracy_scores mean:{}'.format(np.mean(cv_scores['test_accuracy'])))\n",
    "print('accuracy_scores max:{}'.format(np.max(cv_scores['test_accuracy'])))\n",
    "print(np.std(cv_scores['test_accuracy']))\n",
    "print('f1_scores mean:{}'.format(np.mean(cv_scores['test_f1'])))\n",
    "print('f1_scores max:{}'.format(np.max(cv_scores['test_f1'])))\n",
    "print(np.std(cv_scores['test_f1']))\n",
    "print(np.where(cv_scores['test_accuracy']==np.max(cv_scores['test_accuracy'])))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qsDjwymjDcID"
   },
   "source": [
    "#Create confusion matrix evaluator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def confusion_matrix_scorer(log_reg_cv, X, Y):\n",
    "    y_pred = log_reg_cv.predict(X)\n",
    "    cm = confusion_matrix(Y, y_pred)\n",
    "    return {'tn': cm[0, 0], 'fp': cm[0, 1], 'fn': cm[1, 0], 'tp': cm[1, 1]}\n",
    "\n",
    "cv_scores = cross_validate(log_reg_cv, X_scaled, Y.values.ravel(), cv=5, scoring=confusion_matrix_scorer, return_estimator=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 174,
     "status": "ok",
     "timestamp": 1623175582797,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "OYuNkmbaDeFi",
    "outputId": "aaf6fceb-1751-411a-fad8-08b94c9be1b2"
   },
   "source": [
    "#Plot CV confusion matrices\n",
    "a = cv_scores['test_tp']\n",
    "b = cv_scores['test_fp']\n",
    "c = cv_scores['test_fn']\n",
    "d = cv_scores['test_tn']\n",
    "print('number of true positives (forecast = label = \"yes\"):',a)\n",
    "print('number of false positives (forecast = \"yes\" but label = \"no\"):',b)\n",
    "print('number of false negatives (forecast = \"no\" but label = \"yes\"):',c)\n",
    "print('number of true negatives (forecast = label = \"no\"):',d)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 988,
     "status": "ok",
     "timestamp": 1623175617985,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "UW-j51IADnGV",
    "outputId": "546b7a65-b8db-42bb-ddc9-c1336de263fe"
   },
   "source": [
    "#Cross-validation diagnostics, COMBINES ALL CV PREDICTIONS\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "cv_predictions = cross_val_predict(log_reg_cv, X_scaled, Y.values.ravel(), cv=5, method='predict')\n",
    "cnf_matrix = metrics.confusion_matrix(Y.values.ravel(), cv_predictions)\n",
    "cnf_matrix"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CzYb_8eRDpoi"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-4KHcodQvXsp"
   },
   "source": [
    "#grid search, STRATIIED K-FOLD IS USED WITH AN INTEGER FOR CV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#create new a knn model\n",
    "model = RandomForestClassifier()\n",
    "n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\n",
    "max_features = ['sqrt', 'log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [5,10,20,30,40,50,60,70,80,90]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# define grid search\n",
    "grid = dict(n_estimators=n_estimators,max_features=max_features, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, bootstrap=bootstrap)\n",
    "grid_search = RandomizedSearchCV(estimator=model, param_distributions=grid, cv=5, n_iter=1000, scoring=('jaccard', 'neg_brier_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'precision', 'recall', 'roc_auc', 'accuracy', 'f1'), refit='accuracy')\n",
    "#grid_search = GridSearchCV(estimator=model, param_grid=grid, cv=5, scoring=('jaccard', 'neg_brier_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'precision', 'recall', 'roc_auc', 'accuracy', 'f1'), refit='accuracy')\n",
    "grid_result = grid_search.fit(X_scaled, Y.values.ravel())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1794,
     "status": "ok",
     "timestamp": 1623177466139,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "w8BlZ2lnxNk8",
    "outputId": "730fa0fa-105c-465e-9a76-f9047303b4b5"
   },
   "source": [
    "#check gridsearch cv\n",
    "#check top performing n_neighbors value\n",
    "print(grid_result.best_params_)\n",
    "print(grid_result.best_score_)\n",
    "grid_result.cv_results_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 360,
     "status": "ok",
     "timestamp": 1623177472483,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "Lt4OasLWxfJa",
    "outputId": "ed185293-829d-46e4-9538-57be9fda693b"
   },
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_accuracy']\n",
    "stds = grid_result.cv_results_['std_test_accuracy']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "#Best: 0.909434 using {'bootstrap': True, 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 10}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 158,
     "status": "ok",
     "timestamp": 1623178390958,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "rJvtTlTWxXzI",
    "outputId": "0d49b4d3-3a04-421c-bfca-76f843db65d4"
   },
   "source": [
    "#check mean score for the top performing value of n_neighbors\n",
    "#log_gscv.best_score_\n",
    "grid_result.best_score_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 146,
     "status": "ok",
     "timestamp": 1623178393000,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "OvDCLgByLzB4",
    "outputId": "364ab704-b550-44d7-8d5b-229445159589"
   },
   "source": [
    "#check mean score for the top performing value of n_neighbors\n",
    "#log_gscv.best_score_\n",
    "best_loc=np.where(grid_result.cv_results_['mean_test_accuracy']==grid_result.best_score_)[0][0]\n",
    "print(\"Accuracy\",+grid_result.best_score_)\n",
    "print(\"Accuracy SD\",+grid_result.cv_results_['std_test_accuracy'][6])\n",
    "print(\"Jaccard\",+grid_result.cv_results_['mean_test_jaccard'][6])\n",
    "print(\"Jaccard SD\",+grid_result.cv_results_['std_test_jaccard'][6])\n",
    "print(\"Brier Score\",+grid_result.cv_results_['mean_test_neg_brier_score'][6])\n",
    "print(\"Brier Score SD\",+grid_result.cv_results_['std_test_neg_brier_score'][6])\n",
    "print(\"Log Loss\",+grid_result.cv_results_['mean_test_neg_log_loss'][6])\n",
    "print(\"Log Loss SD\",+grid_result.cv_results_['std_test_neg_log_loss'][6])\n",
    "print(\"Mean ABS Error\",+grid_result.cv_results_['mean_test_neg_mean_absolute_error'][6])\n",
    "print(\"Mean ABS Error SD\",+grid_result.cv_results_['std_test_neg_mean_absolute_error'][6])\n",
    "print(\"Mean Sqr Error\",+grid_result.cv_results_['mean_test_neg_mean_squared_error'][6])\n",
    "print(\"Mean Sqr Error SD\",+grid_result.cv_results_['std_test_neg_mean_squared_error'][6])\n",
    "print(\"Precision\",+grid_result.cv_results_['mean_test_precision'][6])\n",
    "print(\"Precision SD\",+grid_result.cv_results_['std_test_precision'][6])\n",
    "print(\"Recall\",+grid_result.cv_results_['mean_test_recall'][6])\n",
    "print(\"Recall SD\",+grid_result.cv_results_['std_test_recall'][6])\n",
    "print(\"ROC_AUC\",+grid_result.cv_results_['mean_test_roc_auc'][6])\n",
    "print(\"ROC_AUC\",+grid_result.cv_results_['std_test_roc_auc'][6])\n",
    "print(\"F1\",+grid_result.cv_results_['mean_test_f1'][6])\n",
    "print(\"F1 SD\",+grid_result.cv_results_['std_test_f1'][6])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 139,
     "status": "ok",
     "timestamp": 1623185386868,
     "user": {
      "displayName": "Michael Sessa",
      "photoUrl": "",
      "userId": "09679350517018803355"
     },
     "user_tz": 300
    },
    "id": "BiALKGMLL4Gm",
    "outputId": "7e5b0604-41f5-41af-89fe-762bfe54cdcd"
   },
   "source": [
    "#Diagnostics from grid_search\n",
    "grid_search_predict = grid_result.predict(X_scaled)\n",
    "cnf_matrix = metrics.confusion_matrix(Y, grid_search_predict)\n",
    "cnf_matrix"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMllwsbw9S4M9QlxJGHJ8Rs",
   "collapsed_sections": [
    "BYM5OGoY343q"
   ],
   "name": "Random_forest_models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
